---
title: "Simple Linear Regression"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
urlcolor: "blue"
---


```{r setup, include=FALSE}
set.seed(1)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.height = 3.5, 
                      fig.width  = 6,
                      fig.align  = "center")
ggplot2::theme_set(ggplot2::theme_bw())
```

# Learning Objectives

- Simple Linear Regression
- Chapter 8 of [OpenIntro Statistics (Fourth Edition)](https://www.openintro.org/stat/textbook.php)
- [Introduction to Broom](https://cran.r-project.org/web/packages/broom/vignettes/broom.html)

# Motivation
- The simple linear model is not sexy.
- But the most commonly used methods in Statistics are either
    [specific applications of it](https://lindeloev.github.io/tests-as-linear/)
    or are generalizations of it.
- Understanding it well will help you better understand methods
  taught in other classes.
- We teach Linear Regression in STAT 415/615 (the most important course we teach). 
  So these notes are just meant to give you a couple tools that you can 
  build on in that course.

# Broom

```{r}
library(broom)
```

- For the most popular model output (t-tests, linear models, generalized
  linear models), the `broom` package provides three
  functions that aid in data analysis.
  1. `tidy()`: Provides summary information of the model (such as parameter
     estimates and $p$-values) in a tidy format. We used this last class.
  2. `augment()`: Creates data derived from the model and adds it to the
     original data, such as residuals
     and fits. It formats this augmented data in a tidy format.
  3. `glance()`: Prints a single row summary of the model fit.
    
- These three functions are *very* useful and incorporate well with the 
  tidyverse.
  
- You will see examples of using these functions for the linear models below.

# `mtcars`

-   For this lesson, we will use the (infamous) `mtcars` dataset that comes
  with R by default.
    ```{r, message=FALSE}
    library(tidyverse)
    data("mtcars")
    glimpse(mtcars)
    ```

- It's "infamous" because [every R class uses it](https://twitter.com/ZachDrakeTweets/status/1151549076992876551?s=20). 
  But it's a really nice dataset to showcase some statistical methods.
    
- The goal of this dataset is to determine which variables affect fuel 
  consumption (the `mpg` variable in the dataset).
  
-   To begin, we'll look at the association between the variables log-weight 
  and mpg (more on why we used log-weight later).
  
    ```{r}
    ggplot(mtcars, aes(x = wt, mpg)) +
      geom_point() +
      scale_x_log10() +
      xlab("Weight") +
      ylab("Miles Per Gallon")
    ```
    
- It seems that log-weight is negatively associated with mpg.    
    
- It seems that the data approximately fall on a line.

# Line Review
- Every line may be represented by a formula of the form
  $$
  Y = \beta_0 + \beta_1 X
  $$
- $Y$ = response variable on $y$-axis
- $X$ = explanatory variable on the $x$-axis
- $\beta_1$ = slope (rise over run)
  - How much larger is $Y$ when $X$ is increased by 1.
- $\beta_0$ = $y$-intercept (the value of the line at $X = 0$)

- You can represent any line in terms of its slope and its $y$-intercept:

  ![](./cartoons/line_review.png)\ 
    
- If $\beta_1 < 0$ then the line slopes downward. If $\beta_1 > 0$ then the 
  line slopes upward. If $\beta_1 = 0$ then the line is horizontal.
    
- **Exercise**: Suppose we consider the line defined by the following equation:
  $$
  Y = 2 + 4X
  $$
  - What is the value of $Y$ at $X = 3$?
  - What is the value of $Y$ at $X = 4$?
  - What is the difference in $Y$ values at $X = 3$ versus $X = 4$?
  - What is the value of $Y$ at $X = 0$?

# Simple Linear Regression Model

- A line does not *exactly* fit the `mtcars` dataset. But a line does
  *approximate* the `mtcars` data.
  
- Model: Response variable = line + noise.
  $$
  Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
  $$

- We typically assume that the noise ($\epsilon_i$'s) for each individual has 
  mean 0 and some variance $\sigma^2$. We estimate $\sigma^2$.

- **LINEAR MODEL IN A NUTSHELL**: 

  > *Given* $X_i$, mean of $Y_i$ is $\beta_0 + \beta_1 X_i$. Points vary about this mean.
 
  ![](./cartoons/reg1.png)\    
    
- Some intuition:
  - The distribution of $Y$ is *conditional* on the value of $X$.
  - The distribution of $Y$ is assumed to have the **same variance**, 
    $\sigma^2$ for **all possible values of $X$**.
  - This last one is a considerable assumption.
  
- Interpretation:
  - Randomized Experiment: A 1 unit increase in $x$ results in a $\beta_1$ unit
    increase in $y$.
  - Observational Study: Individuals that differ only in 1 unit of $x$ are
    expected to differ by $\beta_1$ units of $y$.

- **Exercise**: What is the interpretation of $\beta_0$? 
  
# Estimating Coefficients
- How do we estimate $\beta_0$ and $\beta_1$?
  - $\beta_0$ and $\beta_1$ are **parameters**
  - We want to estimate them from our **sample**\pause
  - Idea: Draw a line through the cloud of points and calculate the slope 
    and intercept of that line?
  - Problem: Subjective\pause
  - Another idea: Minimize residuals (sum of squared residuals).

- Ordinary Least Squares
  - Residuals: $\hat{\epsilon}_i = Y_{i} - (\hat{\beta}_0 + \hat{\beta}_1X_i)$
  - Sum of squared residuals: $\hat{\epsilon}_1^2 + \hat{\epsilon}_2^2 + \cdots + \hat{\epsilon}_n^2$
  - Find $\hat{\beta}_0$ and $\hat{\beta}_1$ that have small sum of squared residuals.
  - The obtained estimates, $\hat{\beta}_0$ and $\hat{\beta}_1$, are called
    the **ordinary least squares** (OLS) estimates.
    
-   Bad Fit:
    ```{r, echo=FALSE}
    mtcars %>%
      select(mpg, wt) %>%
      mutate(logwt = log(wt)) ->
      submt
    beta0 <- 37
    beta1 <- -10
    submt$fitted <- beta0 + submt$logwt * beta1
    ss <- sum((submt$fitted - submt$mpg)^2)
    submt %>%
      ggplot(mapping = aes(x = logwt, y = mpg)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Log-Weight") + 
      ylab("MPG") +
      geom_segment(mapping = aes(x = logwt, xend = logwt, y = mpg, yend = fitted), alpha = 1/2) +
      ggtitle(paste0("Sum of Squares: ", round(ss, digits = 2)))
    ```
    
-   Better Fit:
    ```{r, echo=FALSE}
    beta0 <- 38
    beta1 <- -14
    submt$fitted <- beta0 + submt$logwt * beta1
    ss <- sum((submt$fitted - submt$mpg)^2)
    submt %>%
      ggplot(mapping = aes(x = logwt, y = mpg)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Log-Weight") + 
      ylab("MPG") +
      geom_segment(mapping = aes(x = logwt, xend = logwt, y = mpg, yend = fitted), alpha = 1/2) +
      ggtitle(paste0("Sum of Squares: ", round(ss, digits = 2)))
    ```
    
-   Best Fit (OLS Fit):
    ```{r, echo=FALSE}
    lmout <- lm(mpg ~ logwt, data = submt)
    beta0 <- coef(lmout)[1]
    beta1 <- coef(lmout)[2]
    submt$fitted <- beta0 + submt$logwt * beta1
    ss <- sum((submt$fitted - submt$mpg)^2)
    submt %>%
      ggplot(mapping = aes(x = logwt, y = mpg)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Log-Weight") + 
      ylab("MPG") +
      geom_segment(mapping = aes(x = logwt, xend = logwt, y = mpg, yend = fitted), alpha = 1/2) +
      ggtitle(paste0("Sum of Squares: ", round(ss, digits = 2)))
    ```

- How to find OLS fits in R:
  1. Make sure you have the explanatory variables in the format you want:
  
     ```{r}
     mtcars %>%
       mutate(logwt = log(wt)) ->
       mtcars
     ```
      
  2. Use `lm()`
  
     ```{r}
     lmout <- lm(mpg ~ logwt, data = mtcars)
     lmtide <- tidy(lmout)
     select(lmtide, term, estimate)
     ```

-   The first argument in `lm()` is a **formula**, where the response variable
  is to the left of the tilde and the explanatory variable is to the right of
  the tilde. 
  
    ``` r
    response ~ explanatory
    ```
    
    This formula says that `lm()` should find the OLS estimates of the 
    following model: 
    
    response = $\beta_0$ + $\beta_1$explanatory + noise
  
- The `data` argument tells `lm()` where to find the response and 
  explanatory variables.

- We often put a "hat" over the coefficient names to denote that they are estimates:
  - $\hat{\beta}_0$ = `r round(lmtide$estimate[1], digits = 1)`.
  - $\hat{\beta}_1$ = `r round(lmtide$estimate[2], digits = 1)`.
  
- Thus, the estimated line is:
  - $E[Y_i]$ = `r round(lmtide$estimate[1], digits = 1)` + 
    `r round(lmtide$estimate[2], digits = 1)`$X_i$.
    
-   **Exercise**: Fit a linear model of displacement on weight. What are the
  regression coefficient estimates. Interpret them.
  
    ```{r, eval = FALSE, echo = FALSE}
    lm_disp_wt <- lm(disp ~ wt, data = mtcars)
    tidy(lm_disp_wt) %>%
      select(term, estimate)
    ```
    
    ```{block, echo = FALSE, eval = FALSE}
    A car that weighs one pound heavier than another car is expected to have a 
    displacement 112 cubic inches larger.
    
    Notice the **non-causal** language here. We are just talking about differences
    in expectations, **not** that a car will gain a certain amount
    of displacement if it's heavier.
    ```    

# Estimating Variance

- We assume that the variance of $Y_i$ is the same for each $X_i$. 
- Call this variance $\sigma^2$.
-   We estimate it by the variability in the residuals.
    
    ```{r, echo = FALSE}
    submt %>%
      mutate(resid = mpg - fitted,
             resid2 = resid^2) ->
      submt
    submt %>%
      ggplot(mapping = aes(x = logwt, y = mpg)) +
      geom_point() +
      geom_abline(slope = beta1, intercept = beta0, color = "blue", alpha = 1/2) +
      xlab("Log-Weight") + 
      ylab("MPG") +
      geom_segment(mapping = aes(x = logwt, xend = logwt, y = mpg, yend = fitted), alpha = 1/2)
    
    submt %>%
      ggplot(mapping = aes(x = logwt, y = resid)) +
      geom_point() +
      xlab("Log-Weight") + 
      ylab("Residuals") +
      geom_segment(mapping = aes(x = logwt, xend = logwt, y = 0, yend = resid), alpha = 1/2) +
      geom_hline(yintercept = 0, col = "blue", alpha = 1/2) +
      ylim(-12.5, 12.5)
    
    submt %>%
      summarize(meansquared = mean(resid2),
                meanweight = mean(logwt)) ->
      sumsubmt
    
    submt %>%
      ggplot(aes(x = resid)) +
      geom_histogram(bins = 7, fill = "white", color = "black") +
      xlab("Residuals")
    ```
- The variance of residuals is the estimated variance in the data. This is
  a general technique.
  - Technical note: people adjust for the number of parameters in a model when calculating the variance, so they no-longer divide by "n - 1".

-   In R, use the broom function `glance()` function to get the estimated standard deviation. It's the value in the `sigma` column.
    ```{r}
    glance(lmout) %>%
      select(sigma)
    ```
    
-   Estimating the variance/standard deviation is important because it is a component in the 
  standard error of $\hat{\beta}_1$ and $\hat{\beta}_0$. These standard
  errors are output by `tidy()`.
    ```{r}
    tidy(lmout) %>%
      select(term, std.error)
    ```
    
- The variance is also used when calculating prediction intervals.

# Hypothesis Testing

- The sign of $\beta_1$ denotes different types of relationships between
  the two quantitative variables:
  - $\beta_1 = 0$: The two quantitative variables are not linearly associated.
  - $\beta_1 > 0$: The two quantitative variables are positively associated.
  - $\beta_1 < 0$: The two quantitative variables are negatively associated.
    
- Hypothesis Testing:
  - We are often interested in testing if a relationship exists:
  - Two possibilities: 
    1. Alternative Hypothesis: $\beta_1 \neq 0$.
    2. Null Hypothesis: $\beta_1 = 0$.
  - Strategy: We calculate the probability of the data assuming possibility 2 
    (called a $p$-value). If this probability is low, we conclude possibility 1. 
    If the this probability is high, we donâ€™t conclude anything.

- Graphic: 
  ![](cartoons/linear_p.png) \ 
  
```{r, eval = FALSE, echo = FALSE}
# set.seed(1)
# ybar <- mean(mtcars$mpg)
# ysd  <- sd(mtcars$mpg)
# miny <- min(mtcars$mpg) - 3.7
# maxy <- max(mtcars$mpg) + 0.7
# 
# ggplot(mtcars, aes(x = wt, mpg)) +
#   geom_point(size = 0.5) +
#   scale_x_log10() +
#   theme_minimal() +
#   theme(axis.text = element_blank(),
#         axis.title = element_blank()) +
#   geom_smooth(method = "lm", se = FALSE) +
#   geom_rug(sides = "b") +
#   ylim(miny, maxy) ->
#   pl
# 
# ggsave(filename = "./cartoons/true_dat.pdf", plot = pl, height = 0.7, width = 1.1)
# 
# for (index in 1:5) {
#     mtcars$sim <- rnorm(n = nrow(mtcars), mean = ybar, sd = ysd)
#     lmtemp <- lm(sim ~ log(wt), data = mtcars)
#     cat(coef(lmtemp)[2], "\n")
#     ggplot(mtcars, aes(x = wt, sim)) +
#       geom_point(size = 0.5) +
#       scale_x_log10() +
#       theme_minimal() +
#       theme(axis.text = element_blank(),
#             axis.title = element_blank()) +
#       geom_smooth(method = "lm", se = FALSE) +
#       geom_rug(sides = "b") +
#       ylim(miny, maxy) ->
#       pl
#   ggsave(filename = paste0("./cartoons/simdat_dat", index, ".pdf"), plot = pl, height = 0.7, width = 1.1)
# }
```

- The sampling distribution of $\hat{\beta}_1$ comes from statistical theory. The 
  $t$-statistic is $\hat{\beta}_1 / SE(\hat{\beta}_1)$. It has 
  a $t$-distribution with $n-2$ degrees of freedom.
  - $SE(\hat{\beta}_1)$: Estimated standard deviation of the sampling distribution
  of $\hat{\beta}_1$.
  
-   The confidence intervals for $\beta_0$ and $\beta_1$ are easy to obtain 
  from the output of `tidy()` if you set `conf.int = TRUE`.
  
    ```{r}
    lmtide <- tidy(lmout, conf.int = TRUE)
    select(lmtide, conf.low, conf.high)
    ```
    
-   **Exercise** (8.18 in OpenIntro 4th Edition): Which is higher? Determine if (i) 
  or (ii) is higher or if they are equal. Explain your reasoning. For a
  regression line, the uncertainty associated with the slope estimate, 
  $\hat{\beta}_1$ , is higher when
    i. there is a lot of scatter around the regression line or
    ii. there is very little scatter around the regression line
        
    ```{block, echo = FALSE, eval = FALSE}
    (i). If there is a lot of scatter, then it is hard to pin-point the mean
    relationship. This can be formalized by looking at the contribution of the
    sample size in the standard error formula for $\hat{\beta}_1$.
    ```
    
    
- **Exercise**: Load in the `bac` dataset from the openintro R package.
  1. Create an appropriate plot to visualize the association between the number
     of beers and the BAC.
     ```{r, echo = FALSE, eval = FALSE}
     library(openintro)
     data("bac")
     ggplot(bac, aes(x = Beers, y = BAC)) +
       geom_point()
     ```
  2. Does the relationship appear positive or negative?
     ```{block, echo = FALSE, eval=FALSE}
     Positive. It points up.
     ```
  3. Write out equation of the OLS line.
     ```{r, echo = FALSE, eval = FALSE}
     lmout <- lm(BAC ~ Beers, data = bac)
     tdat <- tidy(lmout, conf.int = TRUE)
     beta0hat <- tdat$estimate[1]
     beta1hat <- tdat$estimate[2]
     ```
     ```{block, echo = FALSE, eval = FALSE}
     y = `r beta0hat` + `r beta1hat` x
     ```
  4. Do we have evidence that the number of beers is associated with BAC? 
     Formally justify.
     ```{block, echo=FALSE, eval = FALSE}
     Yes, the $p$-value is `r tdat$p.value[2]`, which provides strong
     evidence against the null that the two variables are not linearly
     associated.
     ```
  5. Interpret the coefficient estimates.
     ```{block, echo=FALSE, eval = FALSE}
     Every beer a person drinks is expected to increase their blood alcohol
     content by 0.018. I use causal language here because this was a randomized
     experiment.
    
     The BAC at zero beers is expected to be -0.0127.
     ```
  6. What happens to the standard errors of the estimates when we force
     the intercept to be 0? Why?
     ```{r, eval=FALSE, echo=FALSE}
     lmout2 <- lm(BAC ~ Beers - 1, data = bac)
     tdat2 <- tidy(lmout2, conf.int = TRUE)
     tdat$std.error[2]
     tdat2$std.error
     ```
     ```{block, echo=FALSE, eval = FALSE}
     It shrinks. That's because we are having to estimate one less parameter
     and so we have degrees of freedom.
     ```

# Prediction (Interpolation)

- Definitions
  - **Interpolation**: Making estimates/predictions within the range of the data.
  - **Extrapolation**: Making estimates/predictions outside the range of the data.
  - Interpolation is good. Extrapolation is bad.

-   Interpolation
    ```{r, echo = FALSE}
    ggplot(data = submt, mapping = aes(x = logwt, y = mpg)) +
      geom_point() +
      geom_abline(slope = coef(lmout)[2], intercept = coef(lmout)[1], lwd = 1, col = "blue", alpha = 1/2) +
      geom_segment(data = data.frame(x    = 1, 
                                     xend = 1,
                                     y    = 0,
                                     yend = coef(lmout)[1] + 1 * coef(lmout)[2]),
                   mapping = aes(x = x, xend = xend, y = y, yend = yend), 
                   lty = 2, color = "red", lwd = 1) +
      geom_hline(yintercept = 0, lty = 2, alpha = 1/2) +
      ylab("MPG") +
      xlab("Log-weight")
    ```

-   Extrapolation
    ```{r, echo = FALSE}
    ggplot(data = submt, mapping = aes(x = logwt, y = mpg)) +
      geom_point() +
      geom_abline(slope = coef(lmout)[2], intercept = coef(lmout)[1], lwd = 1, col = "blue", alpha = 1/2) +
      geom_segment(data = data.frame(x    = 3, 
                                     xend = 3,
                                     y    = 0,
                                     yend = coef(lmout)[1] + 3 * coef(lmout)[2]),
                   mapping = aes(x = x, xend = xend, y = y, yend = yend), 
                   lty = 2, color = "red", lwd = 1) +
      geom_hline(yintercept = 0, lty = 2, alpha = 1/2) +
      ylab("MPG") +
      xlab("Log-weight")
    ```

- Why is extrapolation bad?
  1. Not sure if the linear relationship is the same outside the range of
     the data (because we don't have data there to see the relationship).
  2. Not sure if the variability is the same outside the range of the 
     data (because we don't have data there to see the variability).

- To make a prediction:
  1. You need a data frame with the exact same
     variable name as the explanatory variable. 
     ```{r}
     newdf <- tribble(~logwt,
                      1, 
                      1.5)
     ```
  2. Then you use the `predict()` function to obtain predictions.
     ```{r}
     newdf %>%
       mutate(predictions = predict(object = lmout, newdata = newdf)) ->
       newdf
     newdf
     ```

-   **Exercise**: Derive the predictions above by hand (not using `predict()`).
    ```{block, eval = FALSE, echo = FALSE}
    39.3 - 17.1 * 1   = 22.2
    39.3 - 17.1 * 1.5 = 13.6
    ```
    
-   **Exercise**: from the BAC dataset, suppose someone had 5 beers. Use
  `predict()` to predict their BAC.
  
    ```{r, eval = FALSE, echo = FALSE}
    library(openintro)
    data("bac")
    lmbeer <- lm(BAC ~ Beers, data = bac)
    newdata <- tribble(~Beers,
                       5)
    predict(object = lmbeer, newdata = newdata)
    ```
  
# Assumptions

## Assumptions and Violations

- The linear model has many assumptions.

- **You should always check these assumptions**.

- Assumptions in *decreasing* order of importance
  1. **Linearity** - The relationship looks like a straight line.
  2. **Independence** - The knowledge of the value of one observation does not 
     give you any information on the value of another.
  3. **Equal Variance** - The spread is the same for every value of $x$
  4. **Normality** - The distribution of the errors isn't too skewed and there aren't 
     any *too* extreme points. (Only an issue if you have outliers and a 
     small number of observations because of the 
     [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)).
     
- Problems when violated
  1. **Linearity** violated - Linear regression line does not pick up actual 
     relationship. Results aren't meaningful.
  2. **Independence** violated - Linear regression line is unbiased, but standard 
     errors are off. Your $p$-values are too small.
  3. **Equal Variance** violated - Linear regression line is unbiased, but standard 
     errors are off. Your $p$-values may be too small, or too large.
  4. **Normality** violated - Unstable results if outliers are present and sample size 
     is small. Not usually a big deal.
     
-   **Exercise**: What assumptions are made about the distribution of the 
  explanatory variable (the $x_i$'s)?
    ```{block, eval = FALSE, echo = FALSE}
    None. Inference is conditional on the $x_i$'s.
    ```
  
## Evaluating Independence

- Think about the problem.
  - Were different responses measured on the same observational/experimental unit?
  - Were data collected in groups?

- Example of non-independence: The temperature today and the temperature 
  tomorrow. If it is warm today, it is probably warm tomorrow.

- Example of non-independence: You are collecting a survey. To obtain 
  individuals, you select a house at random and then ask all participants 
  in this house to answer the survey. The participants' responses inside 
  each house are probably not independent because they probably share 
  similar beliefs/backgrounds/situations.
  
- Example of independence: You are collecting a survey. To obtain individuals,
  you randomly dial phone numbers until an individual picks up the phone.

## Evaluating other assumptions

- **Evaluate issues by plotting the residuals.**

- The **residuals** are the observed values minus the predicted values.
  $$
  r_i = y_i - \hat{y}_i
  $$

- In the linear model, $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i$.

-   Obtain the residuals by using `augment()` from broom. They will be the
  `.resid` variable.
    ```{r}
    aout <- augment(lmout)
    glimpse(aout)
    ```

- You should always make the following scatterplots. The residuals
  always go on the $y$-axis.
  - Fits $\hat{y}_i$ vs residuals $r_i$.
  - Response $y_i$ vs residuals $r_i$.
  - Explanatory variable $x_i$ vs residuals $r_i$.
    
- In the simple linear model, you can probably evaluate these issues by plotting
  the data ($x_i$ vs $y_i$). But residual plots generalize to *much* 
  more complicated models, whereas just plotting the data does not.
  
```{block, eval = FALSE, echo = FALSE}  
- Solutions:
  1. Linearity Violated: Try a transformation. If the relationship looks 
     curved and monotone (i.e. either always increasing or always decreasing) then 
     try a log transformation.
  2. Independence Violated: You'll need more sophisticated statistical 
     techniques. Hire a statistician.
  3. Equal Variance Violated: If the relationship is also curved and monotone, 
     try a log transformation on the response variable. Or figure out how 
     to perform sandwich estimation.
  4. Normality Violated: Do nothing. But don't use prediction intervals 
     (confidence intervals are fine). Just report the violations from 
     normality. 
```
     
### Example 1: A perfect residual plot

```{r, echo = FALSE, message = FALSE}
set.seed(1)
x <- rnorm(100, sd = 1); y <- x + rnorm(100);
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data") + geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Means are straight lines
- Residuals seem to be centered at 0 for all $x$
- Variance looks equal for all $x$
- Everything looks perfect

### Example 2: Curved Monotone Relationship, Equal Variances

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- rexp(100)
    x <- x - min(x) + 0.5
    y <- log(x) * 20 + rnorm(100, sd = 4)
    df_fake <- tibble(x, y)
    ```

```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Curved (but always increasing) relationship between $x$ and $y$.
- Variance looks equal for all $x$
- Residual plot has a parabolic shape.
-   **Solution**: These indicate a $\log$ transformation of $x$ could help.
    ```{r}
    df_fake %>%
      mutate(logx = log(x)) ->
      df_fake
    lm_fake <- lm(y ~ logx, data = df_fake)
    ```

### Example 3: Curved Non-monotone Relationship, Equal Variances

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- rnorm(100)
    y <- -x^2 + rnorm(100)
    df_fake <- tibble(x, y)
    ```

```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Curved relationship between $x$ and $y$
- Sometimes the relationship is increasing, sometimes it is decreasing.
- Variance looks equal for all $x$
- Residual plot has a parabolic form.
-   **Solution**: Include a squared term in the model (or hire a statistician).
    ```{r}
    lmout <- lm(y ~ x^2, data = df_fake)
    ```

### Example 4: Curved Relationship, Variance Increases with $Y$

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- rnorm(100)
    y <- exp(x + rnorm(100, sd = 1/2))
    df_fake <- tibble(x, y)
    ```

```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Curved relationship between $x$ and $y$
- Variance looks like it increases as $y$ increases
- Residual plot has a parabolic form.
- Residual plot variance looks larger to the right and smaller to the left.
-   **Solution**: Take a log-transformation of $y$.
    ```{r}
    df_fake %>%
      mutate(logy = log(y)) ->
      df_fake
    lm_fake <- lm(logy ~ x, data = df_fake)
    ```


### Example 5: Linear Relationship, Equal Variances, Skewed Distribution

```{r, echo = FALSE}
set.seed(1)
x <- runif(200)
y <- 15 * x + rexp(200, 0.2)
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Straight line relationship between $x$ and $y$.
- Variances about equal for all $x$
- Skew for all $x$
- Residual plots show skew.
- **Solution**: Do nothing, but report skew (usually OK to do)

### Example 6: Linear Relationship, Unequal Variances

-   Generate fake data:
    ```{r}
    set.seed(1)
    x <- runif(100) * 10
    y <- 0.85 * x + rnorm(100, sd = (x - 5) ^ 2)
    df_fake <- tibble(x, y)
    ```


```{r, echo = FALSE}
lmout <- lm(y ~ x)
res_vec <- resid(lmout)
fit_vec <- fitted(lmout)
qplot(x, y, xlab = "X", ylab = "Y", main = "Raw Data")+ geom_smooth(se = FALSE, method = "lm")
qplot(fit_vec, res_vec, xlab = "Fitted Values", ylab = "Residuals", main = "Residual Plot") + geom_hline(yintercept = 0)
```

- Linear relationship between $x$ and $y$.
- Variance is different for different values of $x$.
- Residual plots really good at showing this.
-   **Solution**: The modern solution is to use sandwich estimates of the standard errors
  (hire a statistician).
    ```{r}
    library(sandwich)
    lm_fake <- lm(y ~ x, data = df_fake)
    semat <- sandwich(lm_fake)
    tidy(lm_fake) %>%
      mutate(sandwich_se = sqrt(diag(semat)),
             sandwich_t  = estimate / sandwich_se,
             sandwich_p  = 2 * pt(-abs(sandwich_t), df = df.residual(lm_fake)))
    ```
  
  
# Some Exercises

-   **Exercise**: From the `mtcars` data frame, run a regression of MPG on displacement.
  Evaluate the regression fit and fix any issues. Interpret the coefficient estimates.
    ```{r, eval = FALSE, echo = FALSE}
    lm_mtcars <- lm(mpg ~ disp, data = mtcars)
    lmmt_aug <- augment(lm_mtcars)
    ggplot(lmmt_aug, aes(x = .fitted, y = .resid)) +
      geom_point() +
      geom_hline(yintercept = 0, lty = 2)
    
    lm_mtcars <- lm(mpg ~ log(disp), data = mtcars)
    lmmt_aug <- augment(lm_mtcars)
    ggplot(lmmt_aug, aes(x = .fitted, y = .resid)) +
      geom_point() +
      geom_hline(yintercept = 0, lty = 2)
    ## Looks better but still a little curvey
    ```
  
-   **Exercise**: Consider the data frame `case0801` from the Sleuth3 R package:
    ```{r}
    library(Sleuth3)
    data("case0801")
    ```
    Find an appropriate linear regression model for relating the effect of 
    island area on species number. Find the regression estimates. Interpret them.
    ```{r, echo = FALSE, eval = FALSE}
    ggplot(case0801, aes(x = Area, y = Species)) +
      geom_point() +
      scale_x_log10() +
      scale_y_log10()
    
    case0801 %>%
      mutate(log_area = log2(Area),
             log_species = log2(Species)) ->
      case0801
    
    lm_sp <- lm(log_species ~ log_area, data = case0801)
    tidy(lm_sp)
    
    ## Doubling area tends to correspond to having 2^0.25 times as many species
    ```

# Interpreting Coefficients when you use logs

- Generally, when you use logs, you interpret associations on a 
  *multiplicative* scale instead of an *additive* scale.

- No log:
  - Model: $E[y_i] = \beta_0 + \beta_1 x_i$
  - Observations that differ by 1 unit in $x$ tend to differ by $\beta_1$ units in $y$.

- Log $x$:
  - Model: $E[y_i] = \beta_0 + \beta_1 \log_2(x_i)$
  - Observations that are twice as large in $x$ tend to differ by $\beta_1$ units in $y$.

- Log $y$:
  - Model: $E[\log_2(y_i)] = \beta_0 + \beta_1 x_i$
  - Observations that differ by 1 unit in $x$ tend to be $2^{\beta_1}$ times larger in $y$. 

- Log both:
  - Model: $E[\log_2(y_i)] = \beta_0 + \beta_1 \log_2(x_i)$
  - Observations that are twice as large in $x$ tend to be $2^{\beta_1}$ times larger in $y$. 
  
-   **Exercise**: Re-interpret the regression coefficients estimates you calculated
  using the `case0801` dataset.
    ```{block, eval = FALSE, echo = FALSE}
    2^0.25 = 1.19, so:
    
    Islands that are twice as large tend to have 19\% more species.
    ```

-   **Exercise** Re-interpret the regression coefficient estimates you calculated
  in the `mtcars` data frame when you were exploring the effect of displacement
  on mpg.
    ```{r, eval = FALSE, echo = FALSE}
    lmout <- lm(mpg ~ log(disp), data = mtcars)
    tidy(lmout)
    
    ## Cars that have twice as large a displacement tend to have mpg's 10 less.
    ```

# Summary of R commands

- `augment()`:
  - Residuals $r_i = y_i - \hat{y}_i$: `$.resid`
  - Fitted Values $\hat{y}_i$: `$.fitted`
- `tidy()`:
  - Name of variables: `$term`
  - Coefficient Estimates: `$estimate`
  - Standard Error (standard deviation of sampling distribution of coefficient estimates): `$std.error`
  - t-statistic: `$statistic`
  - p-value: `$p.value`
- `glance()`: 
  - R-squared value (proportion of variance explained by regression line, higher is better): `$r.squared`
  - AIC (lower is better): `$AIC`
  - BIC (lower is better): `$BIC`
