[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DATA 413/613 Data Science",
    "section": "",
    "text": "Syllabus\n\nLectures\n\n01 Git and the Terminal\n\n01 The Terminal and Bash Basics.\n01 Git Setup.\n01 Git and GitHub.\n01 Git Large File Storage.\n01 A Quick Introduction to Git and GitHub.\n01 Lab: Review of Tidyverse Concepts.\n\n02 Organization\n\n02 File Structure\n02 Project Structure\n02 Project Management\n\n03 Programming\n\n03 Memory.\n03 Vectors.\n03 Subsetting.\n03 Functions.\n03 Iteration.\n03 OOP.\n03 S3.\n03 Lab.\n\n04 R Packages\n\n04 R Packages.\n04 Unit Testing.\n04 Automated Checks.\n04 Style.\n\n05 Web Scraping\n\n05 APIs.\n\nOAuth2 Example with Eventbrite.\n\n05 Web Scraping.\n\nIMDB.\nHurricanes.\nMosques.\n\n05 Lab.\n\n06 Introduction to Python\n\n06 Python Setup and Reticulate.\n06 Numpy.\n06 Pandas.\n06 Seaborn.\n06 Seaborn.objects.\n\n07 Introduction to SQL\n\n07 SQL.\n07 DBeaver.\n\n08 GNU Make\n\n08 GNU Make.\n08 Penguins Exercise.\n08 Very Basic Makefile Example.\n\n09 Shiny Apps\n\n09 Shiny Basics.\n09 Layouts.\n09 Reactivity.\n09 HTML.\n09 File Uploads.\n09 Shiny Lab 1: World Bank Data.\n09 Shiny Lab 2: Cereals Data.\n\n\n\n\nExtra Lectures\n\nA1 Basic Statistics\n\nA1 Basic Statistics.\nA1 Statistical Tests, P-values, Confidence Intervals, and Power: A Guide to Misinterpretations.\nA1 Lab.\n\nA2 Linear Models\n\nA2 Simple Linear Regression.\nA2 Many Models.\nA2 Lab.\n\nA3 Data.table\n\nA3 Manipulating Large(ish) Datasets with data.table.\nA3 Lab.\n\nA4 Dates and Times\n\nA4 Dates.\nA4 Lab."
  },
  {
    "objectID": "00_workflow/00_workflow_tips.html",
    "href": "00_workflow/00_workflow_tips.html",
    "title": "Workflow Tips",
    "section": "",
    "text": "Improve your remote learning workflow with some tricks and tips."
  },
  {
    "objectID": "00_workflow/00_workflow_tips.html#toggling-between-multiple-windows",
    "href": "00_workflow/00_workflow_tips.html#toggling-between-multiple-windows",
    "title": "Workflow Tips",
    "section": "Toggling Between Multiple Windows",
    "text": "Toggling Between Multiple Windows\n\nWindows and ubuntu:\n\nWhile holding down “Alt”, press “Tab” repeatedly until you make the desired selection.\n\nMac\n\nWhile holding down “Command”, press “Tab” repeatedly until you make the desired selection."
  },
  {
    "objectID": "00_workflow/00_workflow_tips.html#toggling-between-multiple-desktops.",
    "href": "00_workflow/00_workflow_tips.html#toggling-between-multiple-desktops.",
    "title": "Workflow Tips",
    "section": "Toggling Between Multiple Desktops.",
    "text": "Toggling Between Multiple Desktops.\n\nWindows:\n\nCreate a new desktop with Ctrl+Windows+d\nToggle between desktops with Ctrl+Windows+Arrows\n\nUbuntu:\n\nToggle between desktops with Ctrl+Alt+Arrows\n\nMac:\n\nThe software controlling desktops is called “Mission Control”\nHit “F3” to enter Misson Control\nUse Control+Arrows to move to different desktops."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "airfares.csv: Average first quarter domestic airline fares by origin airport for major U.S. airports from 1993 to 2019. The data are from The Bureau of Transportation Statistics (https://www.transtats.bts.gov/AverageFare/). Variables include:\n\ncode: The origin airport code.\nname: The name of the origin airport.\ncity: The city of the origin airport.\nstate: The state of the origin airport.\nfare: Average first quarter fare from the origin airport in a given year. “Fares are based on the total ticket value which consists of the price charged by the airlines plus any additional taxes and fees levied by an outside entity at the time of purchase. Fares include only the price paid at the time of the ticket purchase and do not include other fees, such as baggage fees, paid at the airport or onboard the aircraft. Averages do not include frequent-flyer or ‘zero fares’ or a few abnormally high reported fares.”\nadj_fare: Inflation adjusted average fare (in 2019 dollars). “The inflation adjustment was performed using Consumer Price Index for all urban consumers.”\nyear: The year.\n\nbig_mac.csv: Measurements from the Economist’s famous Big Mac Index from 2000 to 2017. It contains the price (in 2018 US dollars) of a Big Mac for each country during this time. It also contains the data on the implied purchasing power parity using Big Mac prices. Specifically, the variables are of the form:\n\nprice_&lt;year&gt;: The average price of a Big Mac in a given year. For example, price_2017 contains the average price of a Big Mac in 2017.\nppp_&lt;year&gt;: The purchasing power parity based on the Big Mac in a given year. For example, ppp_2011 contains the purchasing power parity of a Big Mac in 2011.\n\nbob.csv: Bob Ross was a painter who hosted a popular television series on PBS. Each episode would consist of him completing an entire painting. His paintings almost always consisted heavily of elements from nature: trees, clouds, mountains, lakes, etc. These data consist of indicators for what elements are in each episode. This dataset was taken from the excellent crew at fivethirtyeight. See here for their article. The data were obtained using this code. The variables are:\n\nEPISODE: The season and episode number of the episode.\nTITLE: The title of the painting.\nEvery other variable is an indicator for whether the episode contains the element described by the variable name. For example, BARN is 0 if the episode does not have a barn in the painting and 1 if the episode does have a barn in the painting.\n\ncivil_war.csv: List of American Civil War battles, taken from Wikipedia. Variables include:\n\nBattle: The name of the battle.\nDate: The date(s) of the battle. If it took place on one day, then the format is “month day, year”. If it took place over multiple days, then the format is “month day_start-day_end, year”. If it took place over multiple days spanning multiple months then the format is “month_start day_start - month_end day_end, year”. If it took place over multiple days spanning multiple years then the format is “month_start day_start, year_start - month_end day_end, year_end”.\nState: The state where the battle took place. Annotations (e.g.  describing that the state was a territory at the time) are in parentheses.\nCWSAC: A rating of the military significance of the battle by the Civil War Sites Advisory Commission. A = Decisive, B = Major, C = Formative, D = Limited.\nOutcome: Usually \"Confederate victory\", \"Union victory\", or \"Inconclusive\", followed by notes.\n\ncollege_score.csv: Contains a subset of the variables found in the 2016 to 2017 College Scorecard database. These data contain information on colleges in the United States. The variables included are:\n\nUNITID and OPEID: Identifiers for the colleges.\nINSTNM: Institution name\nADM_RATE: The Admission Rate.\nSAT_AVE: Average SAT equivalent score of students admitted.\nUGDS: Enrollment of undergraduate certificate/degree-seeking students\nCOSTT4_A: Average cost of attendance (academic year institutions)\nAVGFACSAL: Average faculty salary\nGRAD_DEBT_MDN: The median debt for students who have completed\nAGE_ENTRY: Average age of entry\nICLEVEL: Level of institution (1 = 4-year, 2 = 2-year, 3 = less than 2-year).\nMN_EARN_WNE_P6: Mean earnings of students working and not enrolled 6 years after entry (so students who graduated in the 2009 to 2010 academic year).\n\nEnglish Women Artists: This is a copy of the Wikipedia page on the list of English women artists. I use this for an example on webscraping.\n\nOriginal Wikipedia page: https://en.wikipedia.org/wiki/List_of_English_women_artists\n\nengart.csv: Contains a partially cleaned table of English women artists scraped from Wikipedia (see above). The variables include:\n\nartist: The full name of the artist, as shown on Wikipedia.\nyears: The year information provided by Wikipedia. Most of the time, the format is birth-death. However, these are the following exceptions:\n\nborn YYYY: Gives the birth date with the death date missing because the artist is still alive (or the death date is unknown).\ndied YYYY: Gives the death date with the birth date missing because it is unknown.\nfl.YYYY-YYYY: “fl” means “floruit” which is Latin for “he/she flourished”. So these are the artist’s active years and the birth/death years are unknown.\n\nmedia: The media with which the artist primarily worked: https://en.wikipedia.org/wiki/List_of_art_media\n\neqbig.csv: Summary information on the largest earthquakes for each year between 1999 and 2019. The data are from Wikipedia The variables include\n\nYear: The year of the earthquake.\nMagnitude: The seismic magnitude of the earthquake (larger means more intense).\nDeathtoll: The number of people who died as a result of the earthquake.\nLocation: The region of the epicenter of the earthquake.\nEvent: The name of the earthquake event.\nDate: The month and day of the earthquake.\n\nestate.csv: Data on 522 home sales in a Midwestern city during the year 2002. The 13 variables are\n\nPrice: Sales price of residence (in dollars)\nArea: Finished area of residence (in square feet)\nBed: Total number of bedrooms in residence\nBath: Total number of bathrooms in residence\nAC: 1 = presence of air conditioning, 0 = absence of air conditioning\nGarage: Number of cars that a garage will hold\nPool: 1 = presence of a pool, 0 = absence of a pool\nYear: Year property was originally constructed\nQuality: Index for quality of construction. High, Medium, or Low.\nStyle: Categorical variable indicating architectural style\nLot: Lot size (in square feet)\nHighway: 1 = highway adjacent, 0 = highway not adjacent.\n\nflights.duckdb: DuckDB database of the {nycflights13} data. See that package documentation for details.\nflights14.csv: Data from 253,316 flights originating from New York City in 2014. This is from the data.table vignettes. The 11 variables include:\n\nyear: Year of scheduled departure.\nmonth: Month of scheduled departure.\nday: Day of scheduled departure.\ndep_delay: Departure delay in minutes.\narr_delay: Arrival delay in minutes.\ncarrier: The two-letter carrier abbreviation.\norigin: The originating airport.\ndest: The destination airport.\nair_time: The amount of time in the air, in minutes.\ndistance: The distance traveled, in miles.\nhour: Hour of scheduled departure.\n\nhalberstadt.csv: The composer John Cage asked his organ piece to be played “as slow as possible”, so the Halberstadt Cathedral started a 639 year continuous performance. Each note in the piece lasts months and these data contain the schedule the performance. These data contain the first 71 years of the performance and were taken from Wikipedia. Variables include:\n\nImpulse: The order of the action.\nAction: Either \"Sound\" (for adding a note) or \"Release\" (for stopping a note).\nNotes: The notes being added or stopped.\nDate: The date that the action began.\nLength: The length, in days, of the previous chord.\n\nhamsters.csv: Data from doi:10.1126/science.abe8499.\nEight hamsters were exposed to the wild type (WT) of SARS-CoV-2, and eight were exposed to a variant of SARS-CoV-2 that contained a particular mutation that the researchers hypothesized increased transmissibility (D614G). The researchers paired these 16 hamsters with 16 healthy hamsters, where the pairs were placed in cages a few inches apart. The researchers then monitored if the healthy hamsters became infected two days later. The variables include:\n\nExposure: The virus variant of the infected hamster. Either \"WT\" or \"D614G\".\nInfected: Was the healthy hamster infected (\"Yes\") or not (\"No\")?\n\nLahman Data: DuckDB version of the 2021 Baseball Lahman Data from https://www.seanlahman.com/baseball-archive/statistics/\n\nlahman.duckdb: DuckDB database.\nlahman_readme.txt: README file from the Lahman website with data dictionary.\n\nregnal.csv: Table of regnal years of English monarchs, taken from Wikipedia: https://en.wikipedia.org/wiki/Regnal_years_of_English_monarchs\n“Regnal years” are years that correspond to a monarch, and might differ from the actual reign of that monarch. It’s mostly used for dating legal documents (“nth year of the reign of King X”). It’s a weird English thing. The variables include:\n\nmonarch: The name of the monarch.\nnum_years: The number of years of the reign.\nfirst: The start year of the reign.\nstart_date: The date when each regnal year begins.\nend_date: The date when each regnal year ends.\nfinal: The final date of the reign.\n\nremakes_1.html and remakes_2.html: These are copies of the Wikipedia pages on film remakes. I use these for exercises on webscraping.\n\nA-M original link: https://en.wikipedia.org/wiki/List_of_film_remakes_(A-M)\nN-Z original link: https://en.wikipedia.org/wiki/List_of_film_remakes_(N-Z)\n\nsmoke.csv: A data frame containing information on smoking status and cancer status for 172 individuals. Data are from Dorn, Harold F. “The relationship of cancer of the lung and the use of tobacco.” The American Statistician 8, no. 5 (1954): 7-13. Variables include\n\nindividual: The individual number.\nsmoke: Whether the individual smoked (smoker) or did not (nonsmoker).\ncancer: Whether the individual had cancer (cancer) or was a control (control). \n\nstarwars.duckdb: DuckDB database, copied from the {starwarsdb} package.\nTidying exercise datasets:\n\nflowers1.csv\nflowers2.csv\nmonkeymem.csv\npew.csv\npreg.csv\ntb.csv\nweather.csv\nwine.csv\n\nwords.txt: The list of acceptable 2015 Scrabble Words.\nWorld Bank Data: The World Bank is an international organization that provides loans to countries with the goal of reducing poverty. The following data frames were all taken from the public data repositories of the World Bank.\n\ncountry.csv: Contains information on the countries in the data set. The variables are:\n\nCountry_Code: A three-letter code for the country. Note that not all rows are countries; some are regions.\nRegion: The region of the country.\nIncomeGroup: Either \"High income\", \"Upper middle income\", \"Lower middle income\", or \"Low income\".\nTableName: The full name of the country.\n\nfertility.csv: Contains the fertility rate information for each country for each year. For the variables 1960 to 2017, the values in the cells represent the fertility rate in total births per woman for the that year. Total fertility rate represents the number of children that would be born to a woman if she were to live to the end of her childbearing years and bear children in accordance with age-specific fertility rates of the specified year.\nlife_exp.csv: Contains the life expectancy information for each country for each year. For the variables 1960 to 2017, the values in the cells represent life expectancy at birth in years for the given year. Life expectancy at birth indicates the number of years a newborn infant would live if prevailing patterns of mortality at the time of its birth were to stay the same throughout its life.\npopulation.csv: Contains the population information for each country. For the variables 1960 to 2017, the values in the cells represent the total population in number of people for the given year. Total population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship. The values shown are midyear estimates."
  },
  {
    "objectID": "admin/syllabus.html",
    "href": "admin/syllabus.html",
    "title": "DATA-413/613 Data Science",
    "section": "",
    "text": "Time: See Registrar\nInstructor: Dr. David Gerard\nEmail: dgerard@american.edu\nOffice: DMTI 106E\nOffice Hours: See Canvas"
  },
  {
    "objectID": "admin/syllabus.html#assignments",
    "href": "admin/syllabus.html#assignments",
    "title": "DATA-413/613 Data Science",
    "section": "Assignments",
    "text": "Assignments\n\nMost homework assignments will be posted and returned on GitHub.\n\nThe best practice for a version control system is to commit frequently with informative commit messages. Thus, this will be formal part of your homework grade.\nI expect frequent commits. At the minimum, I expect you to commit after you have completed each question.\nI expect informative messages for each commit.\nExample good message: “Completes Question 1.2 that required tidying the college scorecard data.”\nExample bad message: “More stuff”\nLack of frequent and informative commits will result in up to a 20% reduction in an assignment grade.\n\nBecause git allows me to view your progress on an assignment, I will sometimes accept a late assignment if I see some progress and a consistent commit history in that assignment. This should only be used under extraordinary circumstances. If I do not see any progress in an assignment, I will not accept a late submission.\nTo account for life circumstances that pop up, I will also drop the lowest homework assignment grade."
  },
  {
    "objectID": "admin/syllabus.html#quizzes",
    "href": "admin/syllabus.html#quizzes",
    "title": "DATA-413/613 Data Science",
    "section": "Quizzes",
    "text": "Quizzes\n\nWe will have four closed book, closed computer, closed notes, closed calculator quizzes for a total of 50% of the grade.\nMy intention is to make these straightforward (no trick questions). But you will still have to know the material to do well on them.\nI will provide practice questions to study from.\nI plan on each quiz taking about 30 minutes to 1 hour."
  },
  {
    "objectID": "admin/syllabus.html#group-project",
    "href": "admin/syllabus.html#group-project",
    "title": "DATA-413/613 Data Science",
    "section": "Group Project",
    "text": "Group Project\nAll students will prepare a final project using the tools learned in the class. This project will be completed in groups of 2-3 students. Work with me to get your project topic approved. Tentatively, your project will involve creating an R shiny app."
  },
  {
    "objectID": "admin/syllabus.html#official-policy",
    "href": "admin/syllabus.html#official-policy",
    "title": "DATA-413/613 Data Science",
    "section": "Official Policy",
    "text": "Official Policy\n\nYou are allowed to use generative AI (e.g., ChatGPT) for homework assignments and the final project.\nHowever, you must be able to explain your work and the reasoning behind your solutions. If you cannot do so, your grade may be adjusted accordingly.\n\nIf something in your work seems off, I may ask you to meet with me. If you’re unable to explain how or why you did something, you will receive a zero on that assignment.\nThis is not a penalty for “cheating”. Since generative AI is permitted, using it is not academic misconduct. But I will grade based on your demonstrated understanding, not your submitted answers."
  },
  {
    "objectID": "admin/syllabus.html#my-rant",
    "href": "admin/syllabus.html#my-rant",
    "title": "DATA-413/613 Data Science",
    "section": "My Rant",
    "text": "My Rant\n\nGenerative AI (like ChatGPT) gives the most accurate solutions for simple problems — that is, problems you do in college.\nGenerative AI gives the least accurate solutions for hard problems — that is, problems you do in work.\nBut you need to know the simple stuff (stuff in college) in order to be able to learn the hard stuff (stuff in work).\nSo if you use generative AI for college, I think you are cheating yourself.\nData Science and Statistics are also special fields in that a degree will only get you an interview, it won’t get you a job.\nCompanies want to make sure you know the simple stuff (stuff in college) so that they can be sure you can learn the hard stuff (stuff in work).\nSo if you rely on generative AI, you won’t learn the foundational skills and you will have trouble finding a job.\nThus, I will just have more in-class assessment and lower the weight of homeworks.\nWe will have three quizzes that will be closed notes/books/computer/calculator. No reference sheet will be allowed.\nIf you rely too much on generative AI, you will likely fail those quizzes. Homeworks are the best practice for exams."
  },
  {
    "objectID": "01_git/01_basic_bash.html",
    "href": "01_git/01_basic_bash.html",
    "title": "Basic Bash",
    "section": "",
    "text": "Learning Objectives\n\nOpen up the terminal.\nBasic shell commands.\nChapter 4 of Git for Scientists.\nLinuxCommand\nTerminus.\n\n\n\nThe Command Line\n\nThe command line is like the R command prompt: you insert code, hit enter, and then the computer executes your command.\nHowever, instead of inserting R code, you insert Shell Script.\nIn this class, we will use the command line primarily for two things:\n\nMoving around your file system.\nRunning git commands.\n\nOther words for command line: shell, terminal, command line interface (cli), and console.\n\nThese terms are technically slightly different.\n\nshell: a program that takes commands from keyboard and sends them to the operating system.\nterminal: a program that opens up a window so that you can interact with the shell.\nconsole: nowadays is mostly just a synonym for a terminal, but historically meant something like the keyboard and a screen.\ncommand line interface: Any format where you interact with a computer by commands, rather than by graphics.\ncommand line: Short for command line interface, or the space where the commands are entered.\n\n\nThere are many types of shells, each with their own scripting language. We will use the bash scripting language for this class.\nA huge difference between R and bash is how commands/functions are called.\n\nR: f(x, arg = 1)\nBash: f x --arg=1\nThere are also arguments that are “flags” where its presence alters the behavior of the command. E.g., f x -g would incorporate the g flag and have a different behavior than f x which does not have the g flag.\nTypically, two dashes are for longer named commands and one dash is for shorter (one-letter) commands, like --arg versus -a.\n\nIf you are using Linux or Mac, then you can keep going. If you are using Windows, you need to first download and install git (and thus git bash) from here: http://git-scm.com/download/win. You might need to restart R Studio if you are already running it.\nOpen up the terminal\n\nWindows: Open up the Git Bash app. It should look like this:\n\nMac: On your Mac, do one of the following:\n\nType “Terminal” in the search field, then click Terminal.\nIn the Finder , open the /Applications/Utilities folder, then double-click Terminal.\nIt should look like this:\n\n\nUbuntu: Do one of the following\n\nOpen the dash and search for “terminal”. Open up the terminal.\nUse keyboard shortcut: Ctrl+Alt+T\nIt should look like this:\n\n\n\nAll commands get placed after the dollar sign.\nThe path before the dollar sign is the working directory of the terminal, not R’s working directory. It’s where the shell will reference all files from.\nThe tilde “~” is shorthand for the “home directory”. Each computer has a home directory that is the “default directory”.\n\n\n\nUseful Commands:\n\npwd: Print working directory. Show the current working directory. This is like getwd() in R.\n\npwd\n\n/Users/dgerard/Library/CloudStorage/Dropbox/teaching/stat_413_613/lectures/01_git\n\n\nls: List the current files and folders in a directory.\n\nls\n\n01_basic_bash.qmd\n01_basic_bash.rmarkdown\n01_figs\n01_git_github.qmd\n01_git_lfs.qmd\n01_git_setup.qmd\nblischak_etal_2016.PDF\ncitation.bib\n\n\ncd: Change directories. This is like setwd() in R. As when we specified paths in R, using two periods mean “move back a folder”.\n\ncd ../\npwd\n\n/Users/dgerard/Library/CloudStorage/Dropbox/teaching/stat_413_613/lectures\n\n\n\nIf you use cd without specifying a folder to move to, it will move the working directory to the home directory.\n\ncd\npwd\n\n/Users/dgerard\n\n\nOK, I’m going to move us back to the 01_git directory.\n\ncd ./Dropbox/teaching/data_496_696/lectures/01_git\n\n\nman: Read the manual of a command. Just like help() in R.\n\nman ls\n\n\nThis will open up the man page of ls. You can scroll through this page using the up and down arrows. You can exit this page by typing q.\nThis won’t work for Git Bash (for Windows users). Instead, you’ll need to type\n\nls --help\n\n\nExercise: What is your home directory? What files/folders exist in your home directory? Navigate to it and then navigate back to your notes.\nExercise: Where does the following command take you? How does it work?\n\ncd ~/../../..\n\nExercise: Read the manual page of ls. What does the a flag do? Try it out!\n\n\n\nOther commands.\n\ntouch: Create an empty file.\n\ntouch empty_file.txt\n\nmore: Open up a preview of a document. You can exit the preview by typing q.\n\nmore 01_basic_bash.Rmd\n\ncp: Copy a file.\n\ncp 01_basic_bash.Rmd hellobash.Rmd\nls\n\ncp: 01_basic_bash.Rmd: No such file or directory\n01_basic_bash.qmd\n01_basic_bash.rmarkdown\n01_figs\n01_git_github.qmd\n01_git_lfs.qmd\n01_git_setup.qmd\nblischak_etal_2016.PDF\ncitation.bib\n\n\nmv: Move/rename a file.\n\nmv hellobash.Rmd goodbyebash.Rmd\nls\n\nmv: rename hellobash.Rmd to goodbyebash.Rmd: No such file or directory\n01_basic_bash.qmd\n01_basic_bash.rmarkdown\n01_figs\n01_git_github.qmd\n01_git_lfs.qmd\n01_git_setup.qmd\nblischak_etal_2016.PDF\ncitation.bib\n\n\nrm: Remove a file.\n\nrm goodbyebash.Rmd\nls\n\nrm: goodbyebash.Rmd: No such file or directory\n01_basic_bash.qmd\n01_basic_bash.rmarkdown\n01_figs\n01_git_github.qmd\n01_git_lfs.qmd\n01_git_setup.qmd\nblischak_etal_2016.PDF\ncitation.bib\n\n\nmkdir: Make a directory/folder.\n\nmkdir tempdir\nls\n\n01_basic_bash.qmd\n01_basic_bash.rmarkdown\n01_figs\n01_git_github.qmd\n01_git_lfs.qmd\n01_git_setup.qmd\nblischak_etal_2016.PDF\ncitation.bib\ntempdir\n\n\nrmdir: Remove a directory/folder.\n\nrmdir tempdir\nls\n\n01_basic_bash.qmd\n01_basic_bash.rmarkdown\n01_figs\n01_git_github.qmd\n01_git_lfs.qmd\n01_git_setup.qmd\nblischak_etal_2016.PDF\ncitation.bib\n\n\n\n\n\nString search with grep\nFind any string in any file in the current working directory (or subdirectories or the current working directory).\n\n-r recursive\n-n line number\n-w whole word only\n-e pattern\npdfgrep (need to install separately) for searching text in PDFs.\n\n\ngrep -rnw -e \"move\"\n\n./.Rhistory:2:grep -rnwe \"move\"\n./01_basic_bash.qmd:104:  paths in R, using two periods mean \"move back a folder\".\n./01_basic_bash.qmd:109:    -   If you use `cd` without specifying a folder to move to, it will move the\n./01_basic_bash.qmd:115:    -   OK, I'm going to move us back to the 01_git directory.\n./01_basic_bash.qmd:203:grep -rnw -e \"move\"\n./01_basic_bash.rmarkdown:104:  paths in R, using two periods mean \"move back a folder\".\n./01_basic_bash.rmarkdown:109:    -   If you use `cd` without specifying a folder to move to, it will move the\n./01_basic_bash.rmarkdown:115:    -   OK, I'm going to move us back to the 01_git directory.\n./01_basic_bash.rmarkdown:203:grep -rnw -e \"move\"\n./01_git_github.qmd:64:- You can go back to previous versions of your code/text, then move forward to\n./01_git_github.qmd:221:-   Then move into your new repo\n\n\n\ngrep -rnw -e \"mov\"\n\n./01_basic_bash.qmd:207:grep -rnw -e \"mov\"\n./01_basic_bash.qmd:211:grep -rn -e \"mov\"\n./01_basic_bash.rmarkdown:207:grep -rnw -e \"mov\"\n./01_basic_bash.rmarkdown:211:grep -rn -e \"mov\"\n\n\n\ngrep -rn -e \"mov\"\n\n./.Rhistory:2:grep -rnwe \"move\"\n./.Rhistory:9:conda_remove(envname = \"r-reticulate\")\n./.Rhistory:11:conda_remove(\"bs4\")\n./01_basic_bash.qmd:104:  paths in R, using two periods mean \"move back a folder\".\n./01_basic_bash.qmd:109:    -   If you use `cd` without specifying a folder to move to, it will move the\n./01_basic_bash.qmd:115:    -   OK, I'm going to move us back to the 01_git directory.\n./01_basic_bash.qmd:171:-   `rm`: Remove a file.\n./01_basic_bash.qmd:185:-   `rmdir`: Remove a directory/folder.\n./01_basic_bash.qmd:203:grep -rnw -e \"move\"\n./01_basic_bash.qmd:207:grep -rnw -e \"mov\"\n./01_basic_bash.qmd:211:grep -rn -e \"mov\"\n./01_basic_bash.qmd:236:I'll remove that file now\n./01_basic_bash.rmarkdown:104:  paths in R, using two periods mean \"move back a folder\".\n./01_basic_bash.rmarkdown:109:    -   If you use `cd` without specifying a folder to move to, it will move the\n./01_basic_bash.rmarkdown:115:    -   OK, I'm going to move us back to the 01_git directory.\n./01_basic_bash.rmarkdown:171:-   `rm`: Remove a file.\n./01_basic_bash.rmarkdown:185:-   `rmdir`: Remove a directory/folder.\n./01_basic_bash.rmarkdown:203:grep -rnw -e \"move\"\n./01_basic_bash.rmarkdown:207:grep -rnw -e \"mov\"\n./01_basic_bash.rmarkdown:211:grep -rn -e \"mov\"\n./01_basic_bash.rmarkdown:236:I'll remove that file now\n./01_git_github.qmd:64:- You can go back to previous versions of your code/text, then move forward to\n./01_git_github.qmd:221:-   Then move into your new repo\n./01_git_github.qmd:394:- Lines after a \"`+`\" are being added. Lines after a \"`-`\" are being removed.\n\n\n\n\nDownload data with wget\nNon-interactive downloading of data.\nNot available for Git Bash for Windows.\n\n-nc Don’t download new copies if already there.\n-nd Put all files in current working directory.\n-P Tell where to download the files. Default is current working directory (.)\n-r Recursive downloading. Download all files in the directory up to a certain level.\nl Determine the level for recursive downloading.\n\nE.g. to download the HTML file that contains the Wikipedia list of theological demons, we can go\n\nwget -nc -nd https://en.wikipedia.org/wiki/List_of_theological_demons\n\n--2025-06-04 14:06:24--  https://en.wikipedia.org/wiki/List_of_theological_demons\nResolving en.wikipedia.org (en.wikipedia.org)... 208.80.154.224\nConnecting to en.wikipedia.org (en.wikipedia.org)|208.80.154.224|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 105005 (103K) [text/html]\nSaving to: ‘List_of_theological_demons’\n\n     0K .......... .......... .......... .......... .......... 48% 1.21M 0s\n    50K .......... .......... .......... .......... .......... 97% 4.03M 0s\n   100K ..                                                    100% 50.9K=0.05s\n\n2025-06-04 14:06:24 (1.91 MB/s) - ‘List_of_theological_demons’ saved [105005/105005]\n\n\n\nls\n\n01_basic_bash.qmd\n01_basic_bash.rmarkdown\n01_figs\n01_git_github.qmd\n01_git_lfs.qmd\n01_git_setup.qmd\nblischak_etal_2016.PDF\ncitation.bib\nList_of_theological_demons\n\n\nI’ll remove that file now\n\nrm List_of_theological_demons\n\n\n\nSearch your command history with reverse-i-search\n\nctrl+r to get search prompt\nType a search term\nctrl+r to cycle through matches\nHit enter if you want to reuse the match. Hit ctrl+c to exit out of the search.\n\n\n\nExercises\nExecute these commands in this order:\n\nCreate a file called “foo.txt”\nRename “foo.txt” to “bar.txt”\nCopy “bar.txt” to “foobar.txt”\nCreate a new directory called “newdir”\nMove “foobar.txt” to “newdir”\nDelete “bar.txt”\nChange the working directory to “newdir”\nDelete “foobar.txt”\nChange the working directory up one level.\nDelete the “newdir” directory."
  },
  {
    "objectID": "01_git/01_git_github.html",
    "href": "01_git/01_git_github.html",
    "title": "Git and GitHub",
    "section": "",
    "text": "Version Control with git.\nCollaboration and software-hosting with GitHub.\nGit Cheat Sheet\nAnother Git Cheat Sheet\nGit Handbook\nGit Novice"
  },
  {
    "objectID": "01_git/01_git_github.html#motivation-1-change-code-without-the-fear-of-breaking-it",
    "href": "01_git/01_git_github.html#motivation-1-change-code-without-the-fear-of-breaking-it",
    "title": "Git and GitHub",
    "section": "Motivation 1: Change code without the fear of breaking it",
    "text": "Motivation 1: Change code without the fear of breaking it\n\nYou want to try out something new, but you aren’t sure if it will work.\nNon-git solution: Copy the files\n\nanalysis.R,\nanalysis2.R,\nanalaysis3.R,\nanalysis_final.R,\nanalysis_final_final.R,\nanalysis_absolute_final.R,\nanalysis7.R\nanalysis8.R\n\nIssues:\n\nDifficult to remember differences of files.\nWhich files produced specific results?\n\nGit lets you change files, keeping track of old versions, and reverting to old versions if you decide the new changes don’t work."
  },
  {
    "objectID": "01_git/01_git_github.html#motivation-2-easy-collaboration",
    "href": "01_git/01_git_github.html#motivation-2-easy-collaboration",
    "title": "Git and GitHub",
    "section": "Motivation 2: Easy Collaboration",
    "text": "Motivation 2: Easy Collaboration\n\nIn a group setting, your collaborators might suggest how to change your analysis/code.\nFirst non-git solution: Email files back/forth.\nIssues:\n\nYou have to manually incorporate changes.\nOnly one person can work on the code at a time (otherwise multiple changes might be incompatible).\n\nSecond non-git solution: Share a Dropbox or Google Docs folder (a “centralized” version control system).\nIssues:\n\nAgain, only one person can work on the code at a time.\nLess user-friendly for tracking changes.\n\nGit let’s each individual work on their own local repository and you can automatically incorporate changes."
  },
  {
    "objectID": "01_git/01_git_github.html#motivation-3-great-for-job-interviews",
    "href": "01_git/01_git_github.html#motivation-3-great-for-job-interviews",
    "title": "Git and GitHub",
    "section": "Motivation 3: Great for job interviews",
    "text": "Motivation 3: Great for job interviews\n\nIn a 2021 Stack Overflow Survey, 93.43% of developers say they use git.\nYou can make your final-project repo public so prospective employers can view your work.\nYou can host a website on GitHub, increasing your visibility. I host my personal website and teaching websites on GitHub."
  },
  {
    "objectID": "01_git/01_git_github.html#initialize-a-repository",
    "href": "01_git/01_git_github.html#initialize-a-repository",
    "title": "Git and GitHub",
    "section": "Initialize a repository",
    "text": "Initialize a repository\n\nGit needs to be told that a folder is a repo. Otherwise, it won’t keep files under version control.\nIn this class, you won’t need to tell git this (I’ll tell git this), but in the real world you will. So we’ll go over how to do this on GitHub and on the terminal.\n\n\nOn the terminal\n\nDon’t initialize on your local for this lecture. These are just the steps you would do if you needed to initialize on your local.\nUse cd to enter the folder that you would like to keep under version control.\nThe use git init\n\ngit init\n\nThis will tell git that the folder is a single repo.\nYour files are not yet tracked. You’ll need to do the steps below to tell git which files to track. But at least git now knows that this is a repo where tracking is possible.\n\n\n\nOn GitHub\n\nGit is a version control system, GitHub is a website that hosts git repositories. (so on your resume, say that you know git, not GitHub).\nYou can create a git repo on GitHub (GitHub’s server is called the “remote”), then download (“clone”) the repo onto your computer (your computer is called the “local”).\nOn your GitHub homepage, click on “New”\n\nFill out the form. The options are pretty self-explanatory, and GitHub does a good job of providing descriptions. For this lecture, make sure\n\nRepository name is “test”.\nThe repo is set to be “Private”\nYou check “Add a README File”\n\n\nClick on “Create Repository."
  },
  {
    "objectID": "01_git/01_git_github.html#cloning",
    "href": "01_git/01_git_github.html#cloning",
    "title": "Git and GitHub",
    "section": "Cloning",
    "text": "Cloning\n\n“Cloning” is a fancy way to say download from GitHub.\nBut it also means that your local copy is connected to the remote copy automatically.\nEnter the repo you want to clone, then click on the  Button\nIf you used PAT’s, make sure “HTTPS” is highlighted.\nIf you used SSH, make sure that “SSH” is highlighted.\nThen click on the  button to copy the link.\n\nIn the terminal, navigate to where you want to download the repo, then clone it with git clone\n\ngit clone git@github.com:dcgerard/test.git\n\n\nMake sure to change the link to what you copied (don’t use my link above).\n\nThen move into your new repo\n\nls\ncd test"
  },
  {
    "objectID": "01_git/01_git_github.html#status",
    "href": "01_git/01_git_github.html#status",
    "title": "Git and GitHub",
    "section": "Status",
    "text": "Status\n\nUse git_status to see what files git is tracking and which are untracked.\n\ngit status\n\nGit should tell you that everything is up-to-date\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nnothing to commit, working tree clean\nEdit the README.md file to include your name, so that it looks something like this:\n# test\nDavid Gerard\n\nRepo for trying out GitHub.\nMake sure to save your changes.\nNow check the status again.\n\ngit status\n\nGit should be telling you that README.md has been modified, and the changes are not yet committed.\nOn branch main\nYour branch is up to date with 'origin/main'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   README.md\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nAdd a new file, called “empty.txt” by\n\ntouch empty.txt\n\nExercise: Check the status again. What do you notice?"
  },
  {
    "objectID": "01_git/01_git_github.html#staging",
    "href": "01_git/01_git_github.html#staging",
    "title": "Git and GitHub",
    "section": "Staging",
    "text": "Staging\n\nUse git add to add files to the stage.\n\ngit add README.md\n\nAlways check which files have been added:\n\ngit status\n\nUseful flags for git add:\n\n--all will stage all modified and untracked files.\n--update will stage all modified files, but only if they are already being tracked."
  },
  {
    "objectID": "01_git/01_git_github.html#committing",
    "href": "01_git/01_git_github.html#committing",
    "title": "Git and GitHub",
    "section": "Committing",
    "text": "Committing\n\nUse git commit to commit files that are staged to the commit history.\n\ngit commit -m \"Add name to README.md.\"\n\nYour message (written after the -m argument) should be concise, and describe what has been changed since the last commit.\nIf you forget to add a message, git will open up your default text-editor where you can write down a message, save the file, and exit. The commit will occur after you exit the text editor.\nIf your default text editor is vim, you can exit it using this.\ngit status should no longer have README.md as a modified file.\n\ngit status"
  },
  {
    "objectID": "01_git/01_git_github.html#history-of-changes",
    "href": "01_git/01_git_github.html#history-of-changes",
    "title": "Git and GitHub",
    "section": "History of Changes",
    "text": "History of Changes\n\nYou can use git log to see what commits you have done.\n\ngit log\n\nThere should be only two commits right now. One from GitHub and one from adding the name to README.md.\ncommit 0301eeaf74062f0b80fdb3c27a60cc5ac6f28ca7 (HEAD -&gt; main)\nAuthor: dcgerard &lt;gerard.1787@gmail.com&gt;\nDate:   Tue Nov 16 10:53:42 2021 -0500\n\n    Add name to README.md\n\ncommit fefbaffe03e0b074c33aa215d1135e6f8b68701d (origin/main, origin/HEAD)\nAuthor: David Gerard &lt;gerard.1787@gmail.com&gt;\nDate:   Tue Nov 16 10:04:47 2021 -0500\n\n    Initial commit\nExercise: Add the following line of text to “empty.txt”\nblah blah blah\nSave the output. Now stage and commit the changes."
  },
  {
    "objectID": "01_git/01_git_github.html#looking-at-differences",
    "href": "01_git/01_git_github.html#looking-at-differences",
    "title": "Git and GitHub",
    "section": "Looking at differences",
    "text": "Looking at differences\n\nAdd the following lines of text to README.md\nNever and never, my girl riding far and near\nIn the land of the hearthstone tales, and spelled asleep,\nFear or believe that the wolf in a sheep white hood\nLoping and bleating roughly and blithely leap,\nMy dear, my dear,\nOut of a lair in the flocked leaves in the dew dipped year\nTo eat your heart in the house in the rosy wood.\nAnd delete the line\nRepo for trying out GitHub.\nUse git diff to see changes in all modified files.\n\ngit diff\n\nLines after a “+” are being added. Lines after a “-” are being removed.\nYou can exit git diff by hitting q.\ngit diff won’t check for changes in the staged files by default. But you can see the differences using git diff --staged.\n\ngit diff\ngit diff --staged\n\nExercise: Stage and commit your changes."
  },
  {
    "objectID": "01_git/01_git_github.html#pushing",
    "href": "01_git/01_git_github.html#pushing",
    "title": "Git and GitHub",
    "section": "Pushing",
    "text": "Pushing\n\nUse git push to push commits to GitHub.\n\ngit push origin main\n\nDo this now.\n“origin” is the name of the remote.\n“main” is the name of the branch we are pushing to remote.\nYou can see what the remote is named by typing\n\ngit remote -v\n\nYou can see what branch you are on by\n\ngit branch"
  },
  {
    "objectID": "01_git/01_git_github.html#pulling",
    "href": "01_git/01_git_github.html#pulling",
    "title": "Git and GitHub",
    "section": "Pulling",
    "text": "Pulling\n\nIf a colleague has pushed changes to GitHub, you’ll need to pull those changes ontol your local before you can push anything to GitHub.\nThis is different than cloning. “Cloning” downloads a repo that wasn’t on your local machine. “Pulling” updates your local machine with the changes on the remote.\nUse git pull to pull changes.\n\ngit pull origin main\n\n“origin” is the name of the remote.\n“main” is the name of the branch we are pulling to.\nIf there are no changes on the remote, you’ll get the following message\nFrom github.com:dcgerard/test\n * branch            main       -&gt; FETCH_HEAD\nAlready up to date."
  },
  {
    "objectID": "01_git/01_git_github.html#branching",
    "href": "01_git/01_git_github.html#branching",
    "title": "Git and GitHub",
    "section": "Branching",
    "text": "Branching\n\nA branch is an “alternative universe” of your project, where you can experiment with new ideas (e.g. new data analyses, new data transformations, new statistical methods). After experimenting, you can then “merge” your changes back into the main branch.\nBranching isn’t just for group collaborations, you can use branching to collaborate with yourself, e.g., if you have a new idea you want to play with but do not want to have that idea in main yet.\nThe “main” branch (the default in GitHub) is your best draft. You should consider anything in “main” as the best thing you’ve got.\nThe workflow using branches consists of\n\nCreate a branch with an informative title describing its goal(s).\nAdd commits to this new branch.\nMerge the commits to main\n\n\n\nCreate a branch\n\nYou create a branch with the name &lt;branch&gt; by\n\ngit branch &lt;branch&gt;\n\nSuppose we wanted to calculate some summary statistics, but we are not sure if we want to include these in the report. Let’s create a branch where we explore these summary statistics.\n\ngit branch sumstat\n\nYou can see the list of branches (and the current branch) with\n\ngit branch\n\n\n\n\nMove between branches\n\nYou switch between branches with:\n\ngit checkout &lt;branch&gt;\n\nMove to the sumstat branch with\n\ngit checkout sumstat\n\n\n\n\nEdit Branch\n\nWhen you are on a branch, you can edit and commit as usual.\n\n\n\nPush branch to GitHub\n\nYou can push your new branch to GitHub just like you can push your main branch to GitHub:\n\ngit push origin &lt;branch&gt;\n\n\n\n\nMerge changes into main\n\nSuppose you are satisfied with your changes in your new branch, then you’ll want to merge these into the main branch. You can do this on GitHub (see here). If you do so, then don’t forget to pull the changes from main back into your local machine.\n\ngit pull origin main\n\nAlternatively, you can merge the changes in your local machine. First, checkout the main branch.\n\ngit checkout main\n\nThen use merge to merge the changes from &lt;branch&gt; into main.\n\ngit merge sumstat\n\nDon’t forget to push your changes to GitHub\n\ngit push origin main\n\n\n\n\nResolving Merge Conflicts\n\nIf two branches with incompatible histories try to merge, then git does not merge them.\nInstead, it creates a “merge conflict”, which you need to resolve.\nInstructions on resolving merge conflicts can be found here."
  },
  {
    "objectID": "01_git/01_git_lfs.html",
    "href": "01_git/01_git_lfs.html",
    "title": "Git Large File Storage",
    "section": "",
    "text": "Learning Objectives\n\nVersioning large files in git.\nGit Large File Storage\n\n\n\nMotivation\n\nGit was designed to track small changes in small text files.\nSo by default, it is not well-equipped to handle larger files (e.g. greater than 100 MB).\nBut many datasets (in particular the ones you will use for the final project) are larger.\n\n\n\nInstallation and Usage\n\nGit-LFS is an extension to git to version large files. You can install it and use it with the following steps:\nOnce per computer, install git-lfs:\n\nUbuntu : Open up the terminal and run:\n\nsudo apt-get install git-lfs\n\nMac OSX: Open up the terminal and run:\n\nbrew update\nbrew install git-lfs\n\nWindows:\n\nDownload the windows installer from: https://git-lfs.github.com/\nRun the windows installer\n\n\nOnce per repo, set up git-lfs:\n\nOpen up the terminal.\nNavigate to the repo where you want to use git-lfs.\nRun git lfs install\nSelect the types of files that you would like to place under git-lfs. These should be the large data files. For example, to place all CSV files under git-lfs, run:\n\ngit lfs track \"*.csv\"\n\nCommit the hidden file “.gitattributes”\n\ngit add .gitattributes\ngit commit -m \"Add .gitattributes\"\n\n\nUse git as you normally would.\nIf you accidently committed large files before running git lfs install, then you can retroactively update your repo to commit them with git-lfs by:\n\ngit lfs migrate import --include=\"*.csv\" --everything\n\nThe above code is for CSV files. You should change the above code based on the file type.\n\n\n\nPaying for storage\n\nGitHub charges money for storing large files.\nI used an education discount to obtain free data storage for all of the repos in our class organization. So you won’t need to pay anything.\nTo store large files for your personal repos (outside of your classwork), click on your photo at the top right of the the GitHub website. Go to Settings &gt; Billing &gt; Add more data. You can then choose the data plan you want."
  },
  {
    "objectID": "01_git/01_git_setup.html",
    "href": "01_git/01_git_setup.html",
    "title": "Setting up Git and GitHub",
    "section": "",
    "text": "Installing git\nSetting up GitHub\nConnecting to GitHub with PATs\nConnecting to GitHub with SSH"
  },
  {
    "objectID": "01_git/01_git_setup.html#generate-key-pair-on-r-studio",
    "href": "01_git/01_git_setup.html#generate-key-pair-on-r-studio",
    "title": "Setting up Git and GitHub",
    "section": "Generate Key Pair on R Studio",
    "text": "Generate Key Pair on R Studio\n\nR Studio makes it easy to generate a key pair.\n\n\nCheck to see if you already have an SSH key pair by running the following in R\n\nfile.exists(\"~/.ssh/id_rsa.pub\")\n\nIt will return FALSE if you do not have a key pair.\nOpen up Tools &gt; Global Options… &gt; Git/SVN.\nIf you do not have an SSH key pair, then click on “Create SSH Key…” and follow the prompts.\n\nClick on “View public key”\n\nCopy the entire text that shows up. This is your public key.\nGo to Add Public Key to GitHub to continue authentication setup."
  },
  {
    "objectID": "01_git/01_git_setup.html#generate-key-pair-on-the-terminal",
    "href": "01_git/01_git_setup.html#generate-key-pair-on-the-terminal",
    "title": "Setting up Git and GitHub",
    "section": "Generate Key Pair on the Terminal",
    "text": "Generate Key Pair on the Terminal\n\nIf the R Studio pipeline does not work, then try generating your key pair on the terminal with the following instructions.\n\n\nCheck for existing SSH Keys\n\nIn the terminal run\n\nls -al ~/.ssh\n\nThis will say something like “ls: cannot access ‘/c/Users/Vlad Dracula/.ssh’: No such file or directory” if you don’t have any public/private key pairs.\nThis will list out files names like “id_rsa.pub”, “id_ecdsa.pub”, or “id_ed25519.pub” if you do have a public/private key pair.\n\n\n\nGenerate a new SSH key\n\nIf you do have an SSH key, go to the next section (“Add an SSH key to the ssh-agent”)\nIf you don’t have an SSH key, follow the below steps.\nIn the terminal, run\n\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\nPress enter if prompted where to save the key to accept the default location.\nAt the prompt, type a password that you can remember.\n\n\n\nAdd an SSH key to the ssh-agent\n\nIn the terminal, run the following to start the ssh-agent in the background.\n\neval \"$(ssh-agent -s)\"\n\nIn the terminal, run the following to add your SSH private key to the ssh-agent:\n\n::: {.cell layout-align=\"center\"}\n\n```{.bash .cell-code}\nssh-add ~/.ssh/id_ed25519\n```\n:::\n\n\nCopy Public Key\n\nRun the following in the terminal to show the contents of “id_ed25519.pub”.\n\ncat ~/.ssh/id_ed25519.pub\n\nHighlight the output using your mouse and copy the contents."
  },
  {
    "objectID": "01_git/01_git_setup.html#add-public-key-to-github",
    "href": "01_git/01_git_setup.html#add-public-key-to-github",
    "title": "Setting up Git and GitHub",
    "section": "Add Public Key to GitHub",
    "text": "Add Public Key to GitHub\n\nOn GitHub, in the upper right corner, click on your profile photo and click on “Settings”\n\nOn the left sidebar, click on “SSH and GPG keys”\n\nClick on “New SSH key”\n\nIn the title field, choose a descriptive title, like “Personal Laptop”.\nPaste your key into the “Key” field.\nClick “Add SSH key” and confirm your GitHub password."
  },
  {
    "objectID": "01_git/01_git_setup.html#test-to-see-if-it-worked",
    "href": "01_git/01_git_setup.html#test-to-see-if-it-worked",
    "title": "Setting up Git and GitHub",
    "section": "Test to see if it worked",
    "text": "Test to see if it worked\n\nEnter the following in the terminal:\n\nssh -T git@github.com\n\nType “yes” if prompted to continue connecting.\nYou are successful if you see something like\nHi dcgerard! You've successfully authenticated, but GitHub does not provide shell access."
  },
  {
    "objectID": "03_vectors_iterators/03_functions.html",
    "href": "03_vectors_iterators/03_functions.html",
    "title": "Function Creation",
    "section": "",
    "text": "Creating your own functions.\nChapter 2 of HOPR\nChapter 26 of RDS"
  },
  {
    "objectID": "03_vectors_iterators/03_functions.html#learning-objectives",
    "href": "03_vectors_iterators/03_functions.html#learning-objectives",
    "title": "Function Creation",
    "section": "",
    "text": "Creating your own functions.\nChapter 2 of HOPR\nChapter 26 of RDS"
  },
  {
    "objectID": "03_vectors_iterators/03_functions.html#functions",
    "href": "03_vectors_iterators/03_functions.html#functions",
    "title": "Function Creation",
    "section": "Functions",
    "text": "Functions\n\nAll functions are of the form\nname &lt;- function(arg1, arg2 = default1, arg3 = default2) {\n  ## Code using arg1, arg2, arg3, to create result\n  return(result)\n}\nYou can choose any name you want, but they should be informative.\nYou choose the names of the arguments arg1, arg2, etc…\n\nThese are the inputs the user will use.\n\nArguments can have defaults by setting arg1 = default1, where default1 is whatever the default value of arg1 is. In the above example, arg1 has no default but arg2 and arg3 have defaults.\nYour code creates some output which I call result above.\nYou put the output in a return() call at the end of the function.\nSteps to creating a function:\n\nfigure out the logic in a simple case\nname it something meaningful - usually a verb\nlist the inputs inside function(x,y,z)\nplace code for function in a {} block\ntest your function with some different inputs\nadd error-checking of inputs\n\nCoding standards\n\nSave as text file\nIndent code\nLimit width of code (80 columns?)\nLimit the length of individual functions\nFrequent comments\n\n\nadd_two &lt;- function(a, b) {\n  return(a + b)\n}\nadd_two(2, 4)\n\n[1] 6\n\n\nExample from our book follows.\n\ndf &lt;- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a &lt;- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b &lt;- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c &lt;- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d &lt;- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n\nHow many inputs does each line have?\n\nx &lt;- df$a\n(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n\n [1] 0.78057 0.08637 0.32873 0.88008 0.03995 0.26108 0.00000 1.00000 0.75874\n[10] 0.93993\n\n# get rid of duplication\nrng &lt;- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n\n [1] 0.78057 0.08637 0.32873 0.88008 0.03995 0.26108 0.00000 1.00000 0.75874\n[10] 0.93993\n\n# make it into a function and test it\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(c(0, 5, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\nrescale01(c(1, 2, 3, NA, 5))\n\n[1] 0.00 0.25 0.50   NA 1.00\n\ndf$a &lt;- rescale01(df$a)\ndf$b &lt;- rescale01(df$b)\ndf$c &lt;- rescale01(df$c)\ndf$d &lt;- rescale01(df$d)\n\nNow, if we have a change in requirements, we only have to change it in one place. For instance, perhaps we want to handle columns that have Inf as one of the values.\n\nx &lt;- c(1:10, Inf)\nrescale01(x)\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\nrescale01 &lt;- function(x) {\n  rng &lt;- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(x)\n\n [1] 0.0000 0.1111 0.2222 0.3333 0.4444 0.5556 0.6667 0.7778 0.8889 1.0000\n[11]    Inf\n\n\nDo’s and do not’s of function naming:\n\npick either snake_case or camelCase but don’t use both\nmeaningful names (preferably verbs)\nfor a family of functions, start with the same word\ntry not to overwrite common functions or variables\nuse lots of comments in your code, particularly to explain the “why” of your code or to break up your code into sections using something like # load data --------------------\n\nExercise: Write a function that calculates the \\(z\\)-scores of a numeric vector. The \\(z\\)-score takes each value, subtracts the mean, then divides the standard deviation. It is the measure of how many standard deviations above (or below) the mean a value is.\nExercise: Write a function that takes a numeric vector as input and replaces all instances of -9 with NA.\nExercise: Write a function that takes a numeric vector and returns the coefficient of variation (the mean divided by the standard deviation).\nExercise: Write a function that takes as input a vector and returns the number of missing values.\nExercise (from RDS): Given a vector of birth dates, write a function to compute the age in years.\nExercise: Re-write the the function range(). Use functions: min(), max()\nExercise: Write both_na(), a function that takes two vectors of the same length and returns the number of positions that have an NA in both vectors. Useful functions: is.na(), sum(), logical operators.\nExercise: Read the source code for each of the following three functions, describe what they do, and then brainstorm better names.\n\nf1 &lt;- function(string, prefix) {\n  substr(string, 1, nchar(prefix)) == prefix\n}\n\nf2 &lt;- function(x) {\n  if (length(x) &lt;= 1) return(NULL)\n  x[-length(x)]\n}\n\nf3 &lt;- function(x, y) {\n  rep(y, length.out = length(x))\n}"
  },
  {
    "objectID": "03_vectors_iterators/03_iterators.html",
    "href": "03_vectors_iterators/03_iterators.html",
    "title": "Iteration",
    "section": "",
    "text": "For-loops.\nIteration.\nChapter 11 of HOPR\nPurrr Cheat Sheet.\nPurrr Overview."
  },
  {
    "objectID": "03_vectors_iterators/03_iterators.html#basic-mappings",
    "href": "03_vectors_iterators/03_iterators.html#basic-mappings",
    "title": "Iteration",
    "section": "Basic Mappings",
    "text": "Basic Mappings\n\nR is a functional programming language. Which means that you can pass functions to functions.\nSuppose on mtcars we want to calculate the column-wise mean, the column-wise median, the column-wise standard deviation, the column-wise maximum, the column-wise minimum, and the column-wise MAD. The for-loop would look very similar\n\nfunvec &lt;- rep(NA, length = length(mtcars))\nfor (i in seq_along(funvec)) {\n  funvec[i] &lt;- fun(mtcars[[i]], na.rm = TRUE) \n}\nfunvec\n\nIdeally, we would like to just tell R what function to apply to each column of mtcars. This is what the purrr package allows us to do.\npurrr is a part of the tidyverse, and so does not need to be loaded separately.\nmap_*() takes a vector (or list or data frame) as input, applies a provided function on each element of that vector, and outputs a vector of the same length.\n\nmap() returns a list.\nmap_lgl() returns a logical vector.\nmap_int() returns an integer vector.\nmap_dbl() returns a double vector.\nmap_chr() returns a character vector.\n\n\nmap_dbl(mtcars, mean)\nmap_dbl(mtcars, median)\nmap_dbl(mtcars, sd)\nmap_dbl(mtcars, mad)\nmap_dbl(mtcars, min)\nmap_dbl(mtcars, max)\n\nYou can pass on more arguments in map_*().\n\nmap_dbl(mtcars, mean, na.rm = TRUE)\n\nSuppose you want to get the output of summary() on each column.\n\nmap(mtcars, summary)\n\nExercise (RDS 21.5.3.1): Write code that uses one of the map functions to:\n\nDetermine the type of each column in nycflights13::flights.\nCompute the number of unique values in each column of palmerpenguins::penguins.\nGenerate 10 random normals for each of \\(\\mu = -10, 0, 10, \\ldots, 100\\)."
  },
  {
    "objectID": "03_vectors_iterators/03_iterators.html#shortcuts",
    "href": "03_vectors_iterators/03_iterators.html#shortcuts",
    "title": "Iteration",
    "section": "Shortcuts",
    "text": "Shortcuts\n\nInstead of specifying a built-in funciton, you can create an anonymous function to map over.\nAnonymous functions are non-named functions that are used as inputs to other functions.\nTypically, they are one-liners and are of the form\n\nfunction(args) code-using-args\n\nE.g., an anonymous function that outputs the interquartile range of a vector is\n\nfunction(x) quantile(x, 0.75) - quantile(x, 0.25)\n\nfunction (x) \nquantile(x, 0.75) - quantile(x, 0.25)\n\n\nR 4.0 and above allows for a shorter syntax for anonymous functions.\n\n\\(x) quantile(x, 0.75) - quantile(x, 0.25)\n\nfunction (x) \nquantile(x, 0.75) - quantile(x, 0.25)\n\n\nFor example, the following are three equivalent ways to calculate the mean of each column in mtcars.\n\nmap_dbl(mtcars, mean)\nmap_dbl(mtcars, function(x) mean(x))\nmap_dbl(mtcars, \\(x) mean(x))\n\nYou can think about this as purrr creating an anonymous function\n\n.f &lt;- function(.) {\n  mean(.)\n}\n\nand then calling this function in map().\n\nmap_dbl(mtcars, .f)\n\nWhy is this useful? Consider the following chunk of code which allows us to fit many simple linear regression models:\n\nmtcars |&gt;\n  nest(.by = cyl) |&gt;\n  mutate(lmout = map(data, \\(df) lm(mpg ~ wt, data = df))) -&gt;\n  sumdf\n\n\nnest(.by = cyl) will create a new data frame containing a list column of data frames, where each data frame has the same value of cyl for all units within that data frame.\n\\(df) lm(mpg ~ wt, data = df) defines a function (called an “anonymous function”) that will fit a linear model of mpg on wt where those variables are in the data frame df.\nThe map() call fits that linear model to each of the three data frames in the list-column called data created by nest().\nWhat is returned is a data frame containing a new list column called lmout that contains the three lm objects that you can use to get fits and summaries.\n\n\nsummary(sumdf$lmout[[1]])\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nResiduals:\n     1      2      3      4      5      6      7 \n-0.125  0.584  1.929 -0.690  0.355 -1.045 -1.008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    28.41       4.18    6.79   0.0011\nwt             -2.78       1.33   -2.08   0.0918\n\nResidual standard error: 1.17 on 5 degrees of freedom\nMultiple R-squared:  0.465, Adjusted R-squared:  0.357 \nF-statistic: 4.34 on 1 and 5 DF,  p-value: 0.0918\n\n\nWe can use map() to get a list of summaries.\n\nsumdf |&gt;\n  mutate(sumlm = map(lmout, summary)) -&gt;\n  sumdf\n\nsumdf$sumlm[[1]]\n\n\nCall:\nlm(formula = mpg ~ wt, data = df)\n\nResiduals:\n     1      2      3      4      5      6      7 \n-0.125  0.584  1.929 -0.690  0.355 -1.045 -1.008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    28.41       4.18    6.79   0.0011\nwt             -2.78       1.33   -2.08   0.0918\n\nResidual standard error: 1.17 on 5 degrees of freedom\nMultiple R-squared:  0.465, Adjusted R-squared:  0.357 \nF-statistic: 4.34 on 1 and 5 DF,  p-value: 0.0918\n\n\nIf you want to extract the \\(R^2\\), you can do this using map as well\n\nsumdf$sumlm[[1]]$r.squared ## only gets one R^2 out.\n\n[1] 0.4645\n\n## Gets all R^2 out\nsumdf |&gt;\n  mutate(rsquared = map_dbl(sumlm, \"r.squared\")) -&gt;\n  sumdf\n\nsumdf$rsquared\n\n[1] 0.4645 0.5086 0.4230\n\n\nExercise: A \\(t\\)-test is used to test for differences in population means. R implements this with t.test(). For example, if I want to test for differences between the mean mpg’s of automatics and manuals (coded in variable am), I would use the following syntax.\n\nt.test(mpg ~ am, data = mtcars)$p.value\n\nUse map() to get the \\(p\\)-value for this test within each group of cyl."
  },
  {
    "objectID": "03_vectors_iterators/03_iterators.html#keep-and-discard.",
    "href": "03_vectors_iterators/03_iterators.html#keep-and-discard.",
    "title": "Iteration",
    "section": "keep() and discard().",
    "text": "keep() and discard().\n\nkeep() selects all variables that return TRUE according to some function.\nE.g. let’s keep all numeric variables and calculate their means in the palmerpenguins::penguins data frame.\n\nlibrary(palmerpenguins)\n\n\nAttaching package: 'palmerpenguins'\n\n\nThe following objects are masked from 'package:datasets':\n\n    penguins, penguins_raw\n\ndata(\"penguins\")\npenguins |&gt;\n  keep(is.numeric) |&gt;\n  map_dbl(mean, na.rm = TRUE)\n\n   bill_length_mm     bill_depth_mm flipper_length_mm       body_mass_g \n            43.92             17.15            200.92           4201.75 \n             year \n          2008.03 \n\n\ndiscard() will select all variables that return FALSE according to some function.\nLet’s count the number of each value for each categorical variable:\n\npenguins |&gt;\n  discard(is.numeric) |&gt;\n  map(table)\n\n$species\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n$island\n\n   Biscoe     Dream Torgersen \n      168       124        52 \n\n$sex\n\nfemale   male \n   165    168 \n\n\nOther less useful functions are available in Section 21.9 of RDS.\nExercise: In the mtcars data frame, keep only variables that have a mean greater than 10 and calculate their mean. Hint: You’ll have to use some of the shortcuts above."
  },
  {
    "objectID": "03_vectors_iterators/03_subsetting.html",
    "href": "03_vectors_iterators/03_subsetting.html",
    "title": "Subsetting",
    "section": "",
    "text": "Learning Objectives\n\nHow to subset atomic vectors and lists.\nChapters 6 and 7 of HOPR\nChapter 4 from Advanced R\n\nThese lecture notes are mostly taken straight out of Hadley’s book. Many thanks for making my life easier.\nHis images, which I use here, are licensed under \n\n\n\n\nSubsetting an Atomic Vector\n\nSubsetting is extracting elements from an object.\n\nSubset because you only want some elements of a vector.\nSubset so you can assign new elements to that subset.\n\nSix ways to subset atomic vector.\n\nx &lt;- c(8, 1.2, 33, 14)\n\n\n\nInteger Subsetting:\n\nPut integers in brackets and it will extract those elements. R starts counting at 1.\n\nx[1]\n\n[1] 8\n\nx[c(1, 3)]\n\n[1]  8 33\n\niset &lt;- c(1, 3)\nx[iset]\n\n[1]  8 33\n\n\nThis can be used for sorting\n\norder(x)\n\n[1] 2 1 4 3\n\nx[order(x)]\n\n[1]  1.2  8.0 14.0 33.0\n\n\nYou can use duplicate integers to extract elements more than once.\n\nx[c(2, 2, 2)]\n\n[1] 1.2 1.2 1.2\n\n\n\nNegative Integer Subsetting:\n\nPutting negative integers in instead will return all elements except the negative elements.\n\nx[-1]\n\n[1]  1.2 33.0 14.0\n\nx[c(-1, -3)]\n\n[1]  1.2 14.0\n\nx[-c(1, 3)]\n\n[1]  1.2 14.0\n\n\n\nLogical Vector Subsetting:\n\nWherever there is a TRUE will return the element.\n\nx[c(TRUE, FALSE, TRUE, FALSE)]\n\n[1]  8 33\n\n\n\nNo Subsetting:\n\nEmpty brackets will return the original object.\n\nx[]\n\n[1]  8.0  1.2 33.0 14.0\n\n\n\nZero Subsetting:\n\nUsing 0 in a bracket will return a zero-length vector.\n\nx[0]\n\nnumeric(0)\n\n\n\nNames Subsetting:\n\nIf a vector has names, then you can subset using those names in quotes.\n\nnames(x) &lt;- c(\"a\", \"b\", \"c\", \"d\")\nx[\"a\"]\n\na \n8 \n\nx[c(\"a\", \"c\")]\n\n a  c \n 8 33 \n\nx[c(\"a\", \"a\")]\n\na a \n8 8 \n\n\nIf you know what names you want to remove, use setdiff().\n\nsetdiff(names(x), \"a\")\n\n[1] \"b\" \"c\" \"d\"\n\nx[setdiff(names(x), \"a\")]\n\n   b    c    d \n 1.2 33.0 14.0 \n\n\n\n\n\nExercise: Explain the output of the following\n\ny &lt;- 1:9\ny[c(TRUE, TRUE, FALSE)]\n\n[1] 1 2 4 5 7 8\n\ny[TRUE]\n\n[1] 1 2 3 4 5 6 7 8 9\n\ny[FALSE]\n\ninteger(0)\n\n\nExercise: Explain the output of the following\n\ny &lt;- c(1, 2)\ny[c(TRUE, TRUE, FALSE, TRUE, TRUE, FALSE)]\n\n[1]  1  2 NA NA\n\n\nExercise: Show all the ways to extract the second element of the following vector:\n\ny &lt;- c(af = 3, bd = 6, dd = 2)\n\nDouble brackets enforces that you are only extracting one element. This is really good in places where you know that you should only subset one element (like for-loops).\n\nx &lt;- runif(100)\nsval &lt;- 0\nfor (i in seq_along(x)) {\n  sval &lt;- sval + x[[i]]\n}\n\nDouble brackets remove attributes of the vector (even names).\n\nx &lt;- c(a = 1, b = 2)\nx[1]\n\na \n1 \n\nx[[1]]\n\n[1] 1\n\n\n\n\n\nList subsetting\n\nIf you subset a list using single brackets, you will get a sublist. You can use integers, negative integers, logicals, and names as before\n\nx &lt;- list(a = 1:3, b = \"hello\", c = 4:6)\nstr(x)\n\nList of 3\n $ a: int [1:3] 1 2 3\n $ b: chr \"hello\"\n $ c: int [1:3] 4 5 6\n\nx[1]\n\n$a\n[1] 1 2 3\n\nx[c(1, 3)]\n\n$a\n[1] 1 2 3\n\n$c\n[1] 4 5 6\n\nx[-1]\n\n$b\n[1] \"hello\"\n\n$c\n[1] 4 5 6\n\nx[c(TRUE, FALSE, FALSE)]\n\n$a\n[1] 1 2 3\n\nx[\"a\"]\n\n$a\n[1] 1 2 3\n\nx[c(\"a\", \"c\")]\n\n$a\n[1] 1 2 3\n\n$c\n[1] 4 5 6\n\n\nUsing double brackets extracts out a single element.\n\nx[[1]]\n\n[1] 1 2 3\n\nx[[\"a\"]]\n\n[1] 1 2 3\n\n\nA shorthand for using names inside double brackets is to use dollar signs.\n\nx$a\n\n[1] 1 2 3\n\n\nExericse: Why does this not work. Suggest a correction.\n\nvar &lt;- \"a\"\nx$var\n\nNULL\n\n\n\n\n\nData Frame Subsetting\n\nData frame subsetting behaves both like lists and like matrices.\n\ndf &lt;- data.frame(a = 1:3,\n                 b = c(\"a\", \"b\", \"c\"),\n                 c = 4:6)\n\nIt behaves like a list for $, [[, and [ if you only provide one index. The columns are the elements of the list.\n\ndf$a\n\n[1] 1 2 3\n\ndf[1]\n\n  a\n1 1\n2 2\n3 3\n\ndf[[1]]\n\n[1] 1 2 3\n\ndf[c(1, 3)]\n\n  a c\n1 1 4\n2 2 5\n3 3 6\n\n\nIt behaves like a matrix if you provide two indices.\n\ndf[1:2, 2]\n\n[1] \"a\" \"b\"\n\n\nYou can keep the data frame structure by using drop = FALSE.\n\ndf[1:2, 2, drop = FALSE]\n\n  b\n1 a\n2 b\n\n\nIt is common to filter by rows by using the matrix indexing.\n\ndf[df$a &lt; 3, ]\n\n  a b c\n1 1 a 4\n2 2 b 5\n\n\n\n\n\nHadley’s Advanced R Exercises\n\nFix each of the following common data frame subsetting errors:\n\nmtcars[mtcars$cyl = 4, ]\nmtcars[-1:4, ]\nmtcars[mtcars$cyl &lt;= 5]\nmtcars[mtcars$cyl == 4 | 6, ]\n\nWhy does the following code yield five missing values? (Hint: why is it different from x[NA_real_]?)\n\nx &lt;- 1:5\nx[NA]\n\n[1] NA NA NA NA NA\n\n\nWhat does upper.tri() return? How does subsetting a matrix with it work?\n\nx &lt;- outer(1:5, 1:5, FUN = \"*\")\nx[upper.tri(x)]\n\n [1]  2  3  6  4  8 12  5 10 15 20\n\n\nWhy does mtcars[1:20] return an error? How does it differ from the similar mtcars[1:20, ]?\nAn lm object is a list-like object. Given a linear model, e.g., mod &lt;- lm(mpg ~ wt, data = mtcars), extract the residual degrees of freedom. Then extract the R squared from the model summary (summary(mod)).\n\n\n\nSubassignment\n\nAll subsetting operators can be used to assign subsets of a vector new values. This is called subassignment.\n\nx &lt;- 1:5\nx[[2]] &lt;- 200\nx\n\n[1]   1 200   3   4   5\n\nx[c(1, 3)] &lt;- 0\nx\n\n[1]   0 200   0   4   5\n\nx[x == 0] &lt;- NA_real_\nx\n\n[1]  NA 200  NA   4   5\n\ny &lt;- list(a = 1:3,\n          b = \"hello\",\n          c = 4:6)\ny$a &lt;- \"no way\"\ny\n\n$a\n[1] \"no way\"\n\n$b\n[1] \"hello\"\n\n$c\n[1] 4 5 6\n\n\nRemove a list element with NULL.\n\ny[[1]] &lt;- NULL\ny\n\n$b\n[1] \"hello\"\n\n$c\n[1] 4 5 6\n\ny$b &lt;- NULL\ny\n\n$c\n[1] 4 5 6\n\n\n\n\n\nExercises\nThese are just meant to buff up your Base R skills. Consider the data from the {Sleuth3} package that contains information on sex and salary at a bank. Try to use just base R methods.\n\nlibrary(Sleuth3)\ndata(\"case0102\")\nsal &lt;- case0102\n\n\nWhat is the salary of the person in the 51st row? Use two different subsetting strategies to get this.\nWhat is the mean salary of Male’s?\nHow many Females are in the data?\nHow many Females make over $6000?"
  },
  {
    "objectID": "03_vectors_iterators/03_vectors.html",
    "href": "03_vectors_iterators/03_vectors.html",
    "title": "Vectors",
    "section": "",
    "text": "Understanding vectors, the fundamental objects in R.\nThis is mostly a long list of facts about vectors that you should be aware of.\nChapter 5 of HOPR\nChapter 3 from Advanced R\n\nThese lecture notes are mostly taken straight out of Hadley’s book. Many thanks for making my life easier.\nHis images, which I use here, are licensed under \n\nThe topic should be mostly review, but we will go a little deeper."
  },
  {
    "objectID": "03_vectors_iterators/03_vectors.html#atomic-vectors",
    "href": "03_vectors_iterators/03_vectors.html#atomic-vectors",
    "title": "Vectors",
    "section": "Atomic Vectors",
    "text": "Atomic Vectors\n\nFour basic types:\n\nLogical: Either TRUE or FALSE\nInteger:\n\nExactly an integer. Assign them by adding L behind it (for “long integer”).\n-1L, 0L, 1L, 2L, 3L, etc…\n\nDouble:\n\nDecimal numbers.\n1, 1.0, 1.01, etc…\nInf, -Inf, and NaN are also doubles.\n\nCharacter:\n\nAnything in quotes:\n\"1\", \"one\", \"1 won one\", etc…\n\n\nYou create vectors with c() for “combine”\n\nx &lt;- c(TRUE, TRUE, FALSE, TRUE) ## logical\nx &lt;- c(1L, 1L, 0L, 1L) ## integer\nx &lt;- c(1, 1, 0, 1) ## double\nx &lt;- c(\"1\", \"1\", \"0\", \"1\") ## character\n\nThere are no scalars in R. A “scalar” is just a vector length 1.\n\nis.vector(TRUE)\n\n[1] TRUE\n\n\nIntegers and doubles are together called “numerics”\n\nYou can determine the type with typeof().\n\nx &lt;- c(TRUE, FALSE)\ntypeof(x)\n\n[1] \"logical\"\n\nx &lt;- c(0L, 1L)\ntypeof(x)\n\n[1] \"integer\"\n\nx &lt;- c(0, 1)\ntypeof(x)\n\n[1] \"double\"\n\nx &lt;- c(\"0\", \"1\")\ntypeof(x)\n\n[1] \"character\"\n\n\nThe special values, Inf, -Inf, and NaN are doubles\n\ntypeof(c(Inf, -Inf, NaN))\n\n[1] \"double\"\n\n\nDetermine the length of a vector using length()\n\nlength(x)\n\n[1] 2\n\n\nMissing values are represented by NA.\nNA is technically a logical value.\n\ntypeof(NA)\n\n[1] \"logical\"\n\n\nThis rarely matters because logicals get coerced to other types when needed.\n\ntypeof(c(1L, NA))\n\n[1] \"integer\"\n\ntypeof(c(1, NA))\n\n[1] \"double\"\n\ntypeof(c(\"1\", NA))\n\n[1] \"character\"\n\n\nBut if you need missing values of other types, you can use\n\nNA_integer_ ## integer NA\nNA_real_ ## double NA\nNA_character_ ## character NA\n\nNever use == when testing for missingness. It will return NA since it is always unknown if two unknowns are equal. Use is.na().\n\nx &lt;- c(NA, 1)\nx == NA\n\n[1] NA NA\n\nis.na(x)\n\n[1]  TRUE FALSE\n\n\nYou can check the type with is.logical(), is.integer(), is.double(), and is.character().\n\nis.logical(TRUE)\n\n[1] TRUE\n\nis.integer(1L)\n\n[1] TRUE\n\nis.double(1)\n\n[1] TRUE\n\nis.character(\"1\")\n\n[1] TRUE\n\n\nAttempting to combine vectors of different types coerces them to the same type. The order of preference is character &gt; double &gt; integer &gt; logical.\n\ntypeof(c(1L, TRUE))\n\n[1] \"integer\"\n\ntypeof(c(1, 1L))\n\n[1] \"double\"\n\ntypeof(c(\"1\", 1))\n\n[1] \"character\"\n\n\nExercise (from Advanced R): Predict the output:\n\nc(1, FALSE)\nc(\"a\", 1)\nc(TRUE, 1L)\n\nExercise (from Advanced R): Explain these results:\n\n1 == \"1\"\n\n[1] TRUE\n\n-1 &lt; FALSE\n\n[1] TRUE\n\n\"one\" &lt; 2\n\n[1] FALSE"
  },
  {
    "objectID": "03_vectors_iterators/03_vectors.html#names",
    "href": "03_vectors_iterators/03_vectors.html#names",
    "title": "Vectors",
    "section": "Names",
    "text": "Names\n\nNames are a character vector the same length as the atomic vector. Each name corresponds to a single element.\nYou could set names using attr(), but you should not.\n\nx &lt;- 1:3\nattr(x, \"names\") &lt;- c(\"a\", \"b\", \"c\")\nattributes(x)\n\n$names\n[1] \"a\" \"b\" \"c\"\n\n\nNames are so special, that there are special ways to create them and view them\n\nx &lt;- c(a = 1, b = 2, c = 3)\nnames(x)\n\n[1] \"a\" \"b\" \"c\"\n\nx &lt;- 1:3\nnames(x) &lt;- c(\"a\", \"b\", \"c\")\nnames(x)\n\n[1] \"a\" \"b\" \"c\"\n\n\nThe proper way to think about names is like this:\n\nBut each name corresponds to a specific element, so Hadley does it like this:\n\nNames stay with single bracket subsetting (not double bracket subsetting)\n\nnames(x[1])\n\n[1] \"a\"\n\nnames(x[1:2])\n\n[1] \"a\" \"b\"\n\nnames(x[[1]])\n\nNULL\n\n\nNames can be used for subsetting (more in Chapter 4)\n\nx[[\"a\"]]\n\n[1] 1\n\n\nYou can remove names with unname().\n\nunname(x)\n\n[1] 1 2 3"
  },
  {
    "objectID": "04_packages/04_checking.html",
    "href": "04_packages/04_checking.html",
    "title": "Automatic Checks",
    "section": "",
    "text": "Package Checking\nContinuous Integration\nChapter 19 from R Packages"
  },
  {
    "objectID": "04_packages/04_checking.html#continuous-integration",
    "href": "04_packages/04_checking.html#continuous-integration",
    "title": "Automatic Checks",
    "section": "Continuous Integration",
    "text": "Continuous Integration\n\nYou can set up GitHub Actions so that it will run R CMD check on multiple virtual machines (Windows, Mac, or Ubuntu) each time you push. This is really great for making sure your package is robust and constantly being checked.\nAutomatic checking each time you make a change is called continuous integration.\nIn a package, run\n\nusethis::use_github_action_check_standard()\n\nRunning this will create a new file in a hidden folder via the path “.github/workflows/R-CMD-check.yaml”. This YAML file contains instructions for setting up a virtual machine, installing R and your dependencies, and running R CMD check.\nTo use it, simply commit your files and push to GitHub, then wait for the checks to run. You can see their progress by clicking on the “Actions” tab on the GitHub page of your package.\nIt’s not too important to know what that file does, but there are some parts that you may need to edit.\nYou may comment out one of the operating systems for the check if you know that the error is artificial. Use # for comments in a YAML file. Below, I comment out the Mac.\nstrategy:\n  fail-fast: false\n  matrix:\n    config:\n      - {os: windows-latest, r: 'release'}\n      # - {os: macOS-latest, r: 'release'}\n      - {os: ubuntu-20.04, r: 'release', rspm: \"https://packagemanager.rstudio.com/cran/__linux__/focal/latest\"}\n      - {os: ubuntu-20.04, r: 'devel', rspm: \"https://packagemanager.rstudio.com/cran/__linux__/focal/latest\"}\nSometimes (but rarely) you need to fix the install code for the dependencies. Onetime {remotes} was failing to install the correct Bioconductor packages I needed, so I had to edit it this way:\n\n- name: Install dependencies\n  run: |\n    remotes::install_deps(dependencies = TRUE)\n    remotes::install_cran(\"rcmdcheck\")\n    install.packages(\"BiocManager\") # new line\n    BiocManager::install(\"VariantAnnotation\") # new line\n  shell: Rscript {0}\n\nYou can see a variety of other YAML files at https://github.com/r-lib/actions/tree/v1/examples"
  },
  {
    "objectID": "04_packages/04_packages.html",
    "href": "04_packages/04_packages.html",
    "title": "R Packages",
    "section": "",
    "text": "Structure of an R package.\nDocumenting R packages.\nWorkflow for building R packages.\nRequired: Chapters 1–10 and 13–14 from R Packages.\nResource: Writing R Extensions"
  },
  {
    "objectID": "04_packages/04_packages.html#create-a-package-skeleton",
    "href": "04_packages/04_packages.html#create-a-package-skeleton",
    "title": "R Packages",
    "section": "Create a package skeleton",
    "text": "Create a package skeleton\n\nYou can create a package skeleton with the usethis::create_package().\nBefore running this, change your working directory to where you want to create your R package with “Session &gt; Set Working Directory &gt; Choose Directory…”.\nThis will be the “source” state of the package, so you can choose it to be almost anywhere on your computer.\nChoose a location that is not inside an RStudio project, another R package, another git repo, or inside an R library.\nThen just type\n\nusethis::create_package(path = \".\")\n\nI don’t like RStudio projects, so I typically run\n\nusethis::create_package(path = \".\", rstudio = FALSE, open = FALSE)\n\nYou can use RStudio Projects if you want. But I won’t help with any issues you have with RStudio projects.\nExample: For this lecture, we will create a simple R package called forloop that reproduces some Base R functions using for-loops. Create a folder called “forloop”, set the working directory to this folder, and run\n\nusethis::create_package(path = \".\", rstudio = FALSE, open = FALSE)"
  },
  {
    "objectID": "04_packages/04_packages.html#formatting",
    "href": "04_packages/04_packages.html#formatting",
    "title": "R Packages",
    "section": "Formatting",
    "text": "Formatting\n\nWithin {roxygen2} blocks, you can format your documentation with LaTeX-style code:\n\\emph{italics}: italics.\n\\strong{bold}: bold.\n\\code{fn()}: formatted in-line code\n\\link{fn}: Link to the documentation offn(). Do *not* include parentheses inside`\nI often do \\code{\\link{fn}()} when I link to a function so that it is both linked, code-formatted, and has a parentheses.\nTo link to functions in other packages, use \\link[pkg]{fn}\n\\url{}: Include a URL.\n\\href{www}{word}: Provide a hyperlink.\n\\email{}: Provide an email.\n\\doi{}: Provide the DOI of a paper (with a link to that paper).\nYou can make an itemized list with\n\n#' \\itemize{\n#'   \\item Item 1\n#'   \\item Item 2\n#' }\n\nYou can make an enumerated list with\n\n#' \\enumerate{\n#'   \\item Item 1\n#'   \\item Item 2\n#' }\n\nYou can make a named list with\n\n#' \\describe{\n#'   \\item{Name 1}{Item 1}\n#'   \\item{Name 2}{Item 2}\n#' }\n\nExample: Let’s work together to document our col_means() function.\nExercise: Document sum2() and count_na(). Make sure to include the following tags @title, @description, @details, @param, @return, @author, and @examples.\nExercise: In the @seealso tag, provide a link to each function. Also provide a link to base::sum(). Use an itemized list."
  },
  {
    "objectID": "04_packages/04_packages.html#exporting",
    "href": "04_packages/04_packages.html#exporting",
    "title": "R Packages",
    "section": "Exporting",
    "text": "Exporting\n\nInclude the following tag in the documentation of any function that you want a user to have access to.\n\n@export\n\nThis will add the function to the “NAMESPACE” file (which you should not edit by hand).\nNote devtools::load_all() will attach all functions from your package so you can test them out. But if a user installs your package and uses library(), they will only have access to exported functions.\nYou should only export functions you want others to use.\nExporting a function means that you have to be very wary about changing it in future versions since that might break other folks’ code.\nExercise: Look at the “NAMESPACE” file in {forloop}. Now export all of your functions in {forloop}. Look at the “NAMESPACE” file again."
  },
  {
    "objectID": "04_packages/04_packages.html#importing",
    "href": "04_packages/04_packages.html#importing",
    "title": "R Packages",
    "section": "Importing",
    "text": "Importing\n\nNever use library() or require() in an R package.\nPackage dependencies go in the DESCRIPTION folder. You can tell R that your package depends on another package by running:\n\nusethis::use_package()\n\nThis will make it so that the package is available when your package is installed.\nThen, you call functions from other packages via package::function(), where package is the name of the package and function() is the function name from package.\nYou can suggest (but not require) a package to be installed. This is usually done if the functions from the suggested package are not vital, or the suggested package is hard to install (or has a lot of dependencies itself). To do so, also use usethis::use_package() with type = \"Suggests\".\nIf you suggest a package, whenever you use a function from that package you should first check to see if it is installed with requireNamespace(\"pkg\", quietly = TRUE). E.g.\n\nif (requireNamespace(\"pkg\", quietly = TRUE)) {\n  pkg::fun()\n}"
  },
  {
    "objectID": "04_packages/04_packages.html#technical-notes-on-importing",
    "href": "04_packages/04_packages.html#technical-notes-on-importing",
    "title": "R Packages",
    "section": "Technical notes on importing",
    "text": "Technical notes on importing\n\nImporting functions from a package is different than including a package in the “Imports” field of the DESCRIPTION file.\n\nImporting a function attaches it so that you do not need to use ::.\nIncluding a package in the Imports field makes sure the package is installed.\n\nThe importing part of a namespace is important to avoid ambiguity.\nE.g. many packages use c(). We can (rather foolishly) redefine it via\n\n## Don't run this\nc &lt;- function(x) {\n  return(NA)\n}\n\nand no package will be affected because they all import c() from the {base} R package.\nSearch Path: The list of packages that R will automatically search for functions. See your search path by search().\n\nsearch()\n\n[1] \".GlobalEnv\"        \"package:stats\"     \"package:graphics\" \n[4] \"package:grDevices\" \"package:utils\"     \"package:datasets\" \n[7] \"package:methods\"   \"Autoloads\"         \"package:base\"     \n\n\nLoading a package: Put a package in memory, but do not put it in the search path. You can then use the function by using :: as in dplyr::mutate(). If you call a function via :: then it will load that package automatically.\nAttaching a package: Both load a package and place it in the search path, so you don’t need to use :: to call a function from that package. This is what library() does.\nIf you import a package (via the DESCRIPTION file), then it loads it, it does not attach it, so you need to use ::.\nIf you import a function (via the namespace), then it attaches it, so you do not need to use ::.\nGenerally, I do not recommend importing functions. but you can do it by using @importFrom anywhere in your package.\nE.g. adding this line anywhere in your package will attach dplyr::mutate() whenever your package is attached.\n\n#' @importFrom dplyr mutate\n\nWhy do I recommend rarely using @importFrom? Because that could make coding more complicated for your users\n\nE.g. if you import dplyr::lag() then when a user attached your package, R will now think lag() comes from {dplyr} and not {stats}, which could break the user’s code."
  },
  {
    "objectID": "04_packages/04_packages.html#practical-suggestions",
    "href": "04_packages/04_packages.html#practical-suggestions",
    "title": "R Packages",
    "section": "Practical suggestions",
    "text": "Practical suggestions\n\nYou should try to have as few dependencies as possible. When packages change, that can affect (or break) your package, which means more work on your part to maintain your package.\nTry to avoid dependencies on new packages or on packages from folks who do not have a history of maintaining their packages.\nTry to avoid dependencies on packages from the tidyverse (dplyr, tidyr, etc). These packages have changed frequently in the past. The maintainers are great about notifying folks about breaking changes, but it still means more work on your part.\n\nE.g., if you only use string manipulation in one spot in your package, try using grepl() instead of stringr::str_detect().\n\nHere is a list of nice essays on limiting dependencies: https://www.tinyverse.org/\nIf you do import functions from other packages, put all of those {roxygen2} tags in the same location in one file.\nExample: Together, let’s modify our col_means() function to one called col_stats() that also allows for calculating the standard deviation. However, sd() comes from the {stats} package, and so we need to make sure to tell R which package it is from.\nExercise: Instead of using count_na(), you decide to use the n_na() function from the {na.tools} package. Make these edits to your package now.\nExercise: Now import n_na() and remove your use of :: from the previous exercise."
  },
  {
    "objectID": "04_packages/04_packages.html#external-data",
    "href": "04_packages/04_packages.html#external-data",
    "title": "R Packages",
    "section": "External data",
    "text": "External data\n\nExternal data is available to the user. For example, the mpg dataset from the {ggplot2} is available to us by running\n\ndata(\"mpg\", package = \"ggplot2\")\nstr(mpg)\n\ntibble [234 × 11] (S3: tbl_df/tbl/data.frame)\n $ manufacturer: chr [1:234] \"audi\" \"audi\" \"audi\" \"audi\" ...\n $ model       : chr [1:234] \"a4\" \"a4\" \"a4\" \"a4\" ...\n $ displ       : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ...\n $ year        : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ...\n $ cyl         : int [1:234] 4 4 4 4 6 6 6 4 4 4 ...\n $ trans       : chr [1:234] \"auto(l5)\" \"manual(m5)\" \"manual(m6)\" \"auto(av)\" ...\n $ drv         : chr [1:234] \"f\" \"f\" \"f\" \"f\" ...\n $ cty         : int [1:234] 18 21 20 21 16 18 18 18 16 20 ...\n $ hwy         : int [1:234] 29 29 31 30 26 26 27 26 25 28 ...\n $ fl          : chr [1:234] \"p\" \"p\" \"p\" \"p\" ...\n $ class       : chr [1:234] \"compact\" \"compact\" \"compact\" \"compact\" ...\n\n\nTo include data in a package, simply add it, in the format of an RData file, in a directory called “data”.\nYou can use usethis::use_data() to save a dataset in the “data” directory. The first argument is the data you want to save.\nYou should document your dataset, using roxygen, in a separate R file in the “R” directory. Typically, folks document all of their data in “R/data.R”.\nInstead of a function declaration, you just type the name of the dataset. E.g., from my {updog} R package I have the following documentation for the snpdat tibble.\n\n#' @title GBS data from Shirasawa et al (2017)\n#'\n#' @description Contains counts of reference alleles and total read counts \n#' from the GBS data of Shirasawa et al (2017) for\n#' the three SNPs used as examples in Gerard et. al. (2018).\n#'\n#' @format A \\code{tibble} with 419 rows and 4 columns:\n#' \\describe{\n#'     \\item{id}{The identification label of the individuals.}\n#'     \\item{snp}{The SNP label.}\n#'     \\item{counts}{The number of read-counts that support the reference allele.}\n#'     \\item{size}{The total number of read-counts at a given SNP.}\n#' }\n#'\n#' @source \\doi{10.1038/srep44207}\n#'\n#' @references\n#' \\itemize{\n#'   \\item{Shirasawa, Kenta, Masaru Tanaka, Yasuhiro Takahata, Daifu Ma, Qinghe Cao, Qingchang Liu, Hong Zhai, Sang-Soo Kwak, Jae Cheol Jeong, Ung-Han Yoon, Hyeong-Un Lee, Hideki Hirakawa, and Sahiko Isobe \"A high-density SNP genetic map consisting of a complete set of homologous groups in autohexaploid sweetpotato (Ipomoea batatas).\" \\emph{Scientific Reports 7} (2017). \\doi{10.1038/srep44207}}\n#'   \\item{Gerard, D., Ferrão, L. F. V., Garcia, A. A. F., & Stephens, M. (2018). Genotyping Polyploids from Messy Sequencing Data. \\emph{Genetics}, 210(3), 789-807. \\doi{10.1534/genetics.118.301468}.}\n#' }\n#'\n\"snpdat\"\n\nNever export a dataset (via the @export tag).\nThe @format tag is useful for describing how the data are structured.\nThe @source tag is useful to describe the URL/papers/collection process for the data."
  },
  {
    "objectID": "04_packages/04_packages.html#internal-data",
    "href": "04_packages/04_packages.html#internal-data",
    "title": "R Packages",
    "section": "Internal Data",
    "text": "Internal Data\n\nTo use pre-computed data, you place all internal data in the “R/sysdata.rda” file.\nusethis::use_data() will do this automatically if you use the internal = TRUE argument.\nE.g. the following will put x and y in “R/sysdata.rda”\n\nx &lt;- c(1, 10, 100)\ny &lt;- data.frame(hello = c(\"a\", \"b\", \"c\"), goodbye = 1:3)\nusethis::use_data(x, y, internal = TRUE)\n\nYou can use internal data in a package as you normally would use an object that is loaded into memory.\nExercise: Create a function called fib() that takes as input n and returns the nth Fibonacci number. Recall that the sequence is \\[\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, \\ldots\n\\] where the next number is the sum of the previous two numbers. Put this function in a new R script file (called “fib.R”) and make sure your function is well documented.\nExercise: Save the first 1000 Fibonacci numbers as a vector for internal data. Then create a function called fib1000() that just looks up the nth Fibonacci number from this internal vector."
  },
  {
    "objectID": "04_packages/04_style.html",
    "href": "04_packages/04_style.html",
    "title": "Style Guide",
    "section": "",
    "text": "Coding Style\ntidyverse style guide\nGoogle style guide\nBioconductor Style Guide"
  },
  {
    "objectID": "04_packages/04_style.html#names",
    "href": "04_packages/04_style.html#names",
    "title": "Style Guide",
    "section": "Names",
    "text": "Names\n\nOnly use lower-case snake_case.\n\nGood\n\nred_apple\n\nBad\n\nRed_apple\nred.apple\nredApple\nRedApple\n\n\nVariables should be nouns and functions should be verbs\nNever use single letters as variables/functions\n\nGood:\n\nnum_sim &lt;- 10\n\nBad\n\nsimulate &lt;- 10 ## verb\nx &lt;- 10 ## single letter\n\n\nExceptions: Some letters are standard. Such as n for the sample size in rnorm(), runif(), etc…"
  },
  {
    "objectID": "04_packages/04_style.html#commas",
    "href": "04_packages/04_style.html#commas",
    "title": "Style Guide",
    "section": "Commas",
    "text": "Commas\n\nAlways put a space after a comma, not before (like English).\n\nGood:\n\nmat[1, ]\n\nBad:\n\nmat[1 ,]\nmat[1 , ]\nmat[1,]"
  },
  {
    "objectID": "04_packages/04_style.html#parentheses",
    "href": "04_packages/04_style.html#parentheses",
    "title": "Style Guide",
    "section": "Parentheses",
    "text": "Parentheses\n\nDon’t put a space in or around parentheses for functions.\n\nGood:\n\nmean(x)\n\nBad:\n\nmean (x)\nmean(x )\n\n\nPut spaces around parentheses for if statements, and for and while loops.\n\nGood:\n\nif (x) {\n\n}\n\nBad:\n\nif(x){\n\n}\n\n\nPut a space only after () for function creations.\n\nGood:\n\nsim &lt;- function(x) {\n}\n\nBad:\n\nsim &lt;- function (x) {\n}\n\nsim &lt;- function(x){\n}"
  },
  {
    "objectID": "04_packages/04_style.html#curley-braces",
    "href": "04_packages/04_style.html#curley-braces",
    "title": "Style Guide",
    "section": "Curley Braces",
    "text": "Curley Braces\n\nWhenever you use curly braces {}, the opening brace should be the last character on a line, and the closing brace should be the first character on a line.\n\nGood:\n\nif (condition) {\n  dostuff()\n}\n\nBad\n\nif (condition)\n{\n  dostuff()\n}\n\nif (condition) {\n  dostuff() }"
  },
  {
    "objectID": "04_packages/04_style.html#if-else",
    "href": "04_packages/04_style.html#if-else",
    "title": "Style Guide",
    "section": "if-else",
    "text": "if-else\n\nelse statements should be on the same line as a closing brace.\n\nGood:\n\nif (condition) {\n\n} else if (condition2) {\n\n} else {\n\n}\n\n\nOnly use ifelse() where vectorization is important. If condition should be length 1, then use full if-else statements.\nIn a if-then statement, use || or &&, not | or &, since the latter two vectorize operations."
  },
  {
    "objectID": "04_packages/04_style.html#infix-characters",
    "href": "04_packages/04_style.html#infix-characters",
    "title": "Style Guide",
    "section": "Infix Characters",
    "text": "Infix Characters\n\nAn infix operator is one where arguments on both sides of it are used in a function. The alternative is prefix notation. Compare\n\n5 + 10 ## infix notation\n\n[1] 15\n\n`+`(5, 10) ## prefix notation\n\n[1] 15\n\n\nPut spaces around all infix characters ==, +, -, *, /, ^, |&gt;, etc…\n\nGood:\n\nx + 10\n\nBad:\n\nx+10\nx+ 10\nx +10\n\n\nExceptions: ::, :::, $, @, [, [[, unary -, unary +, :, and ?.\n\nE.g. do ggplot2::qplot() or -1, not ggplot2 :: qplot() and - 1"
  },
  {
    "objectID": "04_packages/04_style.html#code-length",
    "href": "04_packages/04_style.html#code-length",
    "title": "Style Guide",
    "section": "Code Length",
    "text": "Code Length\n\nNo lines should be greater than 80 characters.\nTo get a vertical line displaying the code length, in R studio go to “Tools &gt; Global Options… &gt; Code &gt; Display”. Make sure “Show margin” is checked with “80” in the text box.\nIf a function call/definition is too long, break up arguments on new line.\n\nthis &lt;- is_a_very_long_function_call(\n  that = \"is\",\n  broken = \"up\",\n  into = \"many\",\n  indented = \"lines\",\n  that = \"are\",\n  easier = \"to\",\n  read = NULL\n)"
  },
  {
    "objectID": "04_packages/04_style.html#other-things",
    "href": "04_packages/04_style.html#other-things",
    "title": "Style Guide",
    "section": "Other things",
    "text": "Other things\n\nAlways use &lt;- for assignment, not =.\nAlways use \" for strings, not '.\nAlways use TRUE or FALSE, not T or F\n\nT and F are aliases for TRUE and FALSE, and so may be overwritten by the user, which is scary.\n\nDon’t include non-ASCII characters in your code.\n\nASCII characters are lower case letters (a through z), upper case letters (A through Z), digits (0 through 9), and common punctuation.\nIncluding non-ASCII characters will give you a CRAN note.\nNon-ASCII characters usually show up when you copy and paste from the web. E.g. the following look normal but are non-ASCII (and are all different):\n\nEn Dash: “–”\nEm Dash: “—”\nHorizontal Bar: “―”\nEn Quad: “ ”\nEm Quad: “ ”\nEn Space: “ ”\nEm Space: “ ”\n\nIf you accidentally include such characters, you can find them with\n\ntools::showNonASCIIfile()"
  },
  {
    "objectID": "04_packages/04_style.html#function-argument-length",
    "href": "04_packages/04_style.html#function-argument-length",
    "title": "Style Guide",
    "section": "Function Argument Length",
    "text": "Function Argument Length\n\nIf you have a lot of arguments, indent the arguments on new lines.\n\nrun_me &lt;- function(this,\n                   is,\n                   a = \"lot\",\n                   of = \"arguments\",\n                   that = \"are longer than 80 characters\") {\n}"
  },
  {
    "objectID": "04_packages/04_style.html#function-length",
    "href": "04_packages/04_style.html#function-length",
    "title": "Style Guide",
    "section": "Function Length",
    "text": "Function Length\n\nYou should break up your functions into discrete tasks.\n\nReduces duplicating code, so less prone to bugs.\nAllows you to think more modularly about tasks, which makes code easier to reason about.\nMakes it easier to combine code in new ways.\n\nTo force you to do this, make all functions be less than 50 lines. This is what Bioconductor does."
  },
  {
    "objectID": "04_packages/04_style.html#explicit-returns",
    "href": "04_packages/04_style.html#explicit-returns",
    "title": "Style Guide",
    "section": "Explicit returns",
    "text": "Explicit returns\n\nIn R, the last value evaluated in a function will be implicitly returned. I think this is bad practice since it makes it harder to reason about what R is returning. So always include a return() statement. Never do\n\nadd_two &lt;- function(x, y) {\n  x + y\n}\n\nAlways do\n\nadd_two &lt;- function(x, y) {\n  return(x + y)\n}"
  },
  {
    "objectID": "04_packages/04_style.html#importing",
    "href": "04_packages/04_style.html#importing",
    "title": "Style Guide",
    "section": "Importing",
    "text": "Importing\n\nNever use the @import tag in a package to bring all of a package’s exported functions into the NAMESPACE. This creates too much risk for name collision.\nIn a package, never import functions, always type the package where the function came from. This makes it easier to reason about namespaces. Never do\n\n#' @importFrom ggplot2 qplot\nplot_red &lt;- function(x, y) {\n  qplot(x, y, color = I(\"red\"))\n}\n\nAlways do\n\nplot_red &lt;- function(x, y) {\n  ggplot2::qplot(x, y, color = I(\"red\"))\n}\n\nExceptions:\n\nYou will have to import infix functions (surrounded by percent signs). Such as\n\n#' @importFrom magrittr %&gt;%\n#' @importFrom foreach %dopar%\n\nThere is a small performance penalty for using :: (about 5 µs). So import a function if you are iterating it \\(\\sim\\) million times, and each iteration takes on the order of 1 ns."
  },
  {
    "objectID": "04_packages/04_style.html#order-of-arguments",
    "href": "04_packages/04_style.html#order-of-arguments",
    "title": "Style Guide",
    "section": "Order of Arguments",
    "text": "Order of Arguments\n\nAlways place arguments with defaults after arguments without defaults.\nGood:\n\nfunction(arg1, arg2, arg3 = NULL) {\n\n}\n\nBad:\n\nfunction(arg1, arg3 = NULL, arg2) {\n\n}"
  },
  {
    "objectID": "04_packages/04_testing.html",
    "href": "04_packages/04_testing.html",
    "title": "Assertions and Unit Tests",
    "section": "",
    "text": "Assertions\nUnit Tests\nChapter 12 from R Packages"
  },
  {
    "objectID": "04_packages/04_testing.html#expectation",
    "href": "04_packages/04_testing.html#expectation",
    "title": "Assertions and Unit Tests",
    "section": "Expectation",
    "text": "Expectation\n\nAn expectation returns an error if a function or result is not what you expect.\nIn {testthat} all expectations begin with expect_.\nThe first argument is the actual result of a function in your package. The second argument is the expected result.\nThe most common expectation is to test for equality with expect_equal().\n\nx &lt;- 10\ny &lt;- 10\nexpect_equal(x, y)\n\nYou can specify the tolerance level so for items that are only approximately equal\n\nexpect_equal(10, 10 + 10^-8)\nexpect_equal(10, 10 + 10^-5)\n\nError: 10 (`actual`) not equal to 10 + 10^-5 (`expected`).\n\n  `actual`: 10.000000\n`expected`: 10.000010\n\nexpect_equal(10, 10 + 10^-5, tolerance = 10^-4)\n\nMake sure to only check for equality between two things. If you provide three unnamed arguments, the third one is interpreted as the tolerance. This is a common error that I have done many times.\n\n## Bad\nexpect_equal(10, 10, 10)\n\nWarning: Unused arguments (10)\n\n## Because this will also run OK (tolerance = 10)\nexpect_equal(10, 2, 10)\n\nWarning: Unused arguments (10)\n\n\nError: 10 (`actual`) not equal to 2 (`expected`).\n\n  `actual`: 10.0\n`expected`:  2.0\n\n\nUse ignore_attr = TRUE if your objects have different attributes and you just care about the numeric values (default expect_equal() will throw an error):\n\nlocal_edition(3) ## not necessary for package\nnames(x) &lt;- \"hello\"\nexpect_equal(x, y)\n\nError: `x` (`actual`) not equal to `y` (`expected`).\n\n`names(actual)` is a character vector ('hello')\n`names(expected)` is absent\n\nexpect_equal(x, y, ignore_attr = TRUE)\n\nThe local_edition(3) code makes it so my code chunks use the most recent {testthat} functions. You don’t need to worry about that in your package. {usethis} will automatically assume the third edition. You can explicitly use the third edition by adding the following to your DESCRIPTION file:\n\nConfig/testthat/edition: 3\n\nexpect_match() checks for a regular expression match.\n\nexpect_match(\"hello\", \"ll\")\nexpect_match(\"helo\", \"ll\")\n\nError: \"helo\" does not match \"ll\".\nActual value: \"helo\"\n\n\nYou can use expect_warning() and expect_error() to check that your functions error correctly.\n\nsimreg &lt;- function(n, x, beta0, beta1, sigma2) {\n  ## Check input\n  stopifnot(length(x) == n)\n\n  ## Simulate y\n  eps &lt;- stats::rnorm(n = n, mean = 0, sd = sqrt(sigma2))\n  y &lt;- beta0 + beta1 * x + eps\n  return(y)\n}\n\nx &lt;- runif(100)\nbeta0 &lt;- 0\nbeta1 &lt;- 2\nsigma2 &lt;- 0.5\nexpect_error(simreg(n = 1, \n                    x = x, \n                    beta0 = beta0, \n                    beta1 = beta1, \n                    sigma2 = sigma2))\n\nIt is recommended that you think harder about your unit tests, but you can just test for a non-error by using expect_error() and setting regexp = NA\n\nexpect_error(simreg(n = length(x), \n                    x = x, \n                    beta0 = beta0, \n                    beta1 = beta1, \n                    sigma2 = sigma2), \n             regexp = NA)\n\nexpect_type() is tests for the type of the output (\"double\", \"integer\", \"character\", \"logical\", or \"list\").\n\nlocal_edition(3)\nexpect_type(1, \"double\")\nexpect_type(1L, \"integer\")\nexpect_type(\"1\", \"character\")\nexpect_type(TRUE, \"logical\")\n\nexpect_s3_class() is used to test for the class of the object (e.g. \"data.frame\", \"matrix\", \"tibble\", \"lm\", etc..)\n\ny &lt;- simreg(n = length(x), x = x, beta0 = beta0, beta1 = beta1, sigma2 = sigma2)\nlmout &lt;- lm(y ~ x)\nexpect_s3_class(object = lmout, class = \"lm\")\n\nexpect_true() acts like stopifnot() except for unit tests instead of assertions.\n\nexpect_true(3 == 3)\n\nExample: A common test for a simulation script is to see if estimators that we expect to work well on average do, indeed, work well on average. In the case of the simple linear regression model, we will check that, for large \\(n\\), the OLS estimates are reasonably close to the true value of \\(\\beta_1\\)\n\nx &lt;- runif(1000000)\nbeta0 &lt;- 0\nbeta1 &lt;- 2\nsigma2 &lt;- 0.5\ny &lt;- simreg(n = length(x), x = x, beta0 = beta0, beta1 = beta1, sigma2 = sigma2)\nlmout &lt;- lm(y ~ x)\nexpect_equal(coef(lmout)[[2]], beta1, tolerance = 0.01)\n\nExercise: Write an expectation that the output is a numeric vector."
  },
  {
    "objectID": "04_packages/04_testing.html#test",
    "href": "04_packages/04_testing.html#test",
    "title": "Assertions and Unit Tests",
    "section": "Test",
    "text": "Test\n\nExpectations go inside tests.\nAll {testthat} tests are of the form\n\ntest_that(\"Human Readable Description\", {\n  ## Code running test\n})\n\nThe first argument is a human-readable and informative description of what the test is accomplishing.\nThe second argument is is an expression where you put code.\n\nAn expression is a multi-lined block of R code surrounded by curly braces {}.\n\nLet’s put out a couple expectations in our test for simreg().\n\ntest_that(\"simreg() output is consistent\", {\n  set.seed(991)\n  x &lt;- runif(1000000)\n  beta0 &lt;- 0\n  beta1 &lt;- 2\n  sigma2 &lt;- 0.5\n  y &lt;- simreg(n = length(x), x = x, beta0 = beta0, beta1 = beta1, sigma2 = sigma2)\n  lmout &lt;- lm(y ~ x)\n  expect_equal(coef(lmout)[[2]], beta1, tolerance = 0.001)\n\n  expect_equal(length(x), length(y))\n})\n\nTest passed 🥇\n\n\nNotice that I put two expectations in the same test. This is all connected to the output of simreg(), so it makes sense to put them in the same test.\nWhenever a test generates something randomly, I like to set a seed for reproducibility.\n\nA random seed initializes the pseudorandom process. So any “random draws” in R will be the same if you set the same seed via set.seed().\n\nIf you find yourself printing stuff in the console when writing code, try writing a test instead.\nI usually have a unit test open at the same time that I am coding a function.\nTry to test a function in only one file. If you do change something and need to update your tests, that will make it easier to update."
  },
  {
    "objectID": "04_packages/04_testing.html#testthat-file",
    "href": "04_packages/04_testing.html#testthat-file",
    "title": "Assertions and Unit Tests",
    "section": "Testthat File",
    "text": "Testthat File\n\nA testthat file is just an R script that holds a few related tests.\nYou can create an R script for unit testing by typing\n\nusethis::use_test()\n\nspecifying the name of the R script.\nYou should choose a one or two-word name (separated by dashes -) that describes the collection of tests. E.g.\n\nusethis::use_test(\"sim-code\")\n\nExercise: Edit regsim() so that x is either a vector or NULL. If NULL, then your function should simulate x from a standard normal distribution. You can check if a value is NULL via is.null(). The function should then return a list of length two with the simulated x and y values. Create a new unit test for this new behavior.\nExercise: In simreg(), if x is provided, then n is not really needed since it can be inferred from x. Set the default of n to be NULL and only require it if x is not provided. Give a warning if both x and n are provided, and throw an error if both x and n are NULL. Write a unit test to check all of these new behaviors. Note: It is typical (and good practice) to put all arguments with defaults after all arguments without defaults."
  },
  {
    "objectID": "05_web_scraping/05_apis.html",
    "href": "05_web_scraping/05_apis.html",
    "title": "Getting Data Through APIs",
    "section": "",
    "text": "APIs and R.\nChapter 1, 2, and 3 from An Introduction to APIs.\nChapter 40 from STAT 545.\nGetting Started with {httr2}\nRectangling"
  },
  {
    "objectID": "05_web_scraping/05_apis.html#basic-authentication",
    "href": "05_web_scraping/05_apis.html#basic-authentication",
    "title": "Getting Data Through APIs",
    "section": "Basic Authentication",
    "text": "Basic Authentication\n\n“Basic” authentication, where you just provide a username and password. The basic syntax for this is:\n\nreq_auth_basic(req, username, password)\n\nYou should already have a username and password set up.\nI would suggest keeping your password secure using your .Renviron file or via {keyring} (see below)."
  },
  {
    "objectID": "05_web_scraping/05_apis.html#api-keys",
    "href": "05_web_scraping/05_apis.html#api-keys",
    "title": "Getting Data Through APIs",
    "section": "API keys",
    "text": "API keys\n\nAn API key is a string that you add to your request.\nTo obtain a free key from OMDB and access it in R:\n\nSign up for a free key: https://www.omdbapi.com/apikey.aspx\nOpen up your .Renviron file using the usethis package.\n\nlibrary(usethis)\nedit_r_environ()\n\nAdd the key OMDB sent you by email to the .Renviron package. You can call it OMDB_API_KEY, for example. In which case you would write the following in .Renviron:\nOMDB_API_KEY = &lt;your-private-key&gt;\nWhere “&lt;your-private-key&gt;” is the private key OMDB sent you by email.\nRestart R.\nYou can now always access your private key by\n\nmovie_key &lt;- Sys.getenv(\"OMDB_API_KEY\")\n\nYou typically put your API key as a query parameter via req_url_query() (see below)\n\nIt is important to never save or display your private key in a file you could share. You are responsible for all behavior associated with your key. That is why we saved it to our .Renviron and are only accessing it secretly through Sys.getenv().\nIt is still not great that your key is in a plain text environment. You can add a layer of security by using the keyring package: https://github.com/r-lib/keyring\n\nlibrary(keyring)\nkey_set(\"OMDB_API_KEY_SECURE\") ## do this once to save the key\nmovie_key_2 &lt;- key_get(\"OMDB_API_KEY_SECURE\") ## do this each time you need the key\n\nA person with access to your computer who knows R and the keyring package could still get to your key. But it is more secure than placing your key in a plain text file (which is what .Renviron is). There are more secure ways to access keys in R."
  },
  {
    "objectID": "05_web_scraping/05_apis.html#oauth",
    "href": "05_web_scraping/05_apis.html#oauth",
    "title": "Getting Data Through APIs",
    "section": "OAuth:",
    "text": "OAuth:\n\nSee OAuth from {httr2} for details.\nOAuth (“Open Authorization”) is an open standard for authorization that allows users to securely access resources without giving away their login credentials.\nThe idea is that your software asks the user if it can use the user’s authorization to access the API.\n\nIt does this each time it needs to access the API.\nThis is commonly used in big API’s, like that of Google, Twitter, or Facebook.\n\nAs an example, we will consider the GitHub API.\n\n\n“Register an application” with the API provider before you can access the API.\n\nYou do this by creating a developer account on the API’s website, then registering a new OAuth app.\nYou won’t actually have an app, but API developers use this word for any means where you ask to use a user’s authorization.\nThis typically involves providing a name and a callback URL (typically http://localhost:1410) for your “application”.\nFor GitHub:\n\nRegister at https://github.com/settings/developers\nUse any URL (like https://github.com/) as your homepage URL\nUse http://localhost:1410 as the callback url.\n\n\nThe provider will then give you a client_id and a client_secret that you will need to use.\n\nNeither of these need to be protected like a password since the user will provide their own password/username for authentication.\nFor GitHub:\n\nYou can find these here: https://github.com/settings/developers\nClick on your OAuth app.\n\n\nObtain a “token URL”, which is sometimes called an “access URL”.\n\nThis can be found somewhere in the API docs, but you have to read them carefully.\nFor GitHub:\n\nThe access URL is https://github.com/login/oauth/access_token\n\n\nUse oauth_client() to create a client. You feed in the client_id, the client_secret, the toeken_url, and any name you choose into it.\n\nclient &lt;- oauth_client(\n  id = \"client_id\",\n  secret = \"client_secret\",\n  token_url = \"token_url\",\n  name = \"personal_app_name\"\n)\n\n\nFor GitHub:\n\nclient &lt;- oauth_client(\n  id = \"933ffc6f53e466c58aa1\",\n  secret = \"aa02ef46f93aa51a360f23f30f7640b445118e7f\",\n  token_url = \"https://github.com/login/oauth/access_token\",\n  name = \"gitapp\"\n)\n\n\nYou get an “authorization URL”.\n\nThis again can be found somewhere in the docs.\nFor GitHub:\n\nThe autorization URL is: https://github.com/login/oauth/authorize\n\nauth_url &lt;- \"https://github.com/login/oauth/authorize\"\n\n\n\nFeed the authorization URL and the client into req_oauth_auth_code() during a request.\n\nrequest(\"https://api.github.com/user\") |&gt;\n  req_oauth_auth_code(client = client, auth_url = auth_url) |&gt;\n  req_perform() -&gt;\n  gout\ngout\n\n\n\nThere are sometimes other “flows” for OAuth that require different steps. See here for details: https://httr2.r-lib.org/articles/oauth.html\nWhen using OAuth in a package, folks often (i) “cache” the token securely (because the generated token should be kept private), and (ii) ask folks to generate their own app.\nCaching is easy. Just set cache_disk = TRUE in req_oauth_auth_code().\n\nNote that this creates some security risks since the token will be saved on the disk.\nSo you should inform the user if you do this.\n\nWhen you ask folks to generate their own app, then you should have client_id and client_secret as arguments that the user can provide."
  },
  {
    "objectID": "05_web_scraping/05_apis.html#url-path",
    "href": "05_web_scraping/05_apis.html#url-path",
    "title": "Getting Data Through APIs",
    "section": "URL Path",
    "text": "URL Path\n\nEvery API has a base URL that you modify.\nSome API’s only modify the URL path to obtain the endpoint.\nConsider the Wizard World API\nThe documentation says that the base URL is “https://wizard-world-api.herokuapp.com”.\n\nbaseurl &lt;- \"https://wizard-world-api.herokuapp.com\"\n\nLet’s start a request with this baseurl via request().\n\nwizreq &lt;- request(baseurl)\n\nThe documentation just says that we modify this URL to obtain the different objects. - E.g., to obtain a list of all elixirs that occur in Harry Potter, we just add “Elixirs” at the end.\n\nWe can do this to our request with req_url_path_append()\n\n\nwizreq &lt;- req_url_path_append(wizreq, \"Elixirs\")\nwizreq\n\n&lt;httr2_request&gt;\nGET https://wizard-world-api.herokuapp.com/Elixirs\nBody: empty\n\n\nLet’s look at the http request\n\nreq_dry_run(wizreq)\n\nGET /Elixirs HTTP/1.1\naccept: */*\naccept-encoding: deflate, gzip\nhost: wizard-world-api.herokuapp.com\nuser-agent: httr2/1.2.1 r-curl/7.0.0 libcurl/8.14.1\n\n\nWe then implement this request via req_perform().\n\neout &lt;- req_perform(wizreq)\neout\n\n&lt;httr2_response&gt;\nGET https://wizard-world-api.herokuapp.com/Elixirs\nStatus: 200 OK\nContent-Type: application/json\nBody: In memory (62284 bytes)\n\n\nWe would then clean this output (see rectangling below). But as a preview, we would do\n\ntibble(elixir = resp_body_json(resp = eout)) |&gt;\n  unnest_wider(col = elixir)\n\n# A tibble: 145 × 10\n   id      name  effect sideEffects characteristics time  difficulty ingredients\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;      &lt;list&gt;     \n 1 0106fb… Ferg… Treat… Potential … &lt;NA&gt;            &lt;NA&gt;  Unknown    &lt;list [3]&gt; \n 2 021b40… Mane… Rapid… &lt;NA&gt;        &lt;NA&gt;            &lt;NA&gt;  Unknown    &lt;NULL&gt;     \n 3 024f56… Poti… &lt;NA&gt;   &lt;NA&gt;        &lt;NA&gt;            &lt;NA&gt;  Unknown    &lt;NULL&gt;     \n 4 06beea… Rudi… Helps… &lt;NA&gt;        &lt;NA&gt;            &lt;NA&gt;  Unknown    &lt;list [2]&gt; \n 5 078b53… Lung… Most … &lt;NA&gt;        &lt;NA&gt;            &lt;NA&gt;  Unknown    &lt;NULL&gt;     \n 6 07944d… Esse… Menta… &lt;NA&gt;        Green in colour &lt;NA&gt;  Advanced   &lt;list [2]&gt; \n 7 0dd8d2… Anti… Cures… &lt;NA&gt;        Green in colour &lt;NA&gt;  Moderate   &lt;list [4]&gt; \n 8 0e2240… Rest… Rever… &lt;NA&gt;        Purple coloured &lt;NA&gt;  Unknown    &lt;NULL&gt;     \n 9 0e7228… Skel… Resto… &lt;NA&gt;        Smokes when po… &lt;NA&gt;  Moderate   &lt;list [6]&gt; \n10 0e7f89… Chee… &lt;NA&gt;   &lt;NA&gt;        Yellow in colo… &lt;NA&gt;  Advanced   &lt;list [1]&gt; \n# ℹ 135 more rows\n# ℹ 2 more variables: inventors &lt;list&gt;, manufacturer &lt;chr&gt;"
  },
  {
    "objectID": "05_web_scraping/05_apis.html#queries",
    "href": "05_web_scraping/05_apis.html#queries",
    "title": "Getting Data Through APIs",
    "section": "Queries",
    "text": "Queries\n\nSome APIs require you to modify the URL via queries. Queries occur after a question mark and are of the form\nhttp://www.url.com/?query1=arg1&query2=ar2&query3=arg3\nThe OMDB API requires you to provide queries. The documentation says\n\nSend all data requests to: http://www.omdbapi.com/?apikey=[yourkey]&\n\nBut that documentation already has a query as a part of it (apikey=[yourkey]).\nYou add queries via req_url_query().\n\nYou provide it with name/value paires.\n\nThe documentation for OMDB has a table called “Parameters”, where they list the possible queries.\nLet’s fetch information on the film The Lighthouse, obtaining a short plot in json format.\nThis is what the URL looks like without my API key:\n\nmovie_key &lt;- Sys.getenv(\"OMDB_API_KEY\")\nrequest(\"http://www.omdbapi.com/\") |&gt;\n  req_url_query(t = \"The Lighthouse\",\n                plot = \"short\",\n                r = \"json\") -&gt;\n  movie_req\nmovie_req\n\n&lt;httr2_request&gt;\nGET http://www.omdbapi.com/?t=The%20Lighthouse&plot=short&r=json\nBody: empty\n\n\nLet’s add our API key and perform the request.\n\nmovie_req |&gt;\n  req_url_query(apikey = movie_key) |&gt;\n  req_perform() -&gt;\n  mout\n\nOutput is just a list:\n\nresp_body_json(mout) |&gt;\n  str()\n\nList of 25\n $ Title     : chr \"The Lighthouse\"\n $ Year      : chr \"2019\"\n $ Rated     : chr \"R\"\n $ Released  : chr \"01 Nov 2019\"\n $ Runtime   : chr \"109 min\"\n $ Genre     : chr \"Drama, Fantasy, Horror\"\n $ Director  : chr \"Robert Eggers\"\n $ Writer    : chr \"Robert Eggers, Max Eggers\"\n $ Actors    : chr \"Robert Pattinson, Willem Dafoe, Valeriia Karaman\"\n $ Plot      : chr \"Two lighthouse keepers try to maintain their sanity while living on a remote and mysterious New England island in the 1890s.\"\n $ Language  : chr \"English\"\n $ Country   : chr \"Canada, United States\"\n $ Awards    : chr \"Nominated for 1 Oscar. 34 wins & 139 nominations total\"\n $ Poster    : chr \"https://m.media-amazon.com/images/M/MV5BMTI4MjFhMjAtNmQxYi00N2IxLWJjMGEtYWY1YmU3OTQ0Zjk3XkEyXkFqcGc@._V1_SX300.jpg\"\n $ Ratings   :List of 3\n  ..$ :List of 2\n  .. ..$ Source: chr \"Internet Movie Database\"\n  .. ..$ Value : chr \"7.4/10\"\n  ..$ :List of 2\n  .. ..$ Source: chr \"Rotten Tomatoes\"\n  .. ..$ Value : chr \"90%\"\n  ..$ :List of 2\n  .. ..$ Source: chr \"Metacritic\"\n  .. ..$ Value : chr \"83/100\"\n $ Metascore : chr \"83\"\n $ imdbRating: chr \"7.4\"\n $ imdbVotes : chr \"289,256\"\n $ imdbID    : chr \"tt7984734\"\n $ Type      : chr \"movie\"\n $ DVD       : chr \"N/A\"\n $ BoxOffice : chr \"$10,867,104\"\n $ Production: chr \"N/A\"\n $ Website   : chr \"N/A\"\n $ Response  : chr \"True\"\n\n\nIn the API documentation:"
  },
  {
    "objectID": "05_web_scraping/05_apis.html#headers",
    "href": "05_web_scraping/05_apis.html#headers",
    "title": "Getting Data Through APIs",
    "section": "Headers",
    "text": "Headers\n\nHeaders supply additional options for the return type.\nCommon headers are described by Wikipedia: https://en.wikipedia.org/wiki/List_of_HTTP_header_fields\nYou supply headers to a request via req_headers().\nConsider the icanhazdadjoke API. One option is to include a header to specify plain text returns, rather than JSON returns.\n\nrequest(\"https://icanhazdadjoke.com/\") |&gt;\n  req_headers(Accept = \"text/plain\") |&gt;\n  req_perform() -&gt;\n  dadout\ndadout\nresp_body_string(dadout)\n\nThis is different than a JSON output\n\nrequest(\"https://icanhazdadjoke.com/\") |&gt;\n  req_perform() -&gt;\n  dadout2\nresp_body_string(dadout2)"
  },
  {
    "objectID": "05_web_scraping/05_apis.html#output",
    "href": "05_web_scraping/05_apis.html#output",
    "title": "Getting Data Through APIs",
    "section": "Output:",
    "text": "Output:\n\nFunctions that work with the response are all of the form resp_*().\nThe status code describes whether your request was successful.\n\nList of codes: https://http.cat/\nUse resp_status() to get the code for our request:\n\nresp_status(mout)\n\n[1] 200\n\n\n\nHeaders provide infromation on the request. Use the resp_headers() function to see what headers we got in our request.\n\nresp_headers(mout)\n\nThe body contains the data you are probably most interested in. Use the resp_body_*() functions to access the body:\n\nresp_body_json(mout)\n\nBackground: The body typically comes in the form of either a JSON or XML data structure.\n\nFor JSON, you use resp_body_json()\nFor XML you use resp_body_xml()\nYou can see the unparsed output with resp_body_string()\n\n\nresp_body_string(mout)\n\n[1] \"{\\\"Title\\\":\\\"The Lighthouse\\\",\\\"Year\\\":\\\"2019\\\",\\\"Rated\\\":\\\"R\\\",\\\"Released\\\":\\\"01 Nov 2019\\\",\\\"Runtime\\\":\\\"109 min\\\",\\\"Genre\\\":\\\"Drama, Fantasy, Horror\\\",\\\"Director\\\":\\\"Robert Eggers\\\",\\\"Writer\\\":\\\"Robert Eggers, Max Eggers\\\",\\\"Actors\\\":\\\"Robert Pattinson, Willem Dafoe, Valeriia Karaman\\\",\\\"Plot\\\":\\\"Two lighthouse keepers try to maintain their sanity while living on a remote and mysterious New England island in the 1890s.\\\",\\\"Language\\\":\\\"English\\\",\\\"Country\\\":\\\"Canada, United States\\\",\\\"Awards\\\":\\\"Nominated for 1 Oscar. 34 wins & 139 nominations total\\\",\\\"Poster\\\":\\\"https://m.media-amazon.com/images/M/MV5BMTI4MjFhMjAtNmQxYi00N2IxLWJjMGEtYWY1YmU3OTQ0Zjk3XkEyXkFqcGc@._V1_SX300.jpg\\\",\\\"Ratings\\\":[{\\\"Source\\\":\\\"Internet Movie Database\\\",\\\"Value\\\":\\\"7.4/10\\\"},{\\\"Source\\\":\\\"Rotten Tomatoes\\\",\\\"Value\\\":\\\"90%\\\"},{\\\"Source\\\":\\\"Metacritic\\\",\\\"Value\\\":\\\"83/100\\\"}],\\\"Metascore\\\":\\\"83\\\",\\\"imdbRating\\\":\\\"7.4\\\",\\\"imdbVotes\\\":\\\"289,256\\\",\\\"imdbID\\\":\\\"tt7984734\\\",\\\"Type\\\":\\\"movie\\\",\\\"DVD\\\":\\\"N/A\\\",\\\"BoxOffice\\\":\\\"$10,867,104\\\",\\\"Production\\\":\\\"N/A\\\",\\\"Website\\\":\\\"N/A\\\",\\\"Response\\\":\\\"True\\\"}\"\n\n\nresp_header(\"content-type\") will often tell you the type of body output.\n\nresp_header(mout, \"content-type\")\n\n[1] \"application/json; charset=utf-8\"\n\nresp_header(eout, \"content-type\")\n\n[1] \"application/json; charset=utf-8\""
  },
  {
    "objectID": "05_web_scraping/05_oauth_examples.html",
    "href": "05_web_scraping/05_oauth_examples.html",
    "title": "Other OAuth Examples",
    "section": "",
    "text": "StackExchange\n\nStackExchange has a page for describing authentication: https://api.stackexchange.com/docs/authentication\nIt describes “server side applications” like posting stuff, and “client side applications” like reading stuff. We will use the “implicit” flow for reading stuff since that requires less security.\nRegister for an app key here: https://stackapps.com/users/login?returnurl=/apps/oauth/register\n\n\nauthorize_url &lt;- \"https://stackoverflow.com/oauth/dialog\""
  },
  {
    "objectID": "05_web_scraping/05_web_scraping.html",
    "href": "05_web_scraping/05_web_scraping.html",
    "title": "Web Scraping with rvest",
    "section": "",
    "text": "Learning Objectives\n\nBasics of Web Scraping\nChapter 24 of RDS\nOverview of rvest.\nSelectorGadget.\nWeb Scraping\n\n\n\nData on the Web\n\nThere are at least 4 ways people download data on the web:\n\nClick to download a csv/xls/txt file.\nUse a package that interacts with an API.\nUse an API directly.\nScrape directly from the HTML file.\n\nThis lesson, we talk about how to do 4.\nNote: You shouldn’t download thousands of HTML files from a website to parse — the admins might block you if you send too many requests.\nNote: Web scraping can be illegal in some circumstances, particularly if you intend to make money off of it or if you are collecting personal information (especially in Europe). I don’t give legal advice, so see Chapter 24 of RDS for some general recommendations, and talk to a lawyer if you are not sure.\nLet’s load the tidyverse:\n\nlibrary(tidyverse)\n\n\n\n\nHTML / CSS\n\nWe have to know a little bit about HTML and CSS in order to understand how to extract certain elements from a website.\nHTML stands for “HyperText Markup Language”\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;My First Web Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Welcome!&lt;/h1&gt;\n  &lt;p&gt;This is a &lt;b&gt;simple&lt;/b&gt; paragraph.&lt;/p&gt;\n  &lt;a href=\"https://en.wikipedia.org/\"&gt;Wikipedia&lt;/a&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nHTML consists of elements which start with a tag inside &lt;&gt; (like &lt;head&gt; and &lt;body&gt;), optional attributes that format the element (like href=url), contents (the text), and an end tag (like &lt;/head&gt; and &lt;/body&gt;). The above HTML text would be formatted like this:\n\n\n\n\n\nMy First Web Page\n\n\n\n\nWelcome!\n\n\nThis is a simple paragraph.\n\nWikipedia\n\n\n\n\nImportant tags for web scraping:\n\n&lt;h1&gt;–&lt;h6&gt;: Heading tags, with &lt;h1&gt; as the highest (most important).\n&lt;p&gt;: Paragraph of text.\n&lt;a&gt;: Creates hyperlinks to other pages or resources.\n&lt;img&gt;: Embeds an image.\n&lt;div&gt;: Generic container for layout and styling.\n&lt;span&gt;: Inline container for styling parts of text.\n&lt;ul&gt;: Unordered list (bulleted).\n&lt;ol&gt;: Ordered list (numbered).\n&lt;li&gt;: List item, used inside &lt;ul&gt; or &lt;ol&gt;.\n&lt;table&gt;: Defines a table structure.\n&lt;tr&gt;: Table row.\n&lt;td&gt;: Table data cell.\n&lt;th&gt;: Table header cell.\n&lt;strong&gt;: Strong importance (usually bold).\n&lt;em&gt;: Emphasized text (usually italic).\n\nCSS stands from “Cascading Style Sheets”. It’s a formatting language that indicates how HTML files should look. Every website you have been on is formatted with CSS.\nHere is some example CSS:\nh3 {\n  color: red;\n  font-style: italic;\n}\n\nfooter div.alert {\n  display: none;\n}\nThe part before the curly braces is called a selector. It corresponds to HTML tags. Specifically, for those two they would correspond to:\n&lt;h3&gt;Some text&lt;/h3&gt;\n\n&lt;footer&gt;\n&lt;div class=\"alert\"&gt;More text&lt;/div&gt;\n&lt;/footer&gt;\nThe code inside the curly braces are properties. For example, the h3 properties tells us to make the h3 headers red and in italics. The second CSS chunk says that all &lt;div&gt; tags of class \"alert\" in the &lt;footer&gt; should be hidden.\nCSS applies the same properties to the same selectors. So every time we use h3 will result in the h3 styling of red and italicized text.\nCSS selectors define patterns for selecting HTML elements. This is useful for scraping because we can extract all text in an HTML that corresponds to some CSS selector.\nYou can get a long way just selecting all p elements (standing for “paragraph”) since that is where a lot of text lives.\nThe most common attributes used are id and class.\n\nThe selectors corresponding to class begin with a dot ..\nThe selectors corresponding to id begin with a hashtak #.\n\nThe .a selector selects for “Text 1” in the following\n&lt;p class=\"a\"&gt;Text 1&lt;/p&gt;\nThe .a selector selects for “Text 2” in the following\n&lt;div class=\"a\"&gt;Text 2&lt;/div&gt;\nThe #b selector selects for “Text 3” in the following\n&lt;p id=\"b\"&gt;Text 3&lt;/p&gt;\nThe #b selector selects for “Text 4” in the following\n&lt;div id=\"b\"&gt;Text 4&lt;/div&gt;\nMore complicated selectors (from Richard Ressler):\n\nThe name selector just uses the name value of the element such as h3. All elements with the same name value will be selected.\nThe id selector uses a #, e.g., #my_id, to select a single element with id=my_id (all ids are unique within a page).\nThe class selector uses a ., e.g., .my_class, where class=my_class. All elements with the same class value will be selected.\nWe can combine selectors with ., , and/or \\ to select a single element or groups of similar elements.\n\nA selector of my_name.my_class combines name and class to select all (only) elements with the name=my_name and class=my_class.\n\nThe most important combinator is the white space, , the descendant combination. As an example, p a selects all &lt;a&gt; elements that are a child of (nested beneath) a &lt;p&gt; element in the tree.\nYou can also find elements based on the values of attributes, e.g., find an element based on an attribute containing specific text.\n\nFor a partial text search you would use '[attribute_name*=\"my_text\"]'. Note the combination of single quotes and double quotes so you have double quotes around the value.\n\n\n\n\n\nrvest\n\nWe’ll use rvest to extract elements from HTML files.\n\nlibrary(rvest)\n\nThe typical pipeline for rvest is:\n\nLoad the html file into R using read_html()\nChoose the selectors based on SelectorGadget (see below) or by inspecting the selectors manually using developer tools (see below).\nSelect those selectors using html_elements().\n\nPossibly select elements within those elements via html_element()\nE.g. html_elements() selects the observational units and html_element() selects values of variables within that unit.\n\nExtract the text using html_text2().\n\nOr, extract tables using html_table().\n\nExtreme cleaning using 412/612 tools.\n\nWe’ll do a real example after we cover SelectorGadget and the web developer tools. But for now, let’s create a small html file:\n\nhtml &lt;- minimal_html('\n&lt;p class=\"a\"&gt;Text 1&lt;/p&gt;\n&lt;div class=\"a\"&gt;Text 2&lt;/div&gt;\n&lt;p id=\"b\"&gt;Text 3&lt;/p&gt;\n&lt;div id=\"b\"&gt;Text 4&lt;/div&gt;\n')\n\nWe can get all p tag text via\n\nhtml_elements(html, \"p\") |&gt;\n  html_text2()\n\n[1] \"Text 1\" \"Text 3\"\n\n\nWe can get all div tag text via\n\nhtml_elements(html, \"div\") |&gt;\n  html_text2()\n\n[1] \"Text 2\" \"Text 4\"\n\n\nWe can get all class=a text via\n\nhtml_elements(html, \".a\") |&gt;\n  html_text2()\n\n[1] \"Text 1\" \"Text 2\"\n\n\nWe can get all id=b text via\n\nhtml_elements(html, \"#b\") |&gt;\n  html_text2()\n\n[1] \"Text 3\" \"Text 4\"\n\n\nOnce you use html_elements(), it’s common to then use html_element() to extract even more information.\n\nhtml_k &lt;- minimal_html(\"\n&lt;p&gt;&lt;emph&gt;A&lt;/emph&gt;: &lt;b&gt;Ape&lt;/b&gt; picks an &lt;b&gt;Apple&lt;/b&gt; for &lt;b&gt;Aardvark&lt;/b&gt; below.&lt;/p&gt;\n&lt;p&gt;&lt;emph&gt;L&lt;/emph&gt;: &lt;b&gt;Lion&lt;/b&gt; &lt;b&gt;Lifts&lt;/b&gt; &lt;b&gt;Ladybug's&lt;/b&gt; &lt;b&gt;Luggage&lt;/b&gt;&lt;/p&gt;\n&lt;p&gt;&lt;emph&gt;P&lt;/emph&gt;: &lt;b&gt;Penguin&lt;/b&gt; &lt;b&gt;Plays&lt;/b&gt; with &lt;b&gt;Platypus&lt;/b&gt; in the &lt;b&gt;Pool&lt;/b&gt;&lt;/p&gt;\n\")\n\n\nhtml_k |&gt;\n  html_elements(\"p\") |&gt;\n  html_element(\"emph\") |&gt;\n  html_text2()\n\n[1] \"A\" \"L\" \"P\"\n\n\nIf you want all of the bs that are within a p, you can use .\n\nhtml_k |&gt;\n  html_elements(\"p b\") |&gt;\n  html_text()\n\n [1] \"Ape\"       \"Apple\"     \"Aardvark\"  \"Lion\"      \"Lifts\"     \"Ladybug's\"\n [7] \"Luggage\"   \"Penguin\"   \"Plays\"     \"Platypus\"  \"Pool\"     \n\n\nExercise: Try extracting b with both html_element() and html_elements(). What’s the difference?\nExercise (from R4DS): Get all of the text from the li element below:\n\nhtml &lt;- minimal_html(\"\n  &lt;ul&gt;\n    &lt;li&gt;&lt;b&gt;C-3PO&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;167 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R4-P17&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;R2-D2&lt;/b&gt; is a &lt;i&gt;droid&lt;/i&gt; that weighs &lt;span class='weight'&gt;96 kg&lt;/span&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;b&gt;Yoda&lt;/b&gt; weighs &lt;span class='weight'&gt;66 kg&lt;/span&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n  \")\n\nExercise (from R4DS): Extract the name of each droid. Start with the output of the second exercise.\nExercise (from R4DS): Use the class attribute of weight to extract the weight of each droid. Do not use span. Start with the output of the second exercise.\n\n\n\nSelectorGadget\n\nSelectorGadget is a tool for you to see what selector influences a particular element on a website.\nTo install SelectorGadget, drag this link to your bookmark bar on Chrome: SelectorGadget\nSuppose we wanted to get the top 100 movies of all time from IMDB. The web page is very unstructured:\nhttps://www.imdb.com/list/ls055592025/\n \n\nIf the above link fails, try: https://data-science-master.github.io/lectures/08_web_scraping/imdb_100.html```\n\nIf we click on the ranking of the Godfather, the “1” turns green (indicating what we have selected).\n \nThe “.text-primary” is the selector associated with the “1” we clicked on.\nEverything highlighted in yellow also has the “.text-primary” selector associated with it.\nWe will also want the name of the movie. So if we click on that we get the selector associated with both the rank and the movie name: “a , .text-primary”.\n \nBut we also got a lot of stuff we don’t want (in yellow). If we click one of the yellow items that we don’t want, it turns red. This indicates that we don’t want to select it.\n \nOnly the ranking and the name remain, which are under the selector “.ipc-title-link-wrapper .ipc-title__text–reduced”.\nIt’s important to visually inspect the selected elements throughout the whole HTML file. SelectorGadget doesn’t always get all of what you want, or it sometimes gets too much.\n\n\nExerciseSolution\n\n\nWhat selector can we use to get just the names of each film, the metacritic score, and the IMDB rating?\n\n\nHere is what I got:\n\".ipc-rating-star--rating , .metacritic-score-label, .ipc-title-link-wrapper .ipc-title__text--reduced\"\n\n\n\n\n\nChrome developer tools:\n\nIf you have trouble with SelectorGadget, you can also use the Chrome developer tools.\nChrome works best for web scraping (better than Safari/Edge/Firefox/etc). So install it if you don’t have it.\nOpen up the list of all selectors with: ⋮ &gt; More tools &gt; Developer tools.\nClicking on the element selector on the top left of the developer tools will show you what selectors are possible with each element.\n \nYou can also right click on the part of the website you are interested in and then click “Inspect”.\nIn the developer tools, hover over the element you are interested, right click, and then click Copy &gt; Copy selector. This gives you the selector for that element that you can then inspect.\n\n\n\nMore rvest\n\nLet’s do a more complicated example of rvest.\nUse read_html() to save an HTML file to a variable. The variable will be an “xml_document” object\n\nhtml_obj &lt;- read_html(\"https://www.imdb.com/list/ls055592025/\")\nhtml_obj\nclass(html_obj)\n\nTry read_html_live() if you notice read_html() is not working.\nXML stands for “Extensible Markup Language”. It’s a markup language (like HTML and Markdown), useful for representing a document. rvest will store the HTML file as an XML.\nWe can use html_elements() and the selectors we found in the previous section to get the elements we want. Insert the found selectors as the css argument.\n\nranking_elements &lt;- html_elements(html_obj, css = \".ipc-title-link-wrapper .ipc-title__text--reduced\")\nhead(ranking_elements)\n\n{xml_nodeset (6)}\n[1] &lt;h3 class=\"ipc-title__text ipc-title__text--reduced\"&gt;1. The Godfather&lt;/h3&gt;\n[2] &lt;h3 class=\"ipc-title__text ipc-title__text--reduced\"&gt;2. The Shawshank Red ...\n[3] &lt;h3 class=\"ipc-title__text ipc-title__text--reduced\"&gt;3. Schindler's List&lt; ...\n[4] &lt;h3 class=\"ipc-title__text ipc-title__text--reduced\"&gt;4. Raging Bull&lt;/h3&gt;\n[5] &lt;h3 class=\"ipc-title__text ipc-title__text--reduced\"&gt;5. Casablanca&lt;/h3&gt;\n[6] &lt;h3 class=\"ipc-title__text ipc-title__text--reduced\"&gt;6. Citizen Kane&lt;/h3&gt;\n\n\nNote: html_element() is similar, but will return exactly one response per element, so is useful if some elements have missing components.\nTo extract the text inside the obtained nodes, use html_text() or html_text2():\n\nhtml_text2() just does a little more pre-formatting (like converting line breaks from HTML to R code, removing white spaces, etc). So you should typically use this.\n\n\nranking_text &lt;- html_text2(ranking_elements)\nhead(ranking_text)\n\n[1] \"1. The Godfather\"            \"2. The Shawshank Redemption\"\n[3] \"3. Schindler's List\"         \"4. Raging Bull\"             \n[5] \"5. Casablanca\"               \"6. Citizen Kane\"            \n\n\nAfter you do this, you need to tidy the data using your data munging tools.\n\ntibble(text = ranking_text) |&gt;\n  separate(col = \"text\", into = c(\"ranking\", \"movie\"), sep = \"\\\\.\", extra = \"merge\") -&gt;\nmovierank\nmovierank\n\n# A tibble: 100 × 2\n   ranking movie                             \n   &lt;chr&gt;   &lt;chr&gt;                             \n 1 1       \" The Godfather\"                  \n 2 2       \" The Shawshank Redemption\"       \n 3 3       \" Schindler's List\"               \n 4 4       \" Raging Bull\"                    \n 5 5       \" Casablanca\"                     \n 6 6       \" Citizen Kane\"                   \n 7 7       \" Gone with the Wind\"             \n 8 8       \" The Wizard of Oz\"               \n 9 9       \" One Flew Over the Cuckoo's Nest\"\n10 10      \" Lawrence of Arabia\"             \n# ℹ 90 more rows\n\n\n\n\nExerciseHintSolution\n\n\nExtract the directors and the names of each film. Try to use SelectorGadget to find your own selectors.\n\n\nThere are probably multiple ways to do this. But I used \".dli-parent\" to get get the movies. Then I did two separate calls to html_element() with \".ipc-title__text--reduced\" to get the titles and \".bDNbpf span\" to get the directors.\n\n\n\nhtml_elements(html_obj, \".dli-parent\") |&gt;\n  html_element(\".ipc-title__text--reduced\") |&gt;\n  html_text2() -&gt;\n  titvec\n\nhtml_elements(html_obj, \".dli-parent\")  |&gt;\n  html_element(\".bDNbpf span\") |&gt;\n  html_text2() -&gt;\n  dirvec\n\ntibble(title = titvec, dir = dirvec) |&gt;\n  separate(col = \"title\", into = c(\"rank\", \"title\"), sep = \"\\\\.\") |&gt;\n  mutate(dir = str_extract(string = dir, pattern = \"Director.+Stars\")) |&gt;\n  mutate(dir = str_remove(dir, \"^Directors*\")) |&gt;\n  mutate(dir = str_remove(dir, \"Stars*$\"))\n\n# A tibble: 100 × 3\n   rank  title                              dir                                 \n   &lt;chr&gt; &lt;chr&gt;                              &lt;chr&gt;                               \n 1 1     \" The Godfather\"                   Francis Ford Coppola                \n 2 2     \" The Shawshank Redemption\"        Frank Darabont                      \n 3 3     \" Schindler's List\"                Steven Spielberg                    \n 4 4     \" Raging Bull\"                     Martin Scorsese                     \n 5 5     \" Casablanca\"                      Michael Curtiz                      \n 6 6     \" Citizen Kane\"                    Orson Welles                        \n 7 7     \" Gone with the Wind\"              Victor Fleming                      \n 8 8     \" The Wizard of Oz\"                Victor FlemingGeorge CukorNorman Ta…\n 9 9     \" One Flew Over the Cuckoo's Nest\" Milos Forman                        \n10 10    \" Lawrence of Arabia\"              David Lean                          \n# ℹ 90 more rows\n\n\n\n\n\n\n\nA very simple example\n\nHere is a very simple html file that is generated using rvest:\n\nhtml &lt;- minimal_html(\"\n  &lt;h1&gt;This is a heading&lt;/h1&gt;\n  &lt;p id='first'&gt;This is a paragraph&lt;/p&gt;\n  &lt;p class='important'&gt;This is an important paragraph&lt;/p&gt;\n\")\n\nThe h1 selector selects for h1 tags.\n\nhtml_elements(html, \"h1\") |&gt;\n  html_text()\n\n[1] \"This is a heading\"\n\n\nThe .important selector selects for class attribute that is important\n\nhtml_elements(html, \".important\") |&gt;\n  html_text()\n\n[1] \"This is an important paragraph\"\n\n\nThe #first selector selects for id attribute that is first\n\nhtml_elements(html, \"#first\") |&gt;\n  html_text()\n\n[1] \"This is a paragraph\"\n\n\n\n\n\nBigger example using rvest\n\nYou typically use html_elements() and html_element() together. You first use html_elements() to select observations. You then use html_element() to select values of variables from each observation.\nLet’s try and get the name, rank, year, and metascore for each movie.\nI played with the developer tools until I saw that\n\n“.ipc-metadata-list-summary-item” extracts each movie\n“.ipc-title” extracts the title from a movie\n“.metacritic-score-box” extracts the meteascore from a movie\n“.dli-title-metadata-item” extracts the year, runtime, and rating for each movie\n\n\n\nmovie_list &lt;- html_elements(html_obj, \".ipc-metadata-list-summary-item\") \nlength(movie_list) ## should be 100\n\n[1] 100\n\ntibble(\n  title = movie_list |&gt;\n    html_element(\".ipc-title\") |&gt;\n    html_text2(),\n  meta = movie_list |&gt;\n    html_element(\".metacritic-score-box\") |&gt;\n    html_text2(),\n  year = movie_list |&gt;\n    html_element(\".dli-title-metadata-item\") |&gt;\n    html_text2()\n)\n\n# A tibble: 100 × 3\n   title                              meta  year \n   &lt;chr&gt;                              &lt;chr&gt; &lt;chr&gt;\n 1 1. The Godfather                   100   1972 \n 2 2. The Shawshank Redemption        82    1994 \n 3 3. Schindler's List                95    1993 \n 4 4. Raging Bull                     90    1980 \n 5 5. Casablanca                      100   1942 \n 6 6. Citizen Kane                    100   1941 \n 7 7. Gone with the Wind              97    1939 \n 8 8. The Wizard of Oz                92    1939 \n 9 9. One Flew Over the Cuckoo's Nest 84    1975 \n10 10. Lawrence of Arabia             100   1962 \n# ℹ 90 more rows\n\n\nWe could of course clean the title column here into rank and title.\nIf we wanted the runtime and rating, we could loop over the movie list that we created and extract out each of the three elements that belong to “.dli-title-metadata-item”\n\nyear_vec &lt;- rep(NA, length = length(movie_list))\nruntime_vec &lt;- rep(NA, length = length(movie_list))\nrating_vec &lt;- rep(NA, length = length(movie_list))\nfor (i in seq_along(movie_list)) {\n  movie_list[i] |&gt;\n    html_elements(\".dli-title-metadata-item\") |&gt;\n    html_text2() -&gt;\n    x\n  year_vec[[i]] &lt;- x[[1]]\n  runtime_vec[[i]] &lt;- x[[2]]\n  rating_vec[[i]] &lt;- x[[3]]\n}\nhead(year_vec)\n\n[1] \"1972\" \"1994\" \"1993\" \"1980\" \"1942\" \"1941\"\n\nhead(runtime_vec)\n\n[1] \"2h 55m\" \"2h 22m\" \"3h 15m\" \"2h 9m\"  \"1h 42m\" \"1h 59m\"\n\nhead(rating_vec)\n\n[1] \"R\"  \"R\"  \"R\"  \"R\"  \"PG\" \"PG\"\n\n\n\n\nhtml_table()\n\nWhen data is in the form of a table, you can format it more easily with html_table().\nThe Wikipedia article on hurricanes in 2024: https://en.wikipedia.org/wiki/2024_Atlantic_hurricane_season\nIf the above link fails, try: https://data-science-master.github.io/lectures/08_web_scraping/wiki_2.html\nThis contains many tables which might be a pain to copy and paste into Excel (and we would be prone to error if we did so). Let’s try to automate this procedure.\nSave the HTML\n\nwikixml &lt;- read_html(\"https://en.wikipedia.org/wiki/2024_Atlantic_hurricane_season\")\n\nWe’ll extract all of the “table” elements.\n\nwikidat &lt;- html_elements(wikixml, \"table\")\n\nUse html_table() to get a list of tables from table elements:\n\ntablist &lt;- html_table(wikidat)\nclass(tablist)\n\n[1] \"list\"\n\nlength(tablist)\n\n[1] 27\n\ntablist[[3]]\n\n# A tibble: 10 × 3\n    Rank Cost               Season\n   &lt;int&gt; &lt;chr&gt;               &lt;int&gt;\n 1     1 ≥ $294.803 billion   2017\n 2     2 $172.297 billion     2005\n 3     3 $130.438 billion     2024\n 4     4 $117.708 billion     2022\n 5     5 ≥ $80.827 billion    2021\n 6     6 $72.341 billion      2012\n 7     7 $61.148 billion      2004\n 8     8 $54.336 billion      2020\n 9     9 ≥ $50.526 billion    2018\n10    10 ≥ $48.855 billion    2008\n\n\nYou can clean up, bind, or merge these tables after you have read them in.\n\n\nExerciseHintSolution\n\n\nThe Wikipedia page on the oldest mosques in the world has many tables: https://en.wikipedia.org/wiki/List_of_the_oldest_mosques\nIf the above link fails, try: https://data-science-master.github.io/lectures/08_web_scraping/mosque_2.html\n\nUse rvest to read these tables into R.\nMerge the data frames together. You only need to keep the building name, the country, and the time it was first build.\n\n\n\nIt’s easier if you use a css selector of \"table.wikitable\" with html_elements() first to get the table rather than just \"table\". I found this out by getting to the developer tools in Chrome with CTRL + Shift + I then playing around with the tables.\n\n\n\nmosque &lt;- read_html(\"https://data-science-master.github.io/lectures/05_web_scraping/mosque.html\")\n\n\nmosque |&gt;\n  html_elements(\"h3\") |&gt;\n  html_text2() -&gt;\n  catvec\ncatvec &lt;- c(\"Mentioned in Quran\", catvec)\n\nmosque |&gt;\n  html_elements(\"table.wikitable\") |&gt;\n  html_table() -&gt;\n  tablist\n## Errors if you try bind_rows() because some are integers and some are characters\nfor (i in seq_along(tablist)) {\n  if (any(names(tablist[[i]]) == \"First built\")) {\n    names(tablist[[i]])[names(tablist[[i]]) == \"First built\"] &lt;- \"date\"\n    tablist[[i]]$date &lt;- str_remove_all(tablist[[i]]$date, \"\\\\[.*\\\\]\")\n  }\n}\ntb &lt;- bind_rows(tablist)\ntb |&gt;\n  select(Building, Location, Country, date, Notes, Tradition)\n\n# A tibble: 176 × 6\n   Building                               Location Country date  Notes Tradition\n   &lt;chr&gt;                                  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    \n 1 Al-Haram Mosque                        Mecca    Saudi … Unkn… \"Al-…  &lt;NA&gt;    \n 2 Haram al-Sharif, also known as the Al… Jerusal… Palest… Cons… \"Al-…  &lt;NA&gt;    \n 3 The Sacred Monument                    Muzdali… Saudi … Unkn… \"Al-…  &lt;NA&gt;    \n 4 Quba Mosque                            Medina   Saudi … 622   \"The…  &lt;NA&gt;    \n 5 Mosque of the Companions               Massawa  Eritrea 620s… \"Bel… \"\"       \n 6 Al Nejashi Mosque                      Negash   Ethiop… 7th … \"By … \"\"       \n 7 Mosque of Amr ibn al-As                Cairo    Egypt   641   \"Nam… \"\"       \n 8 Mosque of Ibn Tulun                    Cairo    Egypt   879   \"\"    \"\"       \n 9 Al-Azhar Mosque                        Cairo    Egypt   972   \"\"    \"Sunni\"  \n10 Arba'a Rukun Mosque                    Mogadis… Somalia 1268… \"\"    \"Sunni\"  \n# ℹ 166 more rows"
  },
  {
    "objectID": "09_shiny/09_reactivity.html",
    "href": "09_shiny/09_reactivity.html",
    "title": "Reactivity and the server() Function",
    "section": "",
    "text": "Learn the basics reactivity and the the server() function in Shiny Apps.\nChapter 4 of Mastering Shiny.\nShiny Cheatsheet.\nOptional Resources\n\nShiny Tutorial.\nShiny Video, Part 2\nShiny Examples."
  },
  {
    "objectID": "09_shiny/09_reactivity.html#render-functions",
    "href": "09_shiny/09_reactivity.html#render-functions",
    "title": "Reactivity and the server() Function",
    "section": "Render Functions",
    "text": "Render Functions\n\nYou’ve already seen render functions. They are of the form:\n\nrender*({\n  ## Code goes here.\n})\n\nThe curly braces allow you to write multiple lines of code and submit that as the argument.\nAlways save output of render function as an element of the output list.\nAny input elements inside the render*() function will trigger the render code to be re-evaluated when those input elements change.\nWhen an input element changes, it is said to be “invalidated”. This triggers the reactive elements.\nWhen any input in a code chunk is invalidated, then Shiny will rerun the entire code chunk (not just the portion that was invalidated).\nExercise: Create a Shiny App that asks for a person’s name and prints “Hello” followed by the person’s name."
  },
  {
    "objectID": "09_shiny/09_reactivity.html#reactive-expressions",
    "href": "09_shiny/09_reactivity.html#reactive-expressions",
    "title": "Reactivity and the server() Function",
    "section": "Reactive Expressions",
    "text": "Reactive Expressions\n\nYou can create variables via reactive() (technically not variables but “reactive elements”). They can then be used in different render*() functions.\n\nlibrary(shiny)\nlibrary(stringr)\n\nui &lt;- fluidPage(\n  textInput(\"text\", \"What Text?\", value = \"dog\"),\n  textOutput(\"pigtext1\"),\n  textOutput(\"pigtext2\")\n)\n\nserver &lt;- function(input, output) {\n\n  x &lt;- reactive({\n    str_replace(input$text, \"([^aeiouAEIOU]*)(.*)\", \"\\\\2\\\\1ay\")\n    })\n  output$pigtext1 &lt;- renderText(x())\n  output$pigtext2 &lt;- renderText(x())\n}\n\nshinyApp(ui = ui, server = server)\n\n \nThe variables you create this way are called “reactive elements”.\nYou can use them in render*() functions, but include a “()” after them (like they are function calls, because they technically are).\nThat is, we defined x by\n\nx &lt;- reactive({ str_replace(input$text, \"([^aeiouAEIOU]*)(.*)\", \"\\\\2\\\\1ay\") })\n\nBut when we call it later, we need to use\n\nx()\n\nIf you didn’t use reactive(), you would get an error (because you would be calling input$text outside of a render*() or reactive() call).\nAn example with simulation data\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(broom)\n\nui &lt;- fluidPage(\n  numericInput(\"nsamp\", \"Number of samples\", value = 50, step = 1),\n  numericInput(\"diff\", \"Effect size\", value = 0.5, step = 0.1),\n  plotOutput(\"plot\"),\n  verbatimTextOutput(\"text\")\n)\n\nserver &lt;- function(input, output) {\n  x1 &lt;- reactive({\n    rnorm(n = input$nsamp, mean = 0, sd = 1)\n  })\n\n  x2 &lt;- reactive({\n    rnorm(n = input$nsamp, mean = input$diff, sd = 1)\n  })\n\n  output$plot &lt;- renderPlot({\n    data.frame(`1` = x1(), `2` = x2()) |&gt;\n      gather(key = \"Group\", value = \"y\") |&gt;\n      ggplot(aes(x = Group, y = y)) +\n      geom_boxplot() +\n      theme_bw()\n  })\n\n  output$text &lt;- renderPrint({\n    t.test(x1(), x2()) |&gt;\n      tidy() |&gt;\n      select(estimate, `P-value` = p.value, Lower = conf.low, Higher = conf.high)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n \nExercise: Create a Shiny App that takes as input the sample size, then simulates a sample of that size from a standard normal distribution. It gives a histogram of the data and the output of summary()."
  },
  {
    "objectID": "09_shiny/09_reactivity.html#notes-reactivity",
    "href": "09_shiny/09_reactivity.html#notes-reactivity",
    "title": "Reactivity and the server() Function",
    "section": "Notes Reactivity",
    "text": "Notes Reactivity\n\nShiny only runs reactive code in the server() function when the inputs have changed.\nIn usual R, the order of operations is defined by the order of the lines of code. This is a form of “imperative” programming.\nIn Shiny, the order of operations is defined by the order of when things are needed to run. This is a form of “declarative” programming."
  },
  {
    "objectID": "09_shiny/09_reactivity.html#timed-invalidation",
    "href": "09_shiny/09_reactivity.html#timed-invalidation",
    "title": "Reactivity and the server() Function",
    "section": "Timed Invalidation",
    "text": "Timed Invalidation\n\nWhen an input changes, Shiny calls this “invalidation” and it causes the render*() functions and reactive elements to run.\nYou can cause invalidation in time intervals (so the reactive elements will reevaluate) using reactiveTimer().\nreactiveTimer():\n\nCreates a reactive element that invalidates in time intervals.\nYou just place this reactive element in a new reactive element to invalidate that one in time intervals.\n\nLets resimulate new data every second\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(broom)\n\nui &lt;- fluidPage(\n  numericInput(\"nsamp\", \"Number of samples\", value = 50, step = 1),\n  numericInput(\"diff\", \"Effect size\", value = 0.5, step = 0.1),\n  plotOutput(\"plot\"),\n  verbatimTextOutput(\"text\")\n)\n\nserver &lt;- function(input, output) {\n  timer &lt;- reactiveTimer(1000)\n\n  x1 &lt;- reactive({\n    timer()\n    rnorm(n = input$nsamp, mean = 0, sd = 1)\n  })\n\n  x2 &lt;- reactive({\n    timer()\n    rnorm(n = input$nsamp, mean = input$diff, sd = 1)\n  })\n\n  output$plot &lt;- renderPlot({\n    data.frame(`1` = x1(), `2` = x2()) |&gt;\n      gather(key = \"Group\", value = \"y\") |&gt;\n      ggplot(aes(x = Group, y = y)) +\n      geom_boxplot() +\n      theme_bw()\n  })\n\n  output$text &lt;- renderPrint({\n    t.test(x1(), x2()) |&gt;\n      tidy() |&gt;\n      select(estimate, `P-value` = p.value, Lower = conf.low, Higher = conf.high)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nExercise: Create a Shiny App that takes as input the sample size, then simulates a sample of that size from a standard normal distribution. It gives a histogram of the data and the output of summary(). Make it automatically simulate new data once every two seconds."
  },
  {
    "objectID": "09_shiny/09_reactivity.html#on-click",
    "href": "09_shiny/09_reactivity.html#on-click",
    "title": "Reactivity and the server() Function",
    "section": "On Click",
    "text": "On Click\n\nIf an evaluation takes a long time, you might want the user to click a button before implementing it.\nOtherwise, Shiny will be trying to catch up to the changes in the inputs.\nUse an actionButton() in the UI along with eventReactive() in the server() function to do this.\neventReactive():\n\nIs used in place of reactive().\nTakes the actionButtion() ID as its first argument.\nTakes the expression to evaluate as its second argument.\n\n\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(broom)\n\nui &lt;- fluidPage(\n  numericInput(\"nsamp\", \"Number of samples\", value = 50, step = 1),\n  numericInput(\"diff\", \"Effect size\", value = 0.5, step = 0.1),\n  actionButton(\"simulate\", \"Simulate!\"),\n  plotOutput(\"plot\"),\n  verbatimTextOutput(\"text\")\n)\n\nserver &lt;- function(input, output) {\n\n  x1 &lt;- eventReactive(eventExpr = input$simulate, \n                      valueExpr = {\n    rnorm(n = input$nsamp, mean = 0, sd = 1)\n  })\n\n  x2 &lt;- eventReactive(eventExpr = input$simulate,\n                      valueExpr = {\n    rnorm(n = input$nsamp, mean = input$diff, sd = 1)\n  })\n\n  output$plot &lt;- renderPlot({\n    data.frame(`1` = x1(), `2` = x2()) |&gt;\n      gather(key = \"Group\", value = \"y\") |&gt;\n      ggplot(aes(x = Group, y = y)) +\n      geom_boxplot() +\n      theme_bw()\n  })\n\n  output$text &lt;- renderPrint({\n    t.test(x1(), x2()) |&gt;\n      tidy() |&gt;\n      select(estimate, `P-value` = p.value, Lower = conf.low, Higher = conf.high)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n \nExercise: Create a Shiny App that takes as input the sample size, then simulates a sample of that size from a standard normal distribution. It gives a histogram of the data and the output of summary(). Make it it only update simulated data on an action button click."
  },
  {
    "objectID": "09_shiny/09_reactivity.html#observe",
    "href": "09_shiny/09_reactivity.html#observe",
    "title": "Reactivity and the server() Function",
    "section": "Observe",
    "text": "Observe\n\nUse observeEvent() instead of eventReactive() if you want to run code that does not need to be saved to some output.\n\nNote: You cannot save the output of a call to observeEvent().\n\nThis could be saving data to a file, or printing to the console, or downloading a prespecified file from the internet.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  actionButton(\"greet\", \"Comfort Me\")\n)\n\nserver &lt;- function(input, output) {\n  observeEvent(input$greet,\n               {\n               print(\"You are loved and special!\")\n               })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "09_shiny/09_reactivity.html#prevent-reactions",
    "href": "09_shiny/09_reactivity.html#prevent-reactions",
    "title": "Reactivity and the server() Function",
    "section": "Prevent Reactions",
    "text": "Prevent Reactions\n\nYou can use isolate() to prevent inputs from invalidating outputs.\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  sliderInput(\"bins\", \"Bins\", min = 1, max = 50, value = 20),\n  textInput(\"title\", \"Title\", value = \"Histogram of MPG\"),\n  plotOutput(\"plot\")\n)\n\nserver &lt;- function(input, output) {\n  output$plot &lt;- renderPlot({\n    ggplot(mtcars, aes(x = mpg)) +\n      geom_histogram(bins = input$bins) +\n      ggtitle(isolate( {input$title} ))\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nisolate():\n\nWill just make sure a chunk of code does not invalidate the output.\nIf other parts of the chunk (outside of isolate()) invalidate the output, then it will still update the input elements inside isolate().\n\nIn the example above, this means that changing the title won’t change the plot. But when we move the slider, it will update the bin width and the plot title.\nYou can use req() to require an input before evaluating (so if there is no input provided, the code chunk will not be invalidated.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  textInput(inputId = \"text\", label = \"Give me a word\"),\n  textOutput(outputId = \"newtext\")\n)\n\nserver &lt;- function(input, output, session) {\n\n  output$newtext &lt;- renderText({\n    paste0(\"Your word is \", req(input$text), \"!\")\n  })\n\n}\n\nshinyApp(ui, server)\n\nExercise: Create an app where the user chooses the variable to plot from mtcars and the number of bins, it then outputs a histogram. However, the histogram is not displayed until the user clicks an action button. The variable selection and the number of bins should not make the plot change. The plot should only change when the user clicks the action button."
  },
  {
    "objectID": "09_shiny/09_html.html",
    "href": "09_shiny/09_html.html",
    "title": "Editing HTML Elements",
    "section": "",
    "text": "Learning Objectives\n\nEdit style of Shiny App with HTML elements.\nChapter 13 of Mastering Shiny.\nShiny Cheatsheet.\nOptional Resources\n\nShiny Tutorial.\nShiny Examples.\n\n\n\n\nHTML\n\nHTML (Hypertext Markup Language) is a coding language like markdown used to style web documents.\nWhen you look at a webpage, that is a browser interpreting HTML code.\nThe user interface in all Shiny Apps is basically just a way to write HTML code using R.\n\nlibrary(shiny)\nui &lt;- fluidPage(\n    titlePanel(\"Hello\"),\n    textInput(\"text\", \"What Text?\")\n)\nui\n\n\nHello\n\nWhat Text?\n\n\n\n\n\nElements surounded by “&lt;&gt;” are called HTML “tags”. For example:\n\n&lt;strong&gt;...&lt;/strong&gt;: Makes text bold.\n&lt;strong&gt;Sesquipedalian&lt;/strong&gt;\n\nSesquipedalian\n\n&lt;u&gt;...&lt;/u&gt;: Makes text underlined.\n&lt;u&gt;Sesquipedalian&lt;/u&gt;\n\nSesquipedalian\n\n&lt;s&gt;...&lt;/s&gt;: Makes text strikeout.\n&lt;s&gt;Sesquipedalian&lt;/s&gt;\n\nSesquipedalian\n\n&lt;code&gt;...&lt;/code&gt;: Makes text monospaced.\n&lt;code&gt;Sesquipedalian&lt;/code&gt;\n\nSesquipedalian\n\n&lt;h1&gt;...&lt;/h1&gt;, &lt;h2&gt;...&lt;/h2&gt;, &lt;h3&gt;...&lt;/h3&gt;: Creates headings, subheadings, subsubheadings.\n&lt;h3&gt;Sesquipedalian&lt;/h3&gt;\n\n\nSesquipedalian\n\n\n&lt;p&gt;...&lt;/p&gt;: Makes paragraphs.\n&lt;strong&gt;Sesquipedalian&lt;/strong&gt;\n\n\nSesquipedalian\n\n\nThe following creates an itemized list:\n&lt;ul&gt;\n&lt;li&gt;Item 1&lt;/li&gt;\n&lt;li&gt;Item 2&lt;/li&gt;\n&lt;/ul&gt;\n\n\n\nItem 1\n\n\nItem 2\n\n\n\n&lt;br&gt;&lt;/br&gt;: Inserts a line break.\nSome text\n&lt;br&gt;&lt;/br&gt;\nMore text\n\nSome text  More text\n\n&lt;hr&gt;&lt;/hr&gt;: Inserts a horizontal “rule” (line).\n&lt;hr&gt;&lt;/hr&gt;\n\n\n\n\n&lt;img&gt;&lt;/img&gt;: Inserts images (see below).\n&lt;a&gt;...&lt;/a&gt;: The “anchor” tag. Creates hyperlinks (see below).\nTag arguments are called “attributes” and are placed after the tag name in the first &lt;&gt;. For example, for hyperlinks you need to give it the URL with the href attribute:\n&lt;a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"&gt;Click Me!&lt;/a&gt;\n\nClick Me!\n\n\nAnother example with images:\n&lt;img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c1/Ohio_State_Buckeyes_logo.svg\" \n     height=\"100\" \n     width=\"100\"&gt;\n&lt;/img&gt;\n \n\n\n\nHTML in R\n\nWhen you load the shiny library, it will load a list of functions called tags.\nEach function in tags is an HTML tag. For example, to create an anchor tag use:\n\ntags$a(\"Click Me!\", href = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n\n\n\n&lt;a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"&gt;Click Me!&lt;/a&gt;\n\n\n\nClick Me!\n\nAny named argument becomes an HTML attribute.\nAny unnamed argument is placed between the tags.\nYou can put tags inside tags.\n\ntags$a(tags$h1(\"Click Me!\"), \"Now!\",\n       href = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n\n\n\n&lt;a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"&gt;\n  &lt;h1&gt;Click Me!&lt;/h1&gt;\n  Now!\n&lt;/a&gt;\n\n\n\n\n\nClick Me!\n\nNow! \n\nExercise: Create an itemized list using function elements from tags.\n\n\n\nUsing HTML in a Shiny App\n\nJust put these HTML elements inside the fluidPage() call.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$h1(\"I am a title\"),\n  tags$a(\"I am a link\", href = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n)\n\nserver &lt;- function(input, output, session) {\n\n}\n\nshinyApp(ui, server)\n\n \nExercise: Use the img tag to insert the Ohio State logo into a Shiny App. The url can be found here:\nhttps://upload.wikimedia.org/wikipedia/commons/c/c1/Ohio_State_Buckeyes_logo.svg\nTo add an image not from a webpage, add a “www” folder inside your Shiny app. Add all images into that folder. Reference to those images by name only (not by the path) in the img tag.\n \n \n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tags$img(src = \"osu.png\")\n)\n\nserver &lt;- function(input, output, session) {\n\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "09_shiny/09_upload.html",
    "href": "09_shiny/09_upload.html",
    "title": "File Upload",
    "section": "",
    "text": "Learning Objectives\n\nLearn how to upload datasets for use in the Shiny App.\nChapter 8 of Mastering Shiny.\nShiny Cheatsheet.\nOptional Resources\n\nShiny Tutorial.\nShiny Examples.\n\n\n\n\nStatic Dataset Upload\n\nIf your Shiny app is designed to analyze only a single dataset (which might contain multiple files), then you should upload it at the start of the app.\n\nCreate a new “data” folder in your app.\nPlace all data files in the “data” folder.\nRead in these data at the start of the app.\n\nNew data folder:\n \nPut data in data folder:\n \nLoad data at beginning of app:\n\nlibrary(shiny)\nlibrary(readr)\nestate &lt;- read_csv(\"./data/estate.csv\")\n\nui &lt;- fluidPage(\n  tableOutput(\"estate\")\n)\n\nserver &lt;- function(input, output, session) {\n  output$estate &lt;- renderTable({\n    head(estate)\n  })\n}\n\nshinyApp(ui, server)\n\nWhen a Shiny app is run, the location of “app.R” is the location of the working directory, so all file upload/download must be done from that location as a reference point.\n\n\n\nInteractive File Upload\n\nUse the fileInput() function in the UI to allow users to choose the source of the dataset.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  fileInput(\"file\", \"Where is the file?\")\n)\n\nserver &lt;- function(input, output, session) {\n\n}\n\nshinyApp(ui, server)\n\n \nIn the server() function, the element in input (in this case input$file) is a data frame with the following columns:\n\nname: The name of the file on the user’s computer.\nsize: The size of the file in bytes. Shiny only accepts files up to 5 MB. To increase it to 10 MB, type the following somewhere in “app.R”:\n\noptions(shiny.maxRequestSize = 10 * 1024^2)\n\ntype: The file extension (text/csv, text/plain etc)\ndatapath: A temporary path file.\n\nUseful arguments:\n\naccept: What file extensions are acceptable (\".csv\", \".txt\", etc).\nbuttonLabel: Customize label of button.\nmultiple: Can the user upload multiple files?\n\nExample Shiny App\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  fileInput(\"upload\", NULL, buttonLabel = \"Upload...\", multiple = TRUE),\n  tableOutput(\"files\")\n)\n\nserver &lt;- function(input, output, session) {\n  output$files &lt;- renderTable(input$upload)\n}\n\nshinyApp(ui = ui, server = server)\n\nGeneral strategies:’\n\nPlace code to read in files inside a reactive() call.\nInside the reactive call, always use req() to wait to read in data until the path is available.\nSave the read-in files as reactive elements.\nIn the server function, you use the datapath value as the path argument in read_csv(), read_tsv(), readRDS(), etc.\n\n\nlibrary(shiny)\nlibrary(readr)\n\nui &lt;- fluidPage(\n  fileInput(\"file\", \"What file?\", accept = \"text/csv\"),\n  tableOutput(\"head\")\n)\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    req(input$file)\n    read_csv(input$file$datapath)\n  })\n\n  output$head &lt;- renderTable({\n    head(data(), 5)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "A4_dates/A4_dates.html",
    "href": "A4_dates/A4_dates.html",
    "title": "Dates",
    "section": "",
    "text": "Learning Objectives\n\nManipulating dates and times.\nChapter 18 of RDS.\nDates and Times Cheat Sheet.\nLubridate Overview.\n\n\n\nParsing Dates\n\nThe {lubridate} package has a bunch of convenience functions for working with dates. It’s a part of the tidyverse, so is loaded along with it.\n\nlibrary(tidyverse)\n\nThere are three main classes for date/time data:\n\nDate for just the date.\n\nIn tibbles, this shows up as &lt;date&gt;.\n\nPOSIXct for both the date and the time. “POSIXct” stands for “Portable Operating System Interface Calendar Time” (don’t ask me where the “X” comes from). It is a part of a standardized system of representing time across many computing computing platforms.\n\nIn tibbles, this shows up as &lt;dttm&gt;.\n\nhms from the hms R package for just the time. “hms” stands for “hours, minutes, and seconds.”\n\nIn tibbles, this shows up as &lt;time&gt;.\n\n\ntoday() will give you the current date in the Date class.\n\ntoday()\n\n[1] \"2025-06-03\"\n\nclass(today())\n\n[1] \"Date\"\n\n\nnow() will give you the current date-time in the POSIXct class.\n\nnow()\n\n[1] \"2025-06-03 12:30:12 EDT\"\n\nclass(now())\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\nThere is no built-in R function to find the current time without the date. But you can use hms::as_hms(now()) to get the current time.\n\nhms::as_hms(now())\n\n12:30:12.79113\n\nclass(hms::as_hms(now()))\n\n[1] \"hms\"      \"difftime\"\n\n\n\n\n\nParsing Dates using {readr}\n\nYou can use parse_date(), parse_datetime(), and parse_time() (from {readr}) to parse a date/date-time/time from a string.\n\nx &lt;- parse_date(\"10/11/2020\", format = \"%m/%d/%Y\")\nx\n\n[1] \"2020-10-11\"\n\nclass(x)\n\n[1] \"Date\"\n\ny &lt;- parse_datetime(\"10/11/2020 11:59:20\", format = \"%m/%d/%Y %H:%M:%S\")\ny\n\n[1] \"2020-10-11 11:59:20 UTC\"\n\nclass(y)\n\n[1] \"POSIXct\" \"POSIXt\" \n\nz &lt;- parse_time(\"11:59:20\", \"%H:%M:%S\")\nz\n\n11:59:20\n\nclass(z)\n\n[1] \"hms\"      \"difftime\"\n\n\nTable 18.1 from RDS (2e): All date formats understood by readr:\n\n\n\nType\nCode\nMeaning\nExample\n\n\n\n\nYear\n%Y\n4 digit year\n2021\n\n\n\n%y\n2 digit year\n21\n\n\nMonth\n%m\nNumber\n2\n\n\n\n%b\nAbbreviated name\nFeb\n\n\n\n%B\nFull name\nFebruary\n\n\nDay\n%d\nTwo digits\n02\n\n\n\n%e\nOne or two digits\n2\n\n\nTime\n%H\n24-hour hour\n13\n\n\n\n%I\n12-hour hour\n1\n\n\n\n%p\nAM/PM\npm\n\n\n\n%M\nMinutes\n35\n\n\n\n%S\nSeconds\n45\n\n\n\n%OS\nSeconds with decimal component\n45.35\n\n\n\n%Z\nTime zone name\nAmerica/Chicago\n\n\n\n%z\nOffset from UTC\n+0800\n\n\nOther\n%.\nSkip one non-digit\n:\n\n\n\n%*\nSkip any number of non-digits\n\n\n\n\nExercise: Parse this\n\nt2 &lt;- \"11:15:10.12 PM\"\n\n\n\n\nParsing dates using {lubridate}\n\n{lubridate} comes with a bunch of helper functions to parse dates more automatically. The helper function name itself specifies the order of the year, month, day, hours, minutes, and seconds.\nTo parse dates, look at the help page of\n\nhelp(ymd)\n\n\n## Only the order of year, month, and day matters\nymd(c(\"2011/01-10\", \"2011-01/10\", \"20110110\"))\n\n[1] \"2011-01-10\" \"2011-01-10\" \"2011-01-10\"\n\nmdy(c(\"01/10/2011\", \"01 adsl; 10 df 2011\", \"January 10, 2011\"))\n\n[1] \"2011-01-10\" \"2011-01-10\" \"2011-01-10\"\n\n\nTo parse times, look at the help page of\n\nhelp(ms)\n\n\n## only the order of hours, minutes, and seconds matter\nhms(c(\"10:40:10\", \"10 40 10\"))\n\n[1] \"10H 40M 10S\" \"10H 40M 10S\"\n\n\nNote that ms(), hm(), and hms() won’t recognize “-” as a separator because it treats it as negative time. So use parse_time() here.\n\nms(\"10-10\")\n\n[1] \"10M -10S\"\n\n\nTo parse date-times, look at the help page of\n\nhelp(ymd_hms)\n\nMore generally, you can choose the order of elements with parse_date_time(), which has a different and easier syntax than readr::parse_datetime().\n\nparse_date_time(\"11, 22, 01 here is a trap! 11/02/2002\", orders = \"HMSmdy\")\n\n[1] \"2002-11-02 11:22:01 UTC\"\n\n\nExercise: Parse the following date-times.\n\n\"05/26/2004 UTC 11:11:11.444\"\n\"26 2004 05 UTC 11/11/11.444\"\n\nExercise (RDS1e16.2.4.3): Use the appropriate lubridate function to parse each of the following dates:\n\nd1 &lt;- \"January 1, 2010\"\nd2 &lt;- \"2015-Mar-07\"\nd3 &lt;- \"06-Jun-2017\"\nd4 &lt;- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 &lt;- \"12/30/14\" # Dec 30, 2014 \n\n\n\n\nDates from individual components\n\nIf you have a vector of years, months, days, hours, minutes, or seconds, you can use make_date() or make_datetime() to create dates and date-times.\n\nmake_date(year = 1981, month = 6, day = 25)\n\n[1] \"1981-06-25\"\n\nmake_datetime(year = 1972, month = 2, day = 22, hour = 10, min = 9, sec = 01)\n\n[1] \"1972-02-22 10:09:01 UTC\"\n\n\nnycflights13 example:\n\nlibrary(nycflights13)\ndata(\"flights\")\nflights |&gt;\n  mutate(datetime = make_datetime(year   = year, \n                                  month  = month, \n                                  day    = day,\n                                  hour   = hour,\n                                  min    = minute)) -&gt;\n  flights\n  select(flights, datetime)\n\n# A tibble: 336,776 × 1\n   datetime           \n   &lt;dttm&gt;             \n 1 2013-01-01 05:15:00\n 2 2013-01-01 05:29:00\n 3 2013-01-01 05:40:00\n 4 2013-01-01 05:45:00\n 5 2013-01-01 06:00:00\n 6 2013-01-01 05:58:00\n 7 2013-01-01 06:00:00\n 8 2013-01-01 06:00:00\n 9 2013-01-01 06:00:00\n10 2013-01-01 06:00:00\n# ℹ 336,766 more rows\n\n\nHaving it in the date-time format makes it easier to plot.\n\nggplot(flights, aes(x = datetime)) +\n  geom_freqpoly(bins = 365)\n\n\n\n\n\n\n\n\nIt makes it easier to filter by date\n\nflights |&gt;\n  filter(as_date(datetime) == ymd(20130704)) |&gt;\n  ggplot(aes(x = datetime)) +\n  geom_freqpoly(binwidth = 600)\n\n\n\n\n\n\n\n\nI used as_date() in the previous example. This function will try to coerce an object to a date. Sometimes successfully! It is particularly useful for extracting the date component of a POSIXct object.\nas_datetime() tries to coerce an object to a POSIXct object.\nExercise: Create a date variable from the following data frame. Then filter out all rows before Feb 1, 2010. If you finish early, try converting the month variable to the numeric representation of the month. (Hint: use {stringr} to fix the month variable then use the built-in vector month.abb).\n\nfake &lt;- tribble(~year, ~month, ~day, ~month_num,\n                ##----/-------/----------------\n                2018,  \"Oct\",  1,    10,\n                2011,  \"Nov\",  2,    11,\n                2019,  \"Dec\",  3,    12,\n                2010,  \"JAN\",  5,     1,\n                1999,  \"MAr\",  1,     3,\n                1987,  \"ApR\",  3,     4,\n                2020,  \"maY\",  2,     5,\n                2010,  \"May\",  4,     5)  \n\n\n\n\nExtracting Components\n\nyear() extracts the year.\nmonth() extracts the month.\nweek() extracts the week.\nmday() extracts the day of the month (1, 2, 3, …).\nwday() extracts the day of the week (Saturday, Sunday, Monday …).\nyday() extracts the day of the year (1, 2, 3, …)\nhour() extracts the hour.\nminute() extract the minute.\nsecond() extracts the second.\n\n\nddat &lt;- mdy_hms(\"01/02/1970 03:51:44\")\nddat\n\n[1] \"1970-01-02 03:51:44 UTC\"\n\nyear(ddat)\n\n[1] 1970\n\nmonth(ddat, label = TRUE)\n\n[1] Jan\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\nweek(ddat)\n\n[1] 1\n\nmday(ddat)\n\n[1] 2\n\nwday(ddat, label = TRUE)\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\nyday(ddat)\n\n[1] 2\n\nhour(ddat)\n\n[1] 3\n\nminute(ddat)\n\n[1] 51\n\nsecond(ddat)\n\n[1] 44\n\n\n\nExercise: Load the wmata_ridership data frame into R from https://dcgerard.github.io/stat_412_612/data/wmata_ridership.csv. For each month, calculate the proportion of rides made on a given day of the month. Then make box plots of the proportions of ridership vs day of the weak. But exclude any days from 2004.\nYou can overwrite components.\n\nddat &lt;- mdy_hms(\"01/02/1970 03:51:44\")\nddat\n\n[1] \"1970-01-02 03:51:44 UTC\"\n\nyear(ddat) &lt;- 1988\nddat\n\n[1] \"1988-01-02 03:51:44 UTC\"\n\n\nTo create a new date with the updated component, rather than overwrite a component, use update().\n\nddat\n\n[1] \"1988-01-02 03:51:44 UTC\"\n\nupdate(ddat, year = 1999)\n\n[1] \"1999-01-02 03:51:44 UTC\"\n\nddat ## still 1988\n\n[1] \"1988-01-02 03:51:44 UTC\"\n\n\nThe book provides an example of using update() on larger elements to see fine scale patterns\n\nflights |&gt;\n  mutate(dt = update(datetime, yday = 1)) |&gt;\n  ggplot(aes(x = dt)) +\n  geom_freqpoly(binwidth = 300)\n\n\n\n\n\n\n\n\nYou can round components with round_date(). You round to the nearest “unit” (e.g., year or day).\n\nddat &lt;- mdy_hms(\"01/02/1970 03:51:44\")\nddat\n\n[1] \"1970-01-02 03:51:44 UTC\"\n\nround_date(ddat, unit = \"year\")\n\n[1] \"1970-01-01 UTC\"\n\n\nYou can round down using floor_date() and round up with ceiling_date()\n\nfloor_date(ddat, unit = \"year\")\n\n[1] \"1970-01-01 UTC\"\n\nceiling_date(ddat, unit = \"year\")\n\n[1] \"1971-01-01 UTC\"\n\n\n\n\n\nTime Spans\n\nTo count the number of seconds between two dates, use a duration. You can read about durations using\n\nhelp(\"Duration-class\")\n\nYou first subtract two dates, then use as.duration() to create a duration.\nWe can find out how old Patrick Stewart is using durations\n\nd1 &lt;- ymd(19400713)\nd2 &lt;- today()\nagesec &lt;- as.duration(d2 - d1)\nagesec\n\n[1] \"2678918400s (~84.89 years)\"\n\n\nYou can also create durations from years with dyears(), from days with ddays(), etc…\n\ndyears(1)\n\n[1] \"31557600s (~1 years)\"\n\nddays(1)\n\n[1] \"86400s (~1 days)\"\n\ndhours(1)\n\n[1] \"3600s (~1 hours)\"\n\ndminutes(1)\n\n[1] \"60s (~1 minutes)\"\n\ndseconds(1)\n\n[1] \"1s\"\n\n\nYou can add durations to date-times, but you always add seconds, so if there is daylight savings you get weird results (add a day but the time is not the same as the time the previous day).\n\none_pm &lt;- ymd_hms(\"2016-03-12 13:00:00\", tz = \"America/New_York\")\none_pm\n\n[1] \"2016-03-12 13:00:00 EST\"\n\none_pm + ddays(1)\n\n[1] \"2016-03-13 14:00:00 EDT\"\n\n\nPeriods are human readable time spans. You create periods with\n\nyears(1)\n\n[1] \"1y 0m 0d 0H 0M 0S\"\n\ndays(1)\n\n[1] \"1d 0H 0M 0S\"\n\nhours(1)\n\n[1] \"1H 0M 0S\"\n\nminutes(1)\n\n[1] \"1M 0S\"\n\nseconds(1)\n\n[1] \"1S\"\n\n\nAdding a period takes into account daylight savings.\n\none_pm\n\n[1] \"2016-03-12 13:00:00 EST\"\n\none_pm + days(1)\n\n[1] \"2016-03-13 13:00:00 EDT\"\n\n\nYou can read more about periods with\n\nhelp(\"Period-class\")\n\nIntervals are like durations, but they also have an associated start time and end time. You can read more about intervals with\n\nhelp(\"Interval-class\")\n\nYou create an interval with start_date %--% end_date. E.g.\nThe main use of intervals is when you want to do division.\n\nDivide an interval by a duration to determine its physical length.\nDivide an interval by a period to determine its implied length in clock time.\n\nE.g., the number of days between between Jan 1 2019 and Jan 1 2020 is\n\n(ymd(\"2019-01-01\") %--% ymd(\"2020-01-01\")) / days(1)\n\n[1] 365\n\n\nwhile the number of days between Jan 1 2020 and Jan 1 2021 is\n\n(ymd(\"2020-01-01\") %--% ymd(\"2021-01-01\")) / days(1)\n\n[1] 366\n\n\nbecause of the leap year.\nExercise: How long of a time-span is covered in the WMATA ridership dataset?\n\n\n\nTime Zones\n\nTime zones are specified using the tz or tzone arguments (for example, in the call to ymd_hms() above).\nTime zones are specified by “content/city.” For example, \"America/New_York\" and \"Europe_Paris\"\nYou can see a complete list of time zones with OlsonNames().\nThe default time zone is UTC (which has no daylight savings).\nYou usually don’t have to worry about timezones unless you loaded them in incorrectly. For example, R might think it’s UTC even though it should be America/New_York and then forget daylight savings.\nIf a date-time is labelled with the incorrect time zone, use force_tz().\n\nd1 &lt;- ymd_hms(\"20140101 10:01:11\")\nd1\n\n[1] \"2014-01-01 10:01:11 UTC\"\n\nforce_tz(d1, tzone = \"America/New_York\")\n\n[1] \"2014-01-01 10:01:11 EST\"\n\n\nIf the timezone is correct, but you want to change it, use with_tz().\n\nwith_tz(d1, tzone = \"America/New_York\")\n\n[1] \"2014-01-01 05:01:11 EST\"\n\n\n\n\n\nRegnal Year Exercise\nConsider the regnal.csv, a table of regnal years of English monarchs, taken from Wikipedia: https://en.wikipedia.org/wiki/Regnal_years_of_English_monarchs\n“Regnal years” are years that correspond to a monarch, and might differ from the actual reign of that monarch. It’s mostly used for dating legal documents (“nth year of the reign of King X”). It’s a weird English thing. The variables include:\n\nmonarch: The name of the monarch.\nnum_years: The number of years of the reign.\nfirst: The start year of the reign.\nstart_date: The date when each regnal year begins.\nend_date: The date when each regnal year ends.\nfinal: The final date of the reign.\n\nClean these data to get the start and end dates of each reign in proper date format. E.g.\n\n\n# A tibble: 43 × 4\n   monarch    num_years start      end       \n   &lt;chr&gt;      &lt;chr&gt;     &lt;date&gt;     &lt;date&gt;    \n 1 William I  21        1066-10-14 1087-09-09\n 2 William II 13        1087-09-26 1100-08-02\n 3 Henry I    36        1100-08-05 1135-12-01\n 4 Stephen    19        1135-12-26 1154-10-25\n 5 Henry II   35        1154-12-19 1189-07-06\n 6 Richard I  10        1189-09-03 1199-04-06\n 7 John       18        1199-05-27 1216-10-19\n 8 Henry III  57        1216-10-28 1272-11-16\n 9 Edward I   35        1272-11-20 1307-07-07\n10 Edward II  20        1307-07-08 1327-01-20\n# ℹ 33 more rows\n\n\nUse the start and end columns to verify that the num_years column from Wikipedia is accurate."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html",
    "href": "A3_medium_data/A3_medium_data.html",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "",
    "text": "Reading large datasets into R.\ndata.table syntax for manipulating data frames.\nA data.table and dplyr tour\nIntroduction to data.table.\nEfficient reshaping using data.tables\nInteractive Cheat Sheet"
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#reading-in-and-printing-data",
    "href": "A3_medium_data/A3_medium_data.html#reading-in-and-printing-data",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Reading in and Printing Data",
    "text": "Reading in and Printing Data\n\nLoad the data.table package and (to compare) the tidyverse into R:\n\nlibrary(tidyverse)\nlibrary(data.table)\n\nWe’ll demonstrate most methods with the data from “NYC-flights14” dataset from the data.table package vignettes. I’ve posted it on my own webpage: https://data-science-master.github.io/lectures/data/flights14.csv.\nRead in data with fread() (for “file read”). It accepts all delimiters, which you (optionally) specify with the sep argument. The default is to guess the delimiter.\n\nflights &lt;- fread(\"../data/flights14.csv\")\n\nfread() will return a data.table object.\n\nclass(flights)\n\n[1] \"data.table\" \"data.frame\"\n\n\nUse fwrite() to write a data.table object to a file.\nCompare to read_csv() in the tidyverse.\n\nflights_tib &lt;- read_csv(\"../data/flights14.csv\")\n\nread_csv() will return a tibble.\n\nclass(flights_tib)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\ntibbles and data.tables both print better than data.frames.\n\nflights\nflights_tib\n\nYou usually use the base function str() (for “structure”) to look at the data.table entries.\n\n## data.table way\nstr(flights)\n\n## Similar tidyverse way\nglimpse(flights_tib)\n\nYou can use as.data.table() to convert a tibble or a data.frame into a data.table. But there is rarely a time when you’d do this, since you use data.table for large datasets that you read in with fread().\n\ntemp_dt &lt;- as.data.table(flights_tib)\nclass(temp_dt)\n\n[1] \"data.table\" \"data.frame\""
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#filteringarranging-rows-observations",
    "href": "A3_medium_data/A3_medium_data.html#filteringarranging-rows-observations",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Filtering/Arranging Rows (Observations)",
    "text": "Filtering/Arranging Rows (Observations)\n\nJust like in the tidyverse, we use logicals to filter based on rows. The syntax for this is to place the logicals inside a bracket. Let’s find all flights that left JFK and arrived at LAX.\n\n## data.table way\nflights[origin == \"JFK\" & dest == \"LAX\"]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  filter(origin == \"JFK\", dest == \"LAX\")\n\nTo get a specific row, insert a number into the brackets.\n\n## data.table way\nflights[c(1, 3, 207)]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  slice(1, 3, 207)\n\nReorder rows by using the order() function inside the brackets. Let’s reorder the rows alphabetically by origin, and break ties in reverse alphabetical order by destination.\n\n## data.table way\nflights[order(origin, -dest)]\n\n## equivalent tidyverse way    \nflights_tib %&gt;%\n  arrange(origin, desc(dest))\n\nExercise: Use both data.table and the tidyverse to select all flights from EWR and LGA, and arrange the flights in decreasing order of departure delay."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#selecting-columns-variables",
    "href": "A3_medium_data/A3_medium_data.html#selecting-columns-variables",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Selecting Columns (Variables)",
    "text": "Selecting Columns (Variables)\n\nTo select a variable (a column), you also use bracket notation, but you place a comma before you select the columns. This idea is that you are selecting all rows (empty space before comma).\nThere are lots of ways to select columns that keeps the new object a data.table.\nList method: Use the .() function (which is an alias for list()).\n\n## data.table way\nflights[, .(origin, dest)]\n\n## Or\nflights[, list(origin, dest)]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  select(origin, dest)\n\nCharacter Vector Method: Use c() with their character names\n\nflights[, c(\"origin\", \"dest\")]\n\nRange Method: Use : to select variables within a range of columns.\n\nflights[, origin:dest]\n\nPrespecify Method: Define variables to keep outside of the data.table, then use with = FALSE. This option makes data.table not think that keep_vec is a varible in the data.table.\n\nkeep_vec &lt;- c(\"origin\", \"dest\")\nflights[, keep_vec, with = FALSE]\n\nTo remove a column using the range or character methods, place a ! before the columns to remove\n\nflights[, !c(\"year\", \"month\")]\nflights[, !(year:month)]\n\nTo remove a column using the list method, assign that variable to be NULL using modify by reference (see below). If you run the below code, you’ll need to reload the flights data to get back year and month.\n\nflights[, c(\"year\", \"month\") := .(NULL, NULL)]\n\nExercise: Use data.table to select the year, month, day, and hour columns.\nUnlike the tidyverse, you filter rows and select columns in one call rather than using two separate functions.\n\n## data.table way\nflights[origin == \"JFK\" & dest == \"LAX\", .(year, month, day, hour)]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  filter(origin == \"JFK\", dest == \"LAX\") %&gt;%\n  select(year, month, day, hour)\n\nExercise: Use data.table to print out just the departure delays from JFK."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#creating-new-variables-mutate",
    "href": "A3_medium_data/A3_medium_data.html#creating-new-variables-mutate",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Creating New Variables (Mutate)",
    "text": "Creating New Variables (Mutate)\n\nThe fastest way to create and remove variables in a data.table is by reference, where we modify the data.table, we don’t create a new data.table.\nUse := to modify a data.table by reference. You put the variable names (as a character vector) to the left of :=. You put the new variables (as a list) to the right of :=.\n\n## data.table way\nflights[, c(\"gain\") := .(dep_delay - arr_delay)]\nflights\n\n## equivalent tidyverse way    \nflights_tib %&gt;%\n  mutate(gain = dep_delay - arr_delay) -&gt;\n  flights_tib\nflights_tib\n\nQuickly remove a column by setting that variable to NULL.\n\n## data.table way\nflights[, c(\"gain\") := .(NULL)]\nflights\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  select(-gain) -&gt;\n  flights_tib\nflights_tib\n\nAdd multiple variables by separating them with columns.\n\nflights[, c(\"gain\", \"dist_km\") := .(dep_delay - arr_delay, 1.61 * distance)]\nflights\n\nflights[, c(\"gain\", \"dist_km\") := .(NULL, NULL)]\nflights\n\nExercise: Add a variable called speed that is the average air speed of the plane in miles per hour. Then remove this variable."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#group-summaries",
    "href": "A3_medium_data/A3_medium_data.html#group-summaries",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Group Summaries",
    "text": "Group Summaries\n\nYou calculate summaries in the column slot. It’s best to use the list method.\n\n## data.table way\nflights[, .(dd = mean(dep_delay))]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  summarize(dd = mean(dep_delay))\n\n\n## data.table way\nflights[, .(dd = mean(dep_delay), ad = mean(arr_delay))]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  summarize(dd = mean(dep_delay), ad = mean(arr_delay))\n\nCount the number of rows with .N.\n\n## data.table way\nflights[, .(.N)]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  count()\n\nIn data.table, you create grouped summaries by simultaneously grouping and calculating summaries in one line, not separately like in dplyr.\nIn data.table, you use the by argument to specify the grouping variable. It should also be a list.\n\n## data.table way\nflights[, .(dd = mean(dep_delay)) , by = .(origin)]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  group_by(origin) %&gt;%\n  summarize(dd = mean(dep_delay))\n\nYou can calculate more than one group summary at a time, or group by more than one variable.\n\n## data.table way\nflights[, .(dd = mean(dep_delay), .N), by = .(origin, carrier)]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  group_by(origin, carrier) %&gt;%\n  summarize(dd = mean(dep_delay), N = n())\n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\nExercise: Use data.table to calculate the median air time for each month.\nExercise: Use data.table to calculate the number of trips from each airport for the carrier code DL.\nExercise: Use data.table to calculate the mean departure delay for each origin in the months of January and February."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#recoding",
    "href": "A3_medium_data/A3_medium_data.html#recoding",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Recoding",
    "text": "Recoding\n\nSuppose you want to change the values of a variable. In the tidyverse, we used recode() to do this.\nIn data.table, we filter the rows then mutate by reference.\nLet’s substitute 24 in hour to 0.\n\nsort(unique(flights$hour))\nsort(unique(flights_tib$hour))\n\n\n## data.table way\nflights[hour == 24L, hour := 0L]\n\n## equivalent tidyverse way\nflights_tib %&gt;%\n  mutate(hour = recode(hour, `24` = 0L)) -&gt;\n  flights_tib\n\n\nsort(unique(flights$hour))\nsort(unique(flights_tib$hour))\n\nExercise: In the origin variable, change \"JFK\" to \"John F. Kennedy\", \"LGA\" to \"LaGuardia\", and \"EWR\" to \"Newark Liberty\"."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#gathering",
    "href": "A3_medium_data/A3_medium_data.html#gathering",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Gathering",
    "text": "Gathering\n\nProblem: One variable spread across multiple columns.\nColumn names are actually values of a variable\nRecall table4a and table4b from the tidyr package\n\ndt4a &lt;- as.data.table(tidyr::table4a)\ndt4b &lt;- as.data.table(tidyr::table4b)\ndt4a\n\n       country   1999   2000\n        &lt;char&gt;  &lt;num&gt;  &lt;num&gt;\n1: Afghanistan    745   2666\n2:      Brazil  37737  80488\n3:       China 212258 213766\n\ndt4b\n\n       country      1999      2000\n        &lt;char&gt;     &lt;num&gt;     &lt;num&gt;\n1: Afghanistan 1.999e+07 2.060e+07\n2:      Brazil 1.720e+08 1.745e+08\n3:       China 1.273e+09 1.280e+09\n\n\nSolution: melt():\n\n## data.table way\nmelt(dt4a, \n     id.vars       = c(\"country\"),\n     measure.vars  = c(\"1999\", \"2000\"),\n     variable.name = \"year\",\n     value.name    = \"count\")\n\n## Equivalent tidyverse way\ntidyr::table4a %&gt;%\n  gather(`1999`, `2000`, key = \"year\", value = \"count\")\n\n## or\ntidyr::table4a %&gt;%\n  pivot_longer(cols = c(\"1999\", \"2000\"), \n               names_to = \"year\", \n               values_to = \"count\")\n\nRDS visualization:\n\n\nExercise: gather the monkeymem data frame (available at https://data-science-master.github.io/lectures/data/tidy_exercise/monkeymem.csv). The cell values represent identification accuracy of some objects (in percent of 20 trials)."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#spreading",
    "href": "A3_medium_data/A3_medium_data.html#spreading",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Spreading",
    "text": "Spreading\n\nProblem: One observation is spread across multiple rows.\nOne column contains variable names. One column contains values for the different variables.\nRecall table2 from the tidyr package\n\ndt2 &lt;- as.data.table(tidyr::table2)\ndt2\n\n        country  year       type     count\n         &lt;char&gt; &lt;num&gt;     &lt;char&gt;     &lt;num&gt;\n 1: Afghanistan  1999      cases 7.450e+02\n 2: Afghanistan  1999 population 1.999e+07\n 3: Afghanistan  2000      cases 2.666e+03\n 4: Afghanistan  2000 population 2.060e+07\n 5:      Brazil  1999      cases 3.774e+04\n 6:      Brazil  1999 population 1.720e+08\n 7:      Brazil  2000      cases 8.049e+04\n 8:      Brazil  2000 population 1.745e+08\n 9:       China  1999      cases 2.123e+05\n10:       China  1999 population 1.273e+09\n11:       China  2000      cases 2.138e+05\n12:       China  2000 population 1.280e+09\n\n\nSolution: dcast(). In the formula argument, put the “id variables” to the left and the “key” variables to the right. In tidyverse jargon, the value is everything not stated in the formula and the key is everything to the left of the tilde.\n\n## data.table way\ndcast(dt2, formula = country + year ~ type, value.var = \"count\")\n\n## Equivalent tidyverse way\ntidyr::table2 %&gt;%\n  spread(key = type, value = count)\n\n## or\ntidyr::table2 %&gt;%\n  pivot_wider(id_cols = c(\"country\", \"year\"), \n              names_from = \"type\", \n              values_from = \"count\")\n\nRDS visualization:\n \nExercise: Spread the flowers1 data frame (available at https://data-science-master.github.io/lectures/data/tidy_exercise/flowers1.csv)."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#separating",
    "href": "A3_medium_data/A3_medium_data.html#separating",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Separating",
    "text": "Separating\n\nTo separate a column into two columns, use the base function tstrsplit().\n\ndt3 &lt;- as.data.table(tidyr::table3)\ndt3\n\n       country  year              rate\n        &lt;char&gt; &lt;num&gt;            &lt;char&gt;\n1: Afghanistan  1999      745/19987071\n2: Afghanistan  2000     2666/20595360\n3:      Brazil  1999   37737/172006362\n4:      Brazil  2000   80488/174504898\n5:       China  1999 212258/1272915272\n6:       China  2000 213766/1280428583\n\n\n\n## data.table way\ndt3[, c(\"cases\", \"population\") := tstrsplit(rate, split = \"/\")]\ndt3[, rate := NULL]\ndt3\n\n## equivalent tidyverse way\ntidyr::table3 %&gt;%\n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\nRDS visualization:\n \nExercise: Separate the flowers2 data frame (available at https://data-science-master.github.io/lectures/data/tidy_exercise/flowers2.csv)."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#uniting",
    "href": "A3_medium_data/A3_medium_data.html#uniting",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Uniting",
    "text": "Uniting\n\nTo unite, use paste().\n\ndt5 &lt;- as.data.table(tidyr::table5)\ndt5\n\n       country century   year              rate\n        &lt;char&gt;  &lt;char&gt; &lt;char&gt;            &lt;char&gt;\n1: Afghanistan      19     99      745/19987071\n2: Afghanistan      20     00     2666/20595360\n3:      Brazil      19     99   37737/172006362\n4:      Brazil      20     00   80488/174504898\n5:       China      19     99 212258/1272915272\n6:       China      20     00 213766/1280428583\n\n\n\n## data.table way\ndt5[, year := paste(century, year, sep = \"\")]\ndt5[, century := NULL]\ndt5\n\n## Equivalent tidyverse way\ntidyr::table5 %&gt;%\n  unite(century, year, col = \"year\", sep = \"\")\n\nRDS visualization:\n\n\nExercise: Re-unite the data frame you separated from the flowers2 exercise. Use a comma for the separator."
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#chaining",
    "href": "A3_medium_data/A3_medium_data.html#chaining",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Chaining",
    "text": "Chaining\n\nIn the tidyverse, we chain commands by using the pipe %&gt;%. In data.table, we chain commands by adding additional brackets after the brackets we used. Data.table makes this very efficient.\nLet’s calculate the mean arrival delay for american airlines for each origin/destination pair, then order the results by origin in increasing order, breaking ties by destination in decreasing order.\n\n## data.table way\nflights[carrier == \"AA\", .(ad = mean(arr_delay)), by = .(origin, dest)][order(origin, -dest)]\n\n## Usual indentation for readability:\nflights[carrier == \"AA\", .(ad = mean(arr_delay)), by = .(origin, dest)\n        ][order(origin, -dest)]\n\n## Equivalent tidyverse way\nflights_tib %&gt;%\n  filter(carrier == \"AA\") %&gt;%\n  group_by(origin, dest) %&gt;%\n  summarize(ad = mean(arr_delay)) %&gt;%\n  arrange(origin, desc(dest))"
  },
  {
    "objectID": "A3_medium_data/A3_medium_data.html#joining",
    "href": "A3_medium_data/A3_medium_data.html#joining",
    "title": "Manipulating Large(ish) Datasets with data.table",
    "section": "Joining",
    "text": "Joining\n\nWe’ll use the following data.tables to introduce joining.\n\nxdf &lt;- data.table(mykey = c(\"1\", \"2\", \"3\"),\n                  x_val = c(\"x1\", \"x2\", \"x3\"))\nydf &lt;- data.table(mykey = c(\"1\", \"2\", \"4\"),\n                  y_val = c(\"y1\", \"y2\", \"y3\"))\nxdf\n\n    mykey  x_val\n   &lt;char&gt; &lt;char&gt;\n1:      1     x1\n2:      2     x2\n3:      3     x3\n\nydf\n\n    mykey  y_val\n   &lt;char&gt; &lt;char&gt;\n1:      1     y1\n2:      2     y2\n3:      4     y3\n\n\n \nUse the merge() function for all joining in data.table.\nInner Join:\n \n\n## data.table way\nmerge(xdf, ydf, by = \"mykey\")\n\n## equivalent tidyverse way\ninner_join(xdf, ydf, by = \"mykey\")\n\nOuter Joins\n \nLeft Join\n\n## data.table way\nmerge(xdf, ydf, by = \"mykey\", all.x = TRUE)\n\n## equivalent tidyverse way\nleft_join(xdf, ydf, by = \"mykey\")\n\nRight Join\n\n## data.table way\nmerge(xdf, ydf, by = \"mykey\", all.y = TRUE)\n\n## equivalent tidyverse way\nright_join(xdf, ydf, by = \"mykey\")\n\nOuter Join\n\n## data.table way\nmerge(xdf, ydf, by = \"mykey\", all.x = TRUE, all.y = TRUE)\n\n## equivalent tidyverse way\nfull_join(xdf, ydf, by = \"mykey\")\n\nBinding Rows:\n\n## data.table way\nrbind(xdf, ydf, fill = TRUE)\n\n## equivalent tidyverse way\nbind_rows(xdf, ydf)\n\nWhen you have different key names, use the by.x and by.y arguments instead of the by argument.\n\nnames(ydf)[1] &lt;- \"newkey\"\nydf\n\n   newkey  y_val\n   &lt;char&gt; &lt;char&gt;\n1:      1     y1\n2:      2     y2\n3:      4     y3\n\n\n\n## data.table way\nmerge(xdf, ydf, by.x = \"mykey\", by.y = \"newkey\")\n\n## equivalent tidyverse way\ninner_join(xdf, ydf, by = c(\"mykey\" = \"newkey\"))\n\nExercise: Recall the nycflights13 dataset\n\nlibrary(nycflights13)\ndata(\"flights\")\ndata(\"airlines\")\ndata(\"planes\")\nflights  &lt;- as.data.table(flights)\nairlines &lt;- as.data.table(airlines)\nplanes   &lt;- as.data.table(planes)\n\nAdd the full airline names to the flights data.table.\nExercise: Select all flights that use a plane where you have some annotation."
  },
  {
    "objectID": "A2_linear_regression/A2_simple_linear_regression.html",
    "href": "A2_linear_regression/A2_simple_linear_regression.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Simple Linear Regression\nChapter 8 of OpenIntro Statistics (Fourth Edition)\nIntroduction to Broom"
  },
  {
    "objectID": "A2_linear_regression/A2_simple_linear_regression.html#assumptions-and-violations",
    "href": "A2_linear_regression/A2_simple_linear_regression.html#assumptions-and-violations",
    "title": "Simple Linear Regression",
    "section": "Assumptions and Violations",
    "text": "Assumptions and Violations\n\nThe linear model has many assumptions.\nYou should always check these assumptions.\nAssumptions in decreasing order of importance\n\nLinearity - The relationship looks like a straight line.\nIndependence - The knowledge of the value of one observation does not give you any information on the value of another.\nEqual Variance - The spread is the same for every value of \\(x\\)\nNormality - The distribution of the errors isn’t too skewed and there aren’t any too extreme points. (Only an issue if you have outliers and a small number of observations because of the central limit theorem).\n\nProblems when violated\n\nLinearity violated - Linear regression line does not pick up actual relationship. Results aren’t meaningful.\nIndependence violated - Linear regression line is unbiased, but standard errors are off. Your \\(p\\)-values are too small.\nEqual Variance violated - Linear regression line is unbiased, but standard errors are off. Your \\(p\\)-values may be too small, or too large.\nNormality violated - Unstable results if outliers are present and sample size is small. Not usually a big deal.\n\nExercise: What assumptions are made about the distribution of the explanatory variable (the \\(x_i\\)’s)?"
  },
  {
    "objectID": "A2_linear_regression/A2_simple_linear_regression.html#evaluating-independence",
    "href": "A2_linear_regression/A2_simple_linear_regression.html#evaluating-independence",
    "title": "Simple Linear Regression",
    "section": "Evaluating Independence",
    "text": "Evaluating Independence\n\nThink about the problem.\n\nWere different responses measured on the same observational/experimental unit?\nWere data collected in groups?\n\nExample of non-independence: The temperature today and the temperature tomorrow. If it is warm today, it is probably warm tomorrow.\nExample of non-independence: You are collecting a survey. To obtain individuals, you select a house at random and then ask all participants in this house to answer the survey. The participants’ responses inside each house are probably not independent because they probably share similar beliefs/backgrounds/situations.\nExample of independence: You are collecting a survey. To obtain individuals, you randomly dial phone numbers until an individual picks up the phone."
  },
  {
    "objectID": "A2_linear_regression/A2_simple_linear_regression.html#evaluating-other-assumptions",
    "href": "A2_linear_regression/A2_simple_linear_regression.html#evaluating-other-assumptions",
    "title": "Simple Linear Regression",
    "section": "Evaluating other assumptions",
    "text": "Evaluating other assumptions\n\nEvaluate issues by plotting the residuals.\nThe residuals are the observed values minus the predicted values. \\[\nr_i = y_i - \\hat{y}_i\n\\]\nIn the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\).\nObtain the residuals by using augment() from broom. They will be the .resid variable.\n\naout &lt;- augment(lmout)\nglimpse(aout)\n\nRows: 32\nColumns: 9\n$ .rownames  &lt;chr&gt; \"Mazda RX4\", \"Mazda RX4 Wag\", \"Datsun 710\", \"Hornet 4 Drive…\n$ mpg        &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,…\n$ logwt      &lt;dbl&gt; 0.9632, 1.0561, 0.8416, 1.1678, 1.2355, 1.2413, 1.2726, 1.1…\n$ .fitted    &lt;dbl&gt; 22.80, 21.21, 24.88, 19.30, 18.15, 18.05, 17.51, 19.44, 19.…\n$ .resid     &lt;dbl&gt; -1.79984, -0.21293, -2.07761, 2.09684, 0.55260, 0.05164, -3…\n$ .hat       &lt;dbl&gt; 0.03930, 0.03263, 0.05637, 0.03193, 0.03539, 0.03582, 0.038…\n$ .sigma     &lt;dbl&gt; 2.694, 2.715, 2.686, 2.686, 2.713, 2.715, 2.646, 2.548, 2.6…\n$ .cooksd    &lt;dbl&gt; 9.677e-03, 1.109e-04, 1.917e-02, 1.051e-02, 8.149e-04, 7.21…\n$ .std.resid &lt;dbl&gt; -0.68788, -0.08110, -0.80119, 0.79833, 0.21077, 0.01970, -1…\n\n\nYou should always make the following scatterplots. The residuals always go on the \\(y\\)-axis.\n\nFits \\(\\hat{y}_i\\) vs residuals \\(r_i\\).\nResponse \\(y_i\\) vs residuals \\(r_i\\).\nExplanatory variable \\(x_i\\) vs residuals \\(r_i\\).\n\nIn the simple linear model, you can probably evaluate these issues by plotting the data (\\(x_i\\) vs \\(y_i\\)). But residual plots generalize to much more complicated models, whereas just plotting the data does not.\n\n\nExample 1: A perfect residual plot\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeans are straight lines\nResiduals seem to be centered at 0 for all \\(x\\)\nVariance looks equal for all \\(x\\)\nEverything looks perfect\n\n\n\nExample 2: Curved Monotone Relationship, Equal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rexp(100)\nx &lt;- x - min(x) + 0.5\ny &lt;- log(x) * 20 + rnorm(100, sd = 4)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved (but always increasing) relationship between \\(x\\) and \\(y\\).\nVariance looks equal for all \\(x\\)\nResidual plot has a parabolic shape.\nSolution: These indicate a \\(\\log\\) transformation of \\(x\\) could help.\n\ndf_fake %&gt;%\n  mutate(logx = log(x)) -&gt;\n  df_fake\nlm_fake &lt;- lm(y ~ logx, data = df_fake)\n\n\n\n\nExample 3: Curved Non-monotone Relationship, Equal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- -x^2 + rnorm(100)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved relationship between \\(x\\) and \\(y\\)\nSometimes the relationship is increasing, sometimes it is decreasing.\nVariance looks equal for all \\(x\\)\nResidual plot has a parabolic form.\nSolution: Include a squared term in the model (or hire a statistician).\n\nlmout &lt;- lm(y ~ x^2, data = df_fake)\n\n\n\n\nExample 4: Curved Relationship, Variance Increases with \\(Y\\)\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- exp(x + rnorm(100, sd = 1/2))\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurved relationship between \\(x\\) and \\(y\\)\nVariance looks like it increases as \\(y\\) increases\nResidual plot has a parabolic form.\nResidual plot variance looks larger to the right and smaller to the left.\nSolution: Take a log-transformation of \\(y\\).\n\ndf_fake %&gt;%\n  mutate(logy = log(y)) -&gt;\n  df_fake\nlm_fake &lt;- lm(logy ~ x, data = df_fake)\n\n\n\n\nExample 5: Linear Relationship, Equal Variances, Skewed Distribution\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStraight line relationship between \\(x\\) and \\(y\\).\nVariances about equal for all \\(x\\)\nSkew for all \\(x\\)\nResidual plots show skew.\nSolution: Do nothing, but report skew (usually OK to do)\n\n\n\nExample 6: Linear Relationship, Unequal Variances\n\nGenerate fake data:\n\nset.seed(1)\nx &lt;- runif(100) * 10\ny &lt;- 0.85 * x + rnorm(100, sd = (x - 5) ^ 2)\ndf_fake &lt;- tibble(x, y)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear relationship between \\(x\\) and \\(y\\).\nVariance is different for different values of \\(x\\).\nResidual plots really good at showing this.\nSolution: The modern solution is to use sandwich estimates of the standard errors (hire a statistician).\n\nlibrary(sandwich)\nlm_fake &lt;- lm(y ~ x, data = df_fake)\nsemat &lt;- sandwich(lm_fake)\ntidy(lm_fake) %&gt;%\n  mutate(sandwich_se = sqrt(diag(semat)),\n         sandwich_t  = estimate / sandwich_se,\n         sandwich_p  = 2 * pt(-abs(sandwich_t), df = df.residual(lm_fake)))\n\n# A tibble: 2 × 8\n  term    estimate std.error statistic p.value sandwich_se sandwich_t sandwich_p\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 (Inter…    -2.86     2.01      -1.43 1.57e-1       2.78       -1.03    0.307  \n2 x           1.37     0.345      3.97 1.37e-4       0.508       2.70    0.00827"
  },
  {
    "objectID": "A1_basic_stats/A1_basic_stats.html",
    "href": "A1_basic_stats/A1_basic_stats.html",
    "title": "Basic Statistics",
    "section": "",
    "text": "Learning Objectives\n\nReview intermediate Statistics (STAT 302/320/614).\nProbability distributions in R.\nP-values/confidence intervals.\n\\(t\\)-tests for means in R.\nProportion tests in R.\n\n\n\nA Note on this Review\n\nThis review assumes that you have had some basic statistics course and have seen all of these concepts in the past.\nIf you haven’t taken a basic statistics course, you will find this review very difficult and incomplete.\nIf you are having difficulty remembering these topics, then you can review them in the free OpenIntro book: https://leanpub.com/openintro-statistics. The most pertinent sections (from the third edition) are 3.1, 4.1, 4.2, 4.3, 5.1, 5.2, 5.3, 6.1, 6.2, and 6.5.\n\n\n\nProbability and Distributions in R.\n\nDistribution: The possible values of a variable and how often it takes those values.\nA density describes the distribution of a quantitative variable. You can think of it as approximating a histogram. It is a curve where\n\nThe area under the curve between any two points is approximately the probability of being between those two points.\nThe total area under the curve is 1 (something must happen).\nThe curve is never negative (can’t have negative probabilities).\n\nThe density of birthweights in America:\n \nThe distribution of many variables in Statistics approximate the normal distribution.\n\nIf you know the mean and standard deviation of a normal distribution, then you know the whole distribution.\nLarger standard deviation implies more spread out (larger and smaller values are both more likely).\nMean determines where the data are centered.\n\nNormal densities with different means.\n\n\n\n\n\n\n\n\n\nNormal densities with different standard deviations\n\n\n\n\n\n\n\n\n\nDensity Function (height of curve, NOT probability of a value).\n\ndnorm(x = 2, mean = 1, sd = 1)\n\n[1] 0.242\n\n\n\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nRandom Generation (generate samples from a given normal distribution).\n\nsamp &lt;- rnorm(n = 1000, mean = 1, sd = 1)\nhead(samp)\n\n[1] 0.3735 1.1836 0.1644 2.5953 1.3295 0.1795\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function (probability of being less than or equal to some value).\n\npnorm(q = 2, mean = 1, sd = 1)\n\n[1] 0.8413\n\n\n\n\n\n\n\n\n\n\n\nQuantile function (find value that has a given the probability of being less than or equal to it).\n\nqnorm(p = 0.8413, mean = 1, sd = 1)\n\n[1] 2\n\n\n\n\n\n\n\n\n\n\n\nExercise: Use rnorm() to generate 10,000 random draws from a normal distribution with mean 5 and standard deviation 2. What proportion are less than 3? Can you think up a way to approximate this proportion using a different function?\nExercise: In Hong Kong, human male height is approximately normally distributed with mean 171.5 cm and standard deviation 5.5 cm. What proportion of the Hong Kong population is between 170 cm and 180 cm?\nThe \\(t\\)-distribution shows up a lot in Statistics.\n\nIt is also bell-curved but has “thicker tails” (more extreme observations are more likely).\nIt is always centered at 0.\nIt only has one parameter, called the “degrees of freedom”, which determines how thick the tails are.\nSmaller degrees of freedom mean thicker tails, larger degrees of freedom means thinner tails.\nIf the degrees of freedom is large enough, the \\(t\\)-distribution is approximately the same as a normal distribution with mean 0 and variance 1.\n\n\\(t\\)-distributions with different degrees of freedom:\n\n\n\n\n\n\n\n\n\nDensity Function\n\ndt(x = -6, df = 2)\n\n[1] 0.004269\n\n\n\n\n\n\n\n\n\n\n\nRandom Generation\n\nsamp &lt;- rt(n = 1000, df = 2)\nhead(samp)\n\n[1]  0.89857 -1.07176  0.09639  0.79371 -0.42428 -0.64561\n\n\n\n\n\n\n\n\n\n\n\nCumulative Distribution Function\n\npt(q = 2, df = 2)\n\n[1] 0.9082\n\n\n\n\n\n\n\n\n\n\n\nQuantile Function\n\nqt(p = 0.9082, df = 2)\n\n[1] 1.999\n\n\n\n\n\n\n\n\n\n\n\nThere are many other distributions implemented in R. To see the most common, run:\n\nhelp(\"Distributions\")\n\nExercise: Calculate the 0.75 quantile of the \\(t\\)-distribution for degrees of freedom \\(1, 2, 3,\\ldots,50\\). Reproduce this plot:\n\n\n\n\n\n\n\n\n\nCan you use a function to come up with the asymptotic value of the 0.75 quantile as the degrees of freedom approaches infinity?\n\n\n\nAll of Statistics\n\nObservational/experimental Units: The people/places/things/animals/groups that we collect information about. Also known as “individuals” or “cases”. Sometimes I just say “units”.\nVariable: A property of the observational/experimental units.\n\nE.g.: height of a person, area of a country, marital status.\n\nValue: The specific level of a variable for an observational/experimental unit.\n\nE.g.: Bob is 5’11’’, China has an area of 3,705,407 square miles, Jane is divorced.\n\nQuantitative Variable: The variable takes on numerical values where arithmetic operations (plus/minus/divide/times) make sense.\n\nE.g.: height, weight, area, income.\nCounterexample: Phone numbers, social security numbers.\n\nCategorical Variable: The variable puts observational/experimental units into different groups/categories based on the values of that variable.\n\nE.g.: race/ethnicity, marital status, religion.\n\nBinary Variable: A categorical variable that takes on only two values.\n\nE.g.: dead/alive, treatment/control.\n\nPopulation: The collection of all observational units we are interested in.\nParameter: A numerical summary of the population.\n\nE.g.: Average height, proportion of people who are divorced, standard deviation of weight.\n\nSample: A subset of the population (some observational units, but not all of them).\nStatistic: A numeric summary of the sample.\n\nE.g.: Average height of the sample, proportion of people who are divorced in the sample, standard deviation of weight of a sample.\n\nGraphic:\n \nSampling Distribution: The distribution of a statistic over many hypothetical random samples from the population.\n \nAll of Statistics: We see a pattern in the sample.\n\nEstimation: Guess the pattern in the population based on the sample. Guess a parameter with a statistic. A statistic which is a guess for a parameter is called an estimate.\nHypothesis Testing: Ask if the pattern we see in the sample also exists in the population. Test if a parameter is some value.\nConfidence Intervals: Quantify our (un)certainty of the pattern in the population based on the sample. Provide a range of likely parameter values.\n\nWe will go through a lot of examples of this below\nFor the examples below, we will use the data from the Sleuth3 package in R.\n\nlibrary(Sleuth3)\nlibrary(tidyverse)\nlibrary(broom)\n\nExercise: Read the help page of the ex0126 data frame from the Sleuth3 package. What are the observational units? What are the variables? Which are quantitative and which are categorical?\nExercise: Read the help page of the ex0223 data frame from the Sleuth3 package. What are the observational units? What are the variables? Which are quantitative and which are categorical?\n\n\n\nPattern: Mean is shifted (one quantitative variable)\n\nExample: Researchers measured the volume of the left hippocampus in 15 twins where one twin had schizophrenia and the other did not. They were interested in whether the left hippocampus differed in size between the normal and schizophrenic twin.\nObservational Units: The twins.\nPopulation: All twins where one has schizophrenia and the other does not.\nSample: The 15 twins in our study.\nVariable: The difference in volume in the left hippocampus between the twins. We derived this quantitative variable by subtracting one volume from another.\n\ncase0202 %&gt;%\n  as_tibble() %&gt;%\n  mutate(diff = Unaffected - Affected) %&gt;%\n  select(diff) -&gt;\n  schizo\nglimpse(schizo)\n\nRows: 15\nColumns: 1\n$ diff &lt;dbl&gt; 0.67, -0.19, 0.09, 0.19, 0.13, 0.40, 0.04, 0.10, 0.50, 0.07, 0.23…\n\n\nPattern: Use a histogram/boxplot to visualize the shift from 0.\n\nggplot(schizo, aes(x = diff)) +\n  geom_histogram(bins = 15, fill = \"white\", color = \"black\") +\n  geom_vline(xintercept = 0, lty = 2) +\n  xlab(\"Difference in Brain Volumes\")\n\n\n\n\n\n\n\n\nGraphic:\n \nParameter of interest: Mean difference in left hippocampus volumes for all twins.\nEstimate: Use sample mean\n\nschizo %&gt;%\n  summarize(meandiff = mean(diff))\n\n# A tibble: 1 × 1\n  meandiff\n     &lt;dbl&gt;\n1    0.199\n\n\n0.199 is our “best guess” for the parameter, but it is almost certainly not the value of the parameter (since we didn’t measure everyone).\nHypothesis Testing:\n\nWe are interested in if the mean difference is different from 0.\nTwo possibilities:\n\nAlternative Hypothesis: Mean is different from 0.\nNull Hypothesis: Mean is not different from 0, we just happened by chance to get twins that had a big difference in volume.\n\nStrategy: We calculate the probability of the data assuming possibility 2 (called a \\(p\\)-value). If this probability is low, we conclude possibility 1. If the this probability is high, we don’t conclude anything.\np-value: the probability that you would see data as or more supportive of the alternative hypothesis than what you saw assuming that the null hypothesis is true.\n\nGraphic:\n \nThe distribution of possible null sample means is given by statistical theory. Specifically, the \\(t\\)-statistic (mean divided by the standard deviation of the sampling distribution of the mean) has a \\(t\\) distribution with \\(n - 1\\) degrees of freedom (\\(n\\) is the sample size). It works as long as your data aren’t too skewed or if you have a large enough sample size.\nFunction: t.test()\n\ntout &lt;- t.test(schizo$diff)\ntout\n\n\n    One Sample t-test\n\ndata:  schizo$diff\nt = 3.2, df = 14, p-value = 0.006\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.0667 0.3306\nsample estimates:\nmean of x \n   0.1987 \n\n\nThe tidy() function from the broom package will format the output of common procedures to a convenient data frame.\n\ntdf &lt;- tidy(tout)\ntdf$estimate\n\nmean of x \n   0.1987 \n\ntdf$p.value\n\n[1] 0.006062\n\n\nWe often want a range of “likely” values. These are called confidence intervals. t.test() will return these confidence intervals, giving lowest and highest likely values for the mean difference in volumes:\n\ntdf$conf.low\n\n[1] 0.0667\n\ntdf$conf.high\n\n[1] 0.3306\n\n\nInterpreting confidence intervals:\n\nCORRECT: We used a procedure that would capture the true parameter in 95% of repeated samples.\nCORRECT: Prior to sampling, the probability of capturing the true parameter is 0.95.\nWRONG: After sampling, the probability of capturing the true parameter is 0.95.\n\nBecause after sampling the parameter is either in the interval or it’s not. We just don’t know which.\n\nWRONG: 95% of twins have volume differences within the bounds of the 95% confidence interval.\n\nBecause confidence intervals are statements about parameters, not observational units or statistics.\n\n\nGraphic:\n \nIntuition: Statistical theory tells us that the sample mean will be within (approximately) 2 standard deviations of the population mean in 95% of repeated samples. This is two standard deviations of the sampling distribution of the sample mean, not two standard deviations of the sample. So we just add and subtract (approximately) two standard deviations of the sampling distribution from the sample mean.\nExercise: An investor sued his broker for lack of diversification. Below are the rates of return (in percent) for the investor’s portfolio over 39 months (data from Moore, McCabe, and Craig (2017)). The average of the S&P 500 stock index for the same period was 0.95%. Does the broker perform worse than average? Explore the data, set up hypotheses, and run the appropriate test.\n\nstock &lt;- tibble(return = c(-8.36,   1.63,  -2.27, -2.93,  -2.70, \n                           -2.93,  -9.14,  -2.64,  6.82,  -2.35, \n                           -3.58,   6.13,   7.00, -15.25, -8.66,\n                           -1.03,  -9.16,  -1.25, -1.22,  -10.27,\n                           -5.11,  -0.80,  -1.44,  1.28,  -0.65,\n                            4.34,   12.22, -7.21, -0.09,   7.34, \n                            5.04,  -7.24,  -2.14, -1.01,  -1.41, \n                            12.03, -2.53,   4.33,  1.35))\n\n\n\n\nPattern: Means of two groups are different (one quantitative, one binary)\n\nExample: Beaks depths were measured on Daphne Major finches in 1976 (before a harsh drought) and in 1978 (after a harsh drought). The researchers hypothesized that finches with deeper peaks were more likely to survive.\n\ndata(\"case0201\")\ncase0201 %&gt;%\n  mutate(Year = as.factor(Year)) -&gt;\n  finch\nglimpse(finch)\n\nRows: 178\nColumns: 2\n$ Year  &lt;fct&gt; 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976, 1976…\n$ Depth &lt;dbl&gt; 6.2, 6.8, 7.1, 7.1, 7.4, 7.8, 7.9, 8.0, 8.2, 8.4, 8.4, 8.4, 8.5,…\n\n\nObservational Units: The finches.\nPopulation: All finches.\nSample: The 178 finches that the researches measured.\nVariables: The year the finch was measured (binary/categorical) and the beak depth (quantitative). Possible to also treat year as quantitative.\nPattern: Use a boxplot to see if the groups differ.\n\nggplot(finch, aes(x = Year, y = Depth)) +\n  geom_boxplot(coef = Inf) +\n  geom_jitter(alpha = 1/3)\n\n\n\n\n\n\n\n\nParameter of interest: Difference in mean beak depths between 1976 finches and 1978 finches.\nEstimate: The difference in mean beak depths between 1976 finches and 1978 finches in our sample.\n\nfinch %&gt;%\n  group_by(Year) %&gt;%\n  summarize(meandepth = mean(Depth)) %&gt;%\n  spread(key = Year, value = meandepth) %&gt;%\n  mutate(diff = `1978` - `1976`) ## do you remember why we need back-ticks?\n\n# A tibble: 1 × 3\n  `1976` `1978`  diff\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1   9.47   10.1 0.669\n\n\nHypothesis Test:\n\nWe want to know if the difference in the mean depths in the two years is actually different.\nTwo possibilities:\n\nAlternative Hypothesis: The mean depths are different in the two years.\nNull Hypothesis: The mean depths are the same in the two years. We just happened by chance to get deep 1978 finches and shallow 1976 finches.\nStrategy: We calculate the probability of the data assuming possibility 2 (called a p-value). If this probability is low, we conclude possibility 1. If the this probability is high, we don’t conclude anything.\n\n\nGraphic:\n \nThe distribution of possible null sample means comes from statistical theory. The t-statistic has a \\(t\\) distribution with a complicated degrees of freedom.\nFunction: t.test(). The quantitative variable goes to the left of the tilde and the binary variable goes to the right of the tilde.\n\ntout &lt;- t.test(Depth ~ Year, data = finch)\ntdf &lt;- tidy(tout)\ntdf$estimate\n\n[1] -0.6685\n\ntdf$p.value\n\n[1] 8.739e-06\n\n\nt.test() also returns a 95% confidence interval for the difference in means. This has the exact same interpretation as in the previous section.\n\nc(tdf$conf.low, tdf$conf.high)\n\n[1] -0.9564 -0.3806\n\n\nAssumptions (in decreasing order of importance):\n\nIndependence: conditional on year, beak length of one finch doesn’t give us any information on the beak lengths of any other finch.\nApproximate normality: The distribution of beak lengths is bell-curved in each year. Doesn’t matter for moderate-large sample sizes because of the central limit theorem.\n\nExercise: The Armed Forces Qualifying Test is designed for evaluating suitability of military recruits. There are different subjects tested: arithmetic, word knowledge, paragraph comprehension, and mathematics. They also provide a composite score. These data are stored in the ex0222 data frame from the Sleuth3 package. For which subjects do we have evidence that the genders differ? Make appropriate exploratory plots, set up the appropriate hypotheses, and run appropriate hypothesis tests.\n\n\n\nNote on Assumptions of \\(t\\)-tools\n\nAssumptions of \\(t\\)-tools in decreasing order of importance:\n\nIndependence: Knowing value of one observation does not tell you value of another.\nNormality: Distribution of quantitative variable (within each group) is normal.\n\nHire a statistician if the independence assumption is violated.\nIf data are very skewed (violating assumption 2) and the sample size is small and the quantitative variable is always positive, then take a log to make the data more symmetric.\nExample: Seeded clouds with either silver iodide or a control substance. Measured rainfall.\n\ndata(\"case0301\")\nggplot(case0301, aes(x = Treatment, y = Rainfall)) +\n  geom_boxplot()\n\n\n\n\n\n\n\nggplot(case0301, aes(x = Treatment, y = Rainfall)) +\n  geom_boxplot() +\n  scale_y_log10()\n\n\n\n\n\n\n\ncase0301 %&gt;%\n  mutate(logRainfall = log(Rainfall)) %&gt;%\n  t.test(logRainfall ~ Treatment, data = .) %&gt;%\n  tidy() -&gt;\n  tout\n\nInference is then stated in terms of medians and ratios of medians since the log of the median is the median of the log (which is not true of means). Equivalently, the exponentiation of the median is equal to the median of the exponentiation. Also, when data are symmetric, median = mean.\n\n\\(\\mu_1\\) = mean of log-rainfall in seeded group = median of log-rainfall in seeded group.\n\\(\\mu_2\\) = mean of log-rainfall in unseeded group = median of log-rainfall in unseeded group.\n\n\\(\\mu_1 - \\mu_2\\) = difference in median log-rainfalls in two groups.\n\\(e^{\\mu_1 - \\mu_2} = e^{\\mu_1}/e^{\\mu_2}\\) = median rainfall in seeded group divided by median rainfall in unseeded group.\nWe estimate and get confidence intervals for \\(\\mu_1 - \\mu_2\\), which implies we can estimate and get confidence intervals of \\(e^{\\mu_1}/e^{\\mu_2}\\) by just exponentiating the output of t.test().\n\n\\(p\\)-value:\n\ntout$p.value\n\n[1] 0.01408\n\n\nEstimate ratio of median rainfall in seeded group to median rainfall in unseeded group:\n\nexp(tout$estimate)\n\n[1] 3.139\n\n\nConfidence interval for ratio of median rainfall in seeded group to median rainfall in unseeded group:\n\nexp(c(low = tout$conf.low, high = tout$conf.high))\n\n  low  high \n1.272 7.742 \n\n\nExercise: Read up on the ex0330 dataset from the Sleuth3 R package. Determine if education level is associated with income. Interpret any estimates and confidence intervals you derive.\n\n\n\nPattern: Proportion is shifted (one binary variable).\n\nExample: A 2013 Gallup poll (https://news.gallup.com/poll/161198/favor-russian-nuclear-arms-reductions.aspx) surveyed 1,028 U.S. adults. Of the respondents, 576 said they they support nuclear arms reduction\nObservational Units: U.S. adults\nPopulation: All U.S. adults.\nSample: The 1028 U.S. adults in the survey.\nVariable: Did they support nuclear arms reduction (yes or no). This is a binary variable.\nPattern: Calculate sample proportion.\n\n576 / 1028\n\n[1] 0.5603\n\n\nParameter of interest: Proportion of U.S. adults who support nuclear arms reduction.\nEstimate with sample proportion, 0.56.\nHypothesis Testing:\n\nWe are interested in if more than half of U.S. adults support nuclear arms reduction.\nTwo possibilities:\n\nAlternative Hypothesis: Proportion of U.S. adults who support nuclear arms reduction is different from 0.5.\nNull Hypothesis: Proportion of U.S. adults who support nuclear arms reduction is 0.5. We just happened by chance to get a sample proportion far from 0.5\n\nStrategy: We calculate the probability of the data assuming possibility 2 (called a p-value). If this probability is low, we conclude possibility 1. If this probability is high, we don not conclude anything.\n\nGraphic:\n \nThe distribution of possible null sample proportions comes from statistical theory. The number of successes has a binomial distribution with success probability 0.5 and size parameter equal to the sample size. The sample proportion is the number successes divided by the sample size.\nFunction: prop.test() (when you have a large number of both successes and failures) or binom.test() (for any number of successes and failures).\n\nbout &lt;- tidy(binom.test(x = 576, n = 1028, p = 0.5))\nbout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate  p.value conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.560 0.000123    0.529     0.591\n\n\n\npout &lt;- tidy(prop.test(x = 576, n = 1028, p = 0.5))\npout %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n# A tibble: 1 × 4\n  estimate  p.value conf.low conf.high\n     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    0.560 0.000125    0.529     0.591\n\n\nExercise (from OpenIntro): The 2010 General Social Survey asked 1,259 US residents: “Do you think the use of marijuana should be made legal, or not?” 48% of the respondents said it should be made legal.\n\nIs 48% a sample statistic or a population parameter? Explain.\nConstruct a 95% confidence interval for the proportion of US residents who think marijuana should be made legal, and interpret it in the context of the data.\n\n\n\n\nPattern: Two categorical variables are associated.\n\nExample: 86 lung cancer patients and 86 controls were interviewed on their smoking history in a 1954 study. The data can be found at: https://data-science-master.github.io/lectures/data/smoke.csv.\n\nsmoke &lt;- read_csv(file = \"../data/smoke.csv\")\nglimpse(smoke)\n\nRows: 172\nColumns: 3\n$ individual &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ smoke      &lt;chr&gt; \"smoker\", \"smoker\", \"smoker\", \"smoker\", \"smoker\", \"smoker\",…\n$ cancer     &lt;chr&gt; \"cancer\", \"cancer\", \"control\", \"cancer\", \"cancer\", \"cancer\"…\n\n\nUsually, for two categorical variables, data are presented/provided in two-way tables:\n\ntabdat &lt;- table(smoke$smoke, smoke$cancer)\ntabdat\n\n\n            cancer control\n  nonsmoker      3      14\n  smoker        83      72\n\naddmargins(tabdat)\n\n\n            cancer control Sum\n  nonsmoker      3      14  17\n  smoker        83      72 155\n  Sum           86      86 172\n\n\nOr two-way tables of proportions.\n\nprop.table(tabdat)\n\n\n             cancer control\n  nonsmoker 0.01744 0.08140\n  smoker    0.48256 0.41860\n\n\nObservational Units: People.\nPopulation: All people.\nSample: The 172 people in the study.\nVariables: The smoking status of the person (smoke) and the cancer status of the person (cancer).\nPattern: Visualize by calculating conditional proportions. For example, the following code calculates the proportion of cancer cases that are smokers, and the proportion of controls that are smokers.\n\nprop.table(tabdat, margin = 2)\n\n\n             cancer control\n  nonsmoker 0.03488 0.16279\n  smoker    0.96512 0.83721\n\n\nIf you need a fancy plot for your boss, try a mosaic plot:\n\nas.data.frame(tabdat) %&gt;%\n  ggplot(aes(y = Freq, x = Var2, fill = Var1)) +\n  geom_col(position = \"fill\") +\n  scale_fill_colorblind(name = \"Smoking\\nStatus\") +\n  xlab(\"Cancer Status\") +\n  ylab(\"Proportion\")\n\n\n\n\n\n\n\n\nIntuition: If the two variables are not related, then we would expect about the same proportion of smokers in each group. But if these conditional proportions are very different, then to variables might be related.\nNull hypothesis: cancer status and smoking status are independent.\nAlternative hypothesis: cancer status and smoking status are dependent.\nFunction:fisher.test() tests for whether we see more smokers in one group versus the other. Or, symmetrically, whether we see more cancer cases in the smoker group versus the nonsmoker group. We won’t get into the test statistic or the null distribution of this test.\n\ntidy(fisher.test(tabdat)) %&gt;%\n  select(p.value, estimate)\n\n# A tibble: 1 × 2\n  p.value estimate\n    &lt;dbl&gt;    &lt;dbl&gt;\n1 0.00882    0.188\n\n\nThe value in the “estimate” column is the odds ratio. It’s the multiplicative change in the odds of being a nonsmoker when you consider the cancer versus control groups. Specifically \\[\n\\frac{3/86}{83/86} \\div \\frac{14/86}{72/86}\n\\]\nYou might see a chi-squared test in the wild. They are only appropriate for large numbers of counts in each cell. I don’t see a reason to use it since we have computers, but it’s fine most of the time to use:\n\ntidy(chisq.test(tabdat)) %&gt;%\n  select(p.value)\n\n# A tibble: 1 × 1\n  p.value\n    &lt;dbl&gt;\n1  0.0106\n\n\nExercise: Researches collected information on women 60 years old or older. They asked them about their sexual activity and their coffee drinking status. Data are from: Diokno, Ananias C., Morton B. Brown, and A. Regula Herzog. “Sexual function in the elderly.” Archives of Internal Medicine 150, no. 1 (1990): 197-200. The table is below:\n\ntabdat &lt;- as.table(matrix(c(15, 115, 25, 70), nrow = 2))\ndimnames(tabdat) &lt;- list(coffee = c(\"yes\", \"no\"), sex = c(\"yes\", \"no\"))\ntabdat\n\n      sex\ncoffee yes  no\n   yes  15  25\n   no  115  70\n\n\nDoes there appear to be an association between coffee drinking and sexual activity? Create a table to explore this association. Then run a hypothesis test. What is the null hypothesis?\n\n\n\n\n\n\n\n\n\nReferences\n\nMoore, David S, George P McCabe, and Bruce A Craig. 2017. “Introduction to the Practice of Statistics, Ninth Edition.” W.H. Freeman & Company."
  },
  {
    "objectID": "11_websites/11_websites_blogdown.html",
    "href": "11_websites/11_websites_blogdown.html",
    "title": "Create your own Website with Blogdown",
    "section": "",
    "text": "Learn how to create a website with R Markdown.\nCreate a personal or project website.\nChapters 1 and 2, and section 3.1 of blogdown: Creating Websites with R Markdown"
  },
  {
    "objectID": "11_websites/11_websites_blogdown.html#choosing-a-theme",
    "href": "11_websites/11_websites_blogdown.html#choosing-a-theme",
    "title": "Create your own Website with Blogdown",
    "section": "Choosing a Theme",
    "text": "Choosing a Theme\n\nThere are tons of themes available on the Hugo website: https://themes.gohugo.io/\nThe steps to installing a theme are:\n\nFind the GitHub username/reponame for for a theme you are interested in. On the hugo themes website you can usually find this by clicking on “homepage”.\nWhile creating your site, instead of just running blogdown::new_site(), run blogdown::new_site(theme = \"username/reponame\").\n\nFor example:\n\nSuppose I want to use the Fuji theme: https://themes.gohugo.io/hugo-theme-fuji/\nI need to go to its homepage: https://github.com/amzrk2/hugo-theme-fuji/\nThen, when I create my website, I run\n\nblogdown::new_site(theme = \"amzrk2/hugo-theme-fuji\")\n\n\nNote that the more complicated the theme, the more difficult it is to maintain.\nThe blogdown folks recommend first trying out these simple themes:\n\nXmin\nTanka\nCupper\nsimple-a\nghostwriter"
  },
  {
    "objectID": "11_websites/11_websites_blogdown.html#config.toml",
    "href": "11_websites/11_websites_blogdown.html#config.toml",
    "title": "Create your own Website with Blogdown",
    "section": "config.toml",
    "text": "config.toml\n\nMost Hugo sites contain a config.toml file, where “toml” stands for “Tom’s Obvious, Minimal Language”\nTOML files consist of key = value pairs, where the key is the variable and the value is the assigned variable. For example\nauthor = \"David Gerard\"\nYou can usually just edit the TOML file of the theme that you have downloaded.\nThe double brackets in a TOML file are used in Hugo to denote a menu. So, for example\n[[menu.main]]\n    name = \"Home\"\n    url = \"https://nanx.me\"\n    weight = 1\n[[menu.main]]\n    name = \"GitHub\"\n    url = \"https://github.com/nanxstats\"\n    weight = 2\nis used to define the items in the default main menu of the Tanka theme.\nThe single brackets in a TOML file are used in Hugo to denote a collection of related options. E.g.\n[params]\n    author = \"David Gerard\"\n    dateFormat = \"2020/10/10\"\nwill set options under [params]"
  },
  {
    "objectID": "11_websites/11_websites_blogdown.html#content",
    "href": "11_websites/11_websites_blogdown.html#content",
    "title": "Create your own Website with Blogdown",
    "section": "content/",
    "text": "content/\n\nThis folder is where you place R Markdown files. These R Markdown files will be converted to HTML files.\nTo add a new page to “content”, click on\n\nAddins &gt; New Post\n\nYou can then choose the name of the file, whether you are working with a Markdown file (no R, just formatted text) or an R Markdown file, and make some additonal formatting options for the page.\nYou can change the page name to something different from the default.\nFor example, if you choose the name “resume”, then the default is that the location of the HTML file in the “public” folder will be “resume/index.html”.\n\nThis is instead of what you would assume to be the case, e.g. “resume.html” (NOT DONE).\n\n \nThe big thing for you is when you are linking to a particular page, you need to link to the location “resume/index.html”, not to “resume.html”. E.g."
  },
  {
    "objectID": "13_ethics/13_ethics.html",
    "href": "13_ethics/13_ethics.html",
    "title": "Bias in Machine Learning",
    "section": "",
    "text": "Why do biases arise in predictive algorithms?\nWhy is this bad for society?\nHow do we fix this? (hint: no easy answers)\nhttps://fivethirtyeight.com/features/technology-is-biased-too-how-do-we-fix-it/"
  },
  {
    "objectID": "13_ethics/13_ethics.html#learning-objectives",
    "href": "13_ethics/13_ethics.html#learning-objectives",
    "title": "Bias in Machine Learning",
    "section": "",
    "text": "Why do biases arise in predictive algorithms?\nWhy is this bad for society?\nHow do we fix this? (hint: no easy answers)\nhttps://fivethirtyeight.com/features/technology-is-biased-too-how-do-we-fix-it/"
  },
  {
    "objectID": "07_sql/07_sql.html",
    "href": "07_sql/07_sql.html",
    "title": "SQL",
    "section": "",
    "text": "Learn some SQL\nInterface SQL with R through the {DBI} and {duckdb} packages.\n\nIntroduction to DBI: https://solutions.posit.co/connections/db/r-packages/dbi/\nDuckDB SQL Introduction: https://duckdb.org/docs/sql/introduction\nDuckDB R API: https://duckdb.org/docs/api/r.html\n\nWrite SQL code using the tidyverse and the {dbplyr} package.\n\n{dbplyr} and SQL: https://dbplyr.tidyverse.org/articles/sql.html\nR for Data Science Database Chapter: https://r4ds.hadley.nz/databases.html"
  },
  {
    "objectID": "07_sql/07_sql.html#general",
    "href": "07_sql/07_sql.html#general",
    "title": "SQL",
    "section": "General",
    "text": "General\n\nCase does not matter (i.e. select is the same as SELECT is the same as SeLeCt), but it is standard to have all statements be in UPPERCASE (i.e. SELECT).\nThe statements below must be in the following order: SELECT, FROM, WHERE, GROUP BY, ORDER BY.\nNew lines and white space don’t matter. But it is common to put those five commands above on new lines.\nCharacter values must be in single quotes.\nYou can use invalid variable names by putting them in double quotes (same as using backticks in R).\n\nSome folks always use double quotes because it is not always clear what is an invalid variable name in the database management system. This is what I do below.\n\nComments in SQL have two hyphens --.\nMake sure to put a semicolon ; at the end of a SQL statement. This will allow you to have multiple SQL queries in one chunk."
  },
  {
    "objectID": "07_sql/07_sql.html#showing-tables",
    "href": "07_sql/07_sql.html#showing-tables",
    "title": "SQL",
    "section": "Showing Tables",
    "text": "Showing Tables\n\nThe SHOW TABLES command can be used to get a list of all of the tables\nSQL\n\nSHOW TABLES;\n\n\n5 records\n\n\nname\n\n\n\n\nairlines\n\n\nairports\n\n\nflights\n\n\nplanes\n\n\nweather\n\n\n\n\n\nThe DESCRIBE command can be used to show tables and the variables.\nSQL\n\nDESCRIBE;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\ndatabase\nschema\nname\ncolumn_names\ncolumn_types\ntemporary\n\n\n\n\nflights\nmain\nairlines\ncarrier, name\nVARCHAR, VARCHAR\nFALSE\n\n\nflights\nmain\nairports\nfaa , name , lat , lon , alt , tz , dst , tzone\nVARCHAR, VARCHAR, DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR, VARCHAR\nFALSE\n\n\nflights\nmain\nflights\nyear , month , day , dep_time , sched_dep_time, dep_delay , arr_time , sched_arr_time, arr_delay , carrier , flight , tailnum , origin , dest , air_time , distance , hour , minute , time_hour\nINTEGER , INTEGER , INTEGER , INTEGER , INTEGER , DOUBLE , INTEGER , INTEGER , DOUBLE , VARCHAR , INTEGER , VARCHAR , VARCHAR , VARCHAR , DOUBLE , DOUBLE , DOUBLE , DOUBLE , TIMESTAMP\nFALSE\n\n\nflights\nmain\nplanes\ntailnum , year , type , manufacturer, model , engines , seats , speed , engine\nVARCHAR, INTEGER, VARCHAR, VARCHAR, VARCHAR, INTEGER, INTEGER, INTEGER, VARCHAR\nFALSE\n\n\nflights\nmain\nweather\norigin , year , month , day , hour , temp , dewp , humid , wind_dir , wind_speed, wind_gust , precip , pressure , visib , time_hour\nVARCHAR , INTEGER , INTEGER , INTEGER , INTEGER , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , TIMESTAMP\nFALSE"
  },
  {
    "objectID": "07_sql/07_sql.html#select-specific-columns",
    "href": "07_sql/07_sql.html#select-specific-columns",
    "title": "SQL",
    "section": "Select Specific Columns",
    "text": "Select Specific Columns\n\nTo get the columns from a data frame, use the SELECT command.\nThe syntax is like this\nSQL\nSELECT &lt;column1&gt;, &lt;column2&gt;, &lt;column3&gt; \nFROM &lt;mytable&gt;;\nLet’s use this to get the tailnum, year, and model variables from the planes table.\nSQL\n\nSELECT \"tailnum\", \"year\", \"model\" \nFROM planes;\n\n\nDisplaying records 1 - 10\n\n\ntailnum\nyear\nmodel\n\n\n\n\nN10156\n2004\nEMB-145XR\n\n\nN102UW\n1998\nA320-214\n\n\nN103US\n1999\nA320-214\n\n\nN104UW\n1999\nA320-214\n\n\nN10575\n2002\nEMB-145LR\n\n\nN105UW\n1999\nA320-214\n\n\nN107US\n1999\nA320-214\n\n\nN108UW\n1999\nA320-214\n\n\nN109UW\n1999\nA320-214\n\n\nN110UW\n1999\nA320-214\n\n\n\n\n\nR equivalent\nR\n\nplanes |&gt;\n  select(tailnum, year, model)\n\nYou select every column by using *:\nSQL\n\nSELECT * \nFROM planes;\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\ntailnum\nyear\ntype\nmanufacturer\nmodel\nengines\nseats\nspeed\nengine\n\n\n\n\nN10156\n2004\nFixed wing multi engine\nEMBRAER\nEMB-145XR\n2\n55\nNA\nTurbo-fan\n\n\nN102UW\n1998\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN103US\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN104UW\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN10575\n2002\nFixed wing multi engine\nEMBRAER\nEMB-145LR\n2\n55\nNA\nTurbo-fan\n\n\nN105UW\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN107US\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN108UW\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN109UW\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\nN110UW\n1999\nFixed wing multi engine\nAIRBUS INDUSTRIE\nA320-214\n2\n182\nNA\nTurbo-fan\n\n\n\n\n\nR equivalent\nR\n\nplanes\n\nThere is no equivalent for excluding columns (like dplyr::select(-year)). You just select the ones you want."
  },
  {
    "objectID": "07_sql/07_sql.html#filter-rows",
    "href": "07_sql/07_sql.html#filter-rows",
    "title": "SQL",
    "section": "Filter rows",
    "text": "Filter rows\n\nYou use the WHERE command in SQL to filter by rows.\nSQL\n\nSELECT \"flight\", \"distance\", \"origin\", \"dest\" \nFROM flights\nWHERE \"distance\" &lt; 50;\n\n\n1 records\n\n\nflight\ndistance\norigin\ndest\n\n\n\n\n1632\n17\nEWR\nLGA\n\n\n\n\n\nR equivalent:\nR\n\nflights |&gt;\n  select(flight, distance, origin, dest) |&gt;\n  filter(distance &lt; 50)\n\nTo test for equality, you just use one equals sign.\nSQL\n\nSELECT \"flight\", \"month\" \nFROM flights\nWHERE \"month\" = 12;\n\n\nDisplaying records 1 - 10\n\n\nflight\nmonth\n\n\n\n\n745\n12\n\n\n839\n12\n\n\n1895\n12\n\n\n1487\n12\n\n\n2243\n12\n\n\n939\n12\n\n\n3819\n12\n\n\n1441\n12\n\n\n2167\n12\n\n\n605\n12\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(flight, month) |&gt;\n  filter(month == 12)\n\nFor characters you must use single quotes, not double.\nSQL\n\nSELECT \"flight\", \"origin\" \nFROM flights\nWHERE \"origin\" = 'JFK';\n\n\nDisplaying records 1 - 10\n\n\nflight\norigin\n\n\n\n\n1141\nJFK\n\n\n725\nJFK\n\n\n79\nJFK\n\n\n49\nJFK\n\n\n71\nJFK\n\n\n194\nJFK\n\n\n1806\nJFK\n\n\n1743\nJFK\n\n\n303\nJFK\n\n\n135\nJFK\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(flight, origin) |&gt;\n  filter(origin == \"JFK\")\n\nYou can select multiple criteria using the AND command\nSQL\n\nSELECT \"flight\", \"origin\", \"dest\" \nFROM flights\nWHERE \"origin\" = 'JFK' AND \"dest\" = 'CMH';\n\n\nDisplaying records 1 - 10\n\n\nflight\norigin\ndest\n\n\n\n\n4146\nJFK\nCMH\n\n\n3783\nJFK\nCMH\n\n\n4146\nJFK\nCMH\n\n\n3783\nJFK\nCMH\n\n\n4146\nJFK\nCMH\n\n\n3783\nJFK\nCMH\n\n\n4146\nJFK\nCMH\n\n\n3783\nJFK\nCMH\n\n\n4146\nJFK\nCMH\n\n\n3650\nJFK\nCMH\n\n\n\n\n\nR equivalent:\nR\n\nflights |&gt;\n  select(flight, origin, dest) |&gt;\n  filter(origin == \"JFK\", dest == \"CMH\")\n\nYou can use the OR logical operator too. Just put parentheses around your desired order of operations.\nSQL\n\nSELECT \"flight\", \"origin\", \"dest\" \nFROM flights\nWHERE (\"origin\" = 'JFK' OR \"origin\" = 'LGA') AND dest = 'CMH';\n\n\nDisplaying records 1 - 10\n\n\nflight\norigin\ndest\n\n\n\n\n4490\nLGA\nCMH\n\n\n4485\nLGA\nCMH\n\n\n4426\nLGA\nCMH\n\n\n4429\nLGA\nCMH\n\n\n4146\nJFK\nCMH\n\n\n4626\nLGA\nCMH\n\n\n4555\nLGA\nCMH\n\n\n3783\nJFK\nCMH\n\n\n4490\nLGA\nCMH\n\n\n4485\nLGA\nCMH\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(flight, origin, dest) |&gt;\n  filter(origin == \"JFK\" | origin == \"LGA\", dest == \"CMH\")\n\nMissing data is NULL in SQL (instead of NA). We can remove them by the special command:\nSQL\n\nSELECT \"flight\", \"dep_delay\" \nFROM flights\nWHERE \"dep_delay\" IS NOT NULL;\n\n\nDisplaying records 1 - 10\n\n\nflight\ndep_delay\n\n\n\n\n1545\n2\n\n\n1714\n4\n\n\n1141\n2\n\n\n725\n-1\n\n\n461\n-6\n\n\n1696\n-4\n\n\n507\n-5\n\n\n5708\n-3\n\n\n79\n-3\n\n\n301\n-2\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(flight, dep_delay) |&gt;\n  filter(!is.na(dep_delay))\n\nJust use IS if you want only the missing data observations\nSQL\n\nSELECT \"flight\", \"dep_delay\" \nFROM flights\nWHERE \"dep_delay\" IS NULL;\n\n\nDisplaying records 1 - 10\n\n\nflight\ndep_delay\n\n\n\n\n4308\nNA\n\n\n791\nNA\n\n\n1925\nNA\n\n\n125\nNA\n\n\n4352\nNA\n\n\n4406\nNA\n\n\n4434\nNA\n\n\n4935\nNA\n\n\n3849\nNA\n\n\n133\nNA\n\n\n\n\n\nWhen you are building a query, you often want to subset the rows while you are finishing it (you don’t want to return the whole table each time you are trouble shooting a query). Use LIMIT to show only the top subset.\nSQL\n\nSELECT \"flight\", \"origin\", \"dest\" \nFROM flights\nLIMIT 5;\n\n\n5 records\n\n\nflight\norigin\ndest\n\n\n\n\n1545\nEWR\nIAH\n\n\n1714\nLGA\nIAH\n\n\n1141\nJFK\nMIA\n\n\n725\nJFK\nBQN\n\n\n461\nLGA\nATL\n\n\n\n\n\nYou can also randomly sample rows via USING SAMPLE:\nSQL\n\nSELECT \"flight\", \"origin\", \"dest\" \nFROM flights\nUSING SAMPLE 5 ROWS;\n\n\n5 records\n\n\nflight\norigin\ndest\n\n\n\n\n2181\nLGA\nDCA\n\n\n1519\nEWR\nSTT\n\n\n75\nEWR\nPHX\n\n\n1723\nEWR\nCLT\n\n\n272\nJFK\nSFO\n\n\n\n\n\nExercise: Select all flights that either (a) departed on time or early from JFK, or (b) departed late from LGA. Only get the departure delay, tail number, and the time."
  },
  {
    "objectID": "07_sql/07_sql.html#arranging-rows",
    "href": "07_sql/07_sql.html#arranging-rows",
    "title": "SQL",
    "section": "Arranging Rows",
    "text": "Arranging Rows\n\nUse ORDER BY to rearrange the rows (let’s remove missing values so we can see the ordering)\nSQL\n\nSELECT \"flight\", \"dep_delay\" \nFROM flights\nWHERE \"dep_delay\" IS NOT NULL\nORDER BY \"dep_delay\";\n\n\nDisplaying records 1 - 10\n\n\nflight\ndep_delay\n\n\n\n\n97\n-43\n\n\n1715\n-33\n\n\n5713\n-32\n\n\n1435\n-30\n\n\n837\n-27\n\n\n3478\n-26\n\n\n4573\n-25\n\n\n4361\n-25\n\n\n2223\n-24\n\n\n375\n-24\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(flight, dep_delay) |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  arrange(dep_delay)\n\nUse DESC after the variable to arrange in descending order\nSQL\n\nSELECT \"flight\", \"dep_delay\" \nFROM flights\nWHERE \"dep_delay\" IS NOT NULL\nORDER BY \"dep_delay\" DESC;\n\n\nDisplaying records 1 - 10\n\n\nflight\ndep_delay\n\n\n\n\n51\n1301\n\n\n3535\n1137\n\n\n3695\n1126\n\n\n177\n1014\n\n\n3075\n1005\n\n\n2391\n960\n\n\n2119\n911\n\n\n2007\n899\n\n\n2047\n898\n\n\n172\n896\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(flight, dep_delay) |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  arrange(desc(dep_delay))\n\nYou break ties by adding more variables in the ORDER BY statement\nSQL\n\nSELECT \"flight\", \"origin\", \"dep_delay\" \nFROM flights\nWHERE \"dep_delay\" IS NOT NULL\nORDER BY \"origin\" DESC, \"dep_delay\";\n\n\nDisplaying records 1 - 10\n\n\nflight\norigin\ndep_delay\n\n\n\n\n1715\nLGA\n-33\n\n\n5713\nLGA\n-32\n\n\n1435\nLGA\n-30\n\n\n837\nLGA\n-27\n\n\n3478\nLGA\n-26\n\n\n4573\nLGA\n-25\n\n\n2223\nLGA\n-24\n\n\n375\nLGA\n-24\n\n\n4065\nLGA\n-24\n\n\n1371\nLGA\n-23\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(flight, origin, dep_delay) |&gt;\n  filter(!is.na(dep_delay)) |&gt;\n  arrange(desc(origin), dep_delay)"
  },
  {
    "objectID": "07_sql/07_sql.html#mutate",
    "href": "07_sql/07_sql.html#mutate",
    "title": "SQL",
    "section": "Mutate",
    "text": "Mutate\n\nIn SQL, you mutate variables while you SELECT. You use AS to specify what the new variable is called (choosing a variable name is called “aliasing” in SQL).\nSQL\nSELECT &lt;expression&gt; AS &lt;myvariable&gt; \nFROM &lt;mytable&gt;;\nLet’s calculate average speed from the flights table. We’ll also keep the flight number, distance, and air time variables.\nSQL\n\nSELECT \"flight\", \"distance\" / \"air_time\" AS \"speed\", \"distance\", \"air_time\" \nFROM flights;\n\n\nDisplaying records 1 - 10\n\n\nflight\nspeed\ndistance\nair_time\n\n\n\n\n1545\n6.167\n1400\n227\n\n\n1714\n6.238\n1416\n227\n\n\n1141\n6.806\n1089\n160\n\n\n725\n8.612\n1576\n183\n\n\n461\n6.569\n762\n116\n\n\n1696\n4.793\n719\n150\n\n\n507\n6.740\n1065\n158\n\n\n5708\n4.321\n229\n53\n\n\n79\n6.743\n944\n140\n\n\n301\n5.312\n733\n138\n\n\n\n\n\nR equivalent:\nR\n\nflights |&gt;\n  select(flight, distance, air_time) |&gt;\n  mutate(speed = distance / air_time)\n\nVarious transformation functions also exist:\n\nLN(): Natural log transformation.\nEXP(): Exponentiation.\nSQRT(): Square root.\nPOW(): Power transformation.\n\nPOW(2.0, x) would be \\(2^x\\)\nPOW(x, 2.0) would be \\(x^2\\)\n\n\nDuckDB is good about implicit coercion. E.g., the following integer division results in a double:\n\nSELECT DISTINCT \"month\", \"day\", \"day\" / \"month\" AS \"ratio\"\nFROM flights\nWHERE \"month\" &gt;= 5\nORDER BY \"month\", \"day\";\n\n\nDisplaying records 1 - 10\n\n\nmonth\nday\nratio\n\n\n\n\n5\n1\n0.2\n\n\n5\n2\n0.4\n\n\n5\n3\n0.6\n\n\n5\n4\n0.8\n\n\n5\n5\n1.0\n\n\n5\n6\n1.2\n\n\n5\n7\n1.4\n\n\n5\n8\n1.6\n\n\n5\n9\n1.8\n\n\n5\n10\n2.0\n\n\n\n\n\nBut many SQL backends are not so good about implicit coercion.\nUse CAST() to convert to a double before operations that should produce doubles.\n\nSELECT DISTINCT \"month\", \"day\", CAST(\"day\" AS DOUBLE) / CAST(\"month\" AS DOUBLE) AS \"ratio\"\nFROM flights\nWHERE \"month\" &gt;= 5\nORDER BY \"month\", \"day\";\n\n\nDisplaying records 1 - 10\n\n\nmonth\nday\nratio\n\n\n\n\n5\n1\n0.2\n\n\n5\n2\n0.4\n\n\n5\n3\n0.6\n\n\n5\n4\n0.8\n\n\n5\n5\n1.0\n\n\n5\n6\n1.2\n\n\n5\n7\n1.4\n\n\n5\n8\n1.6\n\n\n5\n9\n1.8\n\n\n5\n10\n2.0\n\n\n\n\n\nMutating over partition can be done with OVER (PARTITION BY &lt;variable&gt;). E.g., here is how you find the flight numbers for the longest flights (in terms of air time) from each airport\n\nSELECT \"flight\", \"origin\", \"air_time\"\nFROM\n(\n  SELECT \"flight\", \"origin\", \"air_time\", MAX(\"air_time\") OVER (PARTITION BY \"origin\") AS \"amax\"\n  FROM flights\n)\nWHERE \"air_time\" = \"amax\"\n\n\n3 records\n\n\nflight\norigin\nair_time\n\n\n\n\n15\nEWR\n695\n\n\n51\nJFK\n691\n\n\n745\nLGA\n331\n\n\n\n\n\nIn the above, I chained two SQL queries. Your FROM statement can be the output of another SQL query surrounded by parentheses.\nUse OVER () to go over all rows. Aggregate functions (introduced later) cannot be used in row-level expressions (inside a select) without a partitioning call. E.g., to calculate the z-score of dep_delay we do\n\nSELECT dep_delay, \n  (dep_delay - AVG(dep_delay) OVER ()) / (STDDEV(dep_delay) OVER ()) AS zscore\nFROM flights\n\n\nDisplaying records 1 - 10\n\n\ndep_delay\nzscore\n\n\n\n\n2\n-0.2646\n\n\n4\n-0.2148\n\n\n2\n-0.2646\n\n\n-1\n-0.3392\n\n\n-6\n-0.4635\n\n\n-4\n-0.4138\n\n\n-5\n-0.4387\n\n\n-3\n-0.3889\n\n\n-3\n-0.3889\n\n\n-2\n-0.3641\n\n\n\n\n\nExercise: Calculate the absolute value of departure delay and arrange in descedning order of this new variable.\nExercise: Calculate a new variable called dep_scaled, departure delay minus the minimum divided by the range. So all values are between 0 and 1. Order in descending order of the absolute value of dep_scaled. Make sure every other variable is still in the query."
  },
  {
    "objectID": "07_sql/07_sql.html#group-summaries",
    "href": "07_sql/07_sql.html#group-summaries",
    "title": "SQL",
    "section": "Group Summaries",
    "text": "Group Summaries\n\nSQL has a few summary functions (SQL calls these “Aggregates”)\n\nCOUNT(): Count the number of rows.\nAVG(): Calculate average.\nMEDIAN(): Median (not standard across all DBMS’s).\nSUM(): Summation.\nMIN(): Minimum.\nMAX(): Maximum.\nSTDDEV(): Standard deviation.\nVARIANCE(): Variance\n\nBy default, all missing data are ignored (like setting na.rm = TRUE).\nThese are calculated in a SELECT command\nLet’s calculate the average departue delay\nSQL\n\nSELECT AVG(\"dep_delay\") AS \"dep_delay\" \nFROM flights;\n\n\n1 records\n\n\ndep_delay\n\n\n\n\n12.64\n\n\n\n\n\nR equivalent:\nR\n\nflights |&gt;\n  summarize(dep_delay = mean(dep_delay, na.rm = TRUE))\n\nUse the GROUP BY command to calculate group summaries.\nSQL\n\nSELECT \"origin\", AVG(\"dep_delay\")\nFROM flights\nGROUP BY \"origin\";\n\n\n3 records\n\n\norigin\navg(dep_delay)\n\n\n\n\nEWR\n15.11\n\n\nLGA\n10.35\n\n\nJFK\n12.11\n\n\n\n\n\nR equivalent\nR\n\nflights |&gt;\n  select(origin, dep_delay) |&gt;\n  group_by(origin) |&gt;\n  summarize(dep_delay = mean(dep_delay, na.rm = TRUE))\n\nYou can get distinct rows by using the prefix DISTINCT in a SELECT statement. E.g. the following will pick up all unique origins\n\nSELECT DISTINCT \"origin\" \nFROM flights;\n\n\n3 records\n\n\norigin\n\n\n\n\nEWR\n\n\nJFK\n\n\nLGA\n\n\n\n\n\nExercise: Calculate the t-statistic of departure delay for each originating airport."
  },
  {
    "objectID": "07_sql/07_sql.html#creating-grouped-summaries-without-grouping-the-results",
    "href": "07_sql/07_sql.html#creating-grouped-summaries-without-grouping-the-results",
    "title": "SQL",
    "section": "Creating Grouped Summaries without Grouping the Results",
    "text": "Creating Grouped Summaries without Grouping the Results\n\nThis section is based on Richard Ressler’s notes.\nThere may be times when you want to add a grouped summary to the data without collapsing the data into the groups.\nAs an example, you want to calculate the total departure delay time for each destination so you can use it to then calculate the percentage of that departure delay time for each airline flying to that destination.\nYou could do that with a summarized dataframe and then a join.\nin R, this can be done by using mutate() instead of summarize() to add a new column to the data frame while preserving all of the rows.\nIn SQL, you have to indicate you want to use the aggregate function as a Window Function to combine grouped aggregated and non-aggregated data into a single result-set table.\nWhen operating as a window function, the table is partitioned into sets of records based on a Field.\nThen, the aggregate function is applied to the set of records in each partition and a new field is added to the record with the aggregated value for that partition.\nTo use an aggregate function as a window function, use the OVER clause with a (PARTITION BY myfield) modifier.\nHere is how you find the flight numbers for the longest flights (in terms of air time) from each airport.\nSQL\n\nSELECT\n    \"flight\",\n    \"origin\",\n    \"air_time\",\n    MAX(\"air_time\") \n    OVER (PARTITION BY \"origin\") AS \"amax\"\nFROM\n    flights\nLIMIT 10 OFFSET 120827;\n\n\nDisplaying records 1 - 10\n\n\nflight\norigin\nair_time\namax\n\n\n\n\n4120\nEWR\n58\n695\n\n\n1895\nEWR\n111\n695\n\n\n4129\nEWR\n53\n695\n\n\n505\nEWR\n167\n695\n\n\n1110\nEWR\n343\n695\n\n\n3840\nEWR\n181\n695\n\n\n4688\nEWR\n42\n695\n\n\n4104\nEWR\n50\n695\n\n\n1691\nJFK\n309\n691\n\n\n1447\nJFK\n75\n691\n\n\n\n\n\nNote the amax field is new and the values are all the same for the records for each origin.\nSee SQL Window Functions or SQL PARTITION BY Clause overview for other examples.\nThis approach can be useful as a sub-query (or inner nested query) inside the outer query FROM clause.\nHere is how you find the flight numbers and destination for the longest flights (in terms of air time) from each airport after using the sub-query to find the longest air time from each origin.\nSQL\n\nSELECT\n    \"flight\",\n    \"origin\",\n    \"dest\",\n    \"air_time\"\nFROM\n    (\n    SELECT\n        \"flight\",\n        \"origin\",\n        \"dest\",\n        \"air_time\",\n        MAX(\"air_time\") OVER (PARTITION BY \"origin\") AS \"amax\"\n    FROM\n        flights\n)\nWHERE\n    \"air_time\" = \"amax\"\n\n\n3 records\n\n\nflight\norigin\ndest\nair_time\n\n\n\n\n745\nLGA\nDEN\n331\n\n\n51\nJFK\nHNL\n691\n\n\n15\nEWR\nHNL\n695\n\n\n\n\n\nNote that all fields used in the outer query must be returned by the inner query\nExercise: For each originating airport, calculate a “robust z-score” (rzscore) of departure delay, the value of the variable minus the median, divided by the median absolute deviation. Make sure all variables are selected. Arrange in descending order of rzscore."
  },
  {
    "objectID": "07_sql/07_sql.html#recoding",
    "href": "07_sql/07_sql.html#recoding",
    "title": "SQL",
    "section": "Recoding",
    "text": "Recoding\n\nUse the following CASE-WHEN syntax to recode values\nSQL\n\nSELECT \"flight\", \"origin\", CASE\n    WHEN (\"origin\" = 'JFK') THEN 'John F. Kennedy'\n    WHEN (\"origin\" = 'LGA') THEN 'LaGaurdia'\n    WHEN (\"origin\" = 'EWR') THEN 'Newark Liberty'\n    END AS \"olong\"\nFROM flights;\n\n\nDisplaying records 1 - 10\n\n\nflight\norigin\nolong\n\n\n\n\n1545\nEWR\nNewark Liberty\n\n\n1714\nLGA\nLaGaurdia\n\n\n1141\nJFK\nJohn F. Kennedy\n\n\n725\nJFK\nJohn F. Kennedy\n\n\n461\nLGA\nLaGaurdia\n\n\n1696\nEWR\nNewark Liberty\n\n\n507\nEWR\nNewark Liberty\n\n\n5708\nLGA\nLaGaurdia\n\n\n79\nJFK\nJohn F. Kennedy\n\n\n301\nLGA\nLaGaurdia\n\n\n\n\n\nR equivalent:\nR\n\nflights |&gt;\n  select(flight, origin) |&gt;\n  mutate(olong = case_when(\n    origin == \"JFK\" ~ \"John F. Kennedy\",\n    origin == \"LGA\" ~ \"LaGuardia\",\n    origin == \"EWR\" ~ \"Newark Liberty\")\n  )\n\n## or\nflights |&gt;\n  select(flight, origin) |&gt;\n  mutate(olong = recode(\n    origin,\n    \"JFK\" = \"John F. Kennedy\",\n    \"LGA\" = \"LaGuardia\",\n    \"EWR\" = \"Newark Liberty\")\n  )\n\nYou can also use CASE-WHEN to recode based on other logical operations\nSQL\n\nSELECT \"flight\", \"air_time\", CASE\n    WHEN (\"air_time\" &gt; 2500) THEN 'Long'\n    WHEN (\"air_time\" &lt;= 2500) THEN 'Short'\n    END AS \"qual_dist\"\nFROM flights;\n\n\nDisplaying records 1 - 10\n\n\nflight\nair_time\nqual_dist\n\n\n\n\n1545\n227\nShort\n\n\n1714\n227\nShort\n\n\n1141\n160\nShort\n\n\n725\n183\nShort\n\n\n461\n116\nShort\n\n\n1696\n150\nShort\n\n\n507\n158\nShort\n\n\n5708\n53\nShort\n\n\n79\n140\nShort\n\n\n301\n138\nShort\n\n\n\n\n\nR equivalent:\nR\n\nflights |&gt;\n  select(flight, air_time) |&gt;\n  mutate(qual_dist = case_when(\n    air_time &gt; 2500 ~ \"Long\",\n    air_time &lt;= 2500 ~ \"Short\")\n  )\n\n## or\nflights |&gt;\n  select(flight, air_time) |&gt;\n  mutate(qual_dist = if_else(air_time &gt; 2500, \"Long\", \"Short\"))"
  },
  {
    "objectID": "07_sql/07_sql.html#joining",
    "href": "07_sql/07_sql.html#joining",
    "title": "SQL",
    "section": "Joining",
    "text": "Joining\n\nFor joining, in the SELECT call, you write out all of the columns in both tables that you are joining.\nIf there are shared column names, you need to distinguish between the two via table1.\"var\" or table2.\"var\" etc…\nUse LEFT JOIN to declare a left join, and ON to declare the keys.\nSQL\n\n-- flight is from the flights table\n-- type is from the planes table\n-- both tables have a tailnum column, so we need to tell them apart\n-- if you list both tailnums in SELECT, you'll get two tailnum columns\nSELECT \"flight\", flights.\"tailnum\", \"type\" \nFROM flights\nJOIN planes\nON flights.\"tailnum\" = planes.\"tailnum\";\n\n\nDisplaying records 1 - 10\n\n\nflight\ntailnum\ntype\n\n\n\n\n461\nN693DL\nFixed wing multi engine\n\n\n569\nN846UA\nFixed wing multi engine\n\n\n4424\nN19966\nFixed wing multi engine\n\n\n6177\nN34111\nFixed wing multi engine\n\n\n731\nN319NB\nFixed wing multi engine\n\n\n684\nN809UA\nFixed wing multi engine\n\n\n1279\nN328NB\nFixed wing multi engine\n\n\n1691\nN34137\nFixed wing multi engine\n\n\n1447\nN117UW\nFixed wing multi engine\n\n\n583\nN632JB\nFixed wing multi engine\n\n\n\n\n\nR equivalent:\nR\n\nplanes |&gt;\n  select(tailnum, type) -&gt;\n  planes2\nflights |&gt;\n  select(flight, tailnum) |&gt;\n  left_join(planes2, by = \"tailnum\")\n\n# A tibble: 336,776 × 3\n   flight tailnum type                   \n    &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                  \n 1   1545 N14228  Fixed wing multi engine\n 2   1714 N24211  Fixed wing multi engine\n 3   1141 N619AA  Fixed wing multi engine\n 4    725 N804JB  Fixed wing multi engine\n 5    461 N668DN  Fixed wing multi engine\n 6   1696 N39463  Fixed wing multi engine\n 7    507 N516JB  Fixed wing multi engine\n 8   5708 N829AS  Fixed wing multi engine\n 9     79 N593JB  Fixed wing multi engine\n10    301 N3ALAA  &lt;NA&gt;                   \n# ℹ 336,766 more rows\n\n\nThe other joins are:\n\nRIGHT JOIN\nINNER JOIN\nFULL OUTER JOIN"
  },
  {
    "objectID": "07_sql/07_sql.html#creating-some-tables",
    "href": "07_sql/07_sql.html#creating-some-tables",
    "title": "SQL",
    "section": "Creating some Tables",
    "text": "Creating some Tables\n\nYou can create tables using SQL.\nLet’s create a new, temporary, connection that we can put some example tables in.\nR\n\ntmpcon &lt;- dbConnect(duckdb())\n\nYou can use CREATE TABLE to declare new tables and INSERT INTO to insert new rows into that table.\n\nNote that I am putting a semicolon “;” after each SQL call in the same code chunk.\n\nSQL\n\nCREATE TABLE table4a (\n    \"country\" VARCHAR,\n    \"1999\"   INTEGER,\n    \"2000\"   INTEGER\n);\n\nINSERT INTO table4a (\"country\", \"1999\", \"2000\") \n    VALUES\n        ('Afghanistan',    745,   2666),\n        ('Brazil'     ,  37737,  80488),\n        ('China'      , 212258, 213766);\n\nBut DBI::dbWriteTable() will add a table to a connection from R.\nR\n\ndata(\"table1\", package = \"tidyr\")\ndbWriteTable(conn = tmpcon, name = \"table1\", value = table1)\ndata(\"table2\", package = \"tidyr\")\ndbWriteTable(conn = tmpcon, name = \"table2\", value = table2)\ndata(\"table3\", package = \"tidyr\")\ndbWriteTable(conn = tmpcon, name = \"table3\", value = table3)\ndata(\"table4b\", package = \"tidyr\")\ndbWriteTable(conn = tmpcon, name = \"table4b\", value = table4b)\ndata(\"table5\", package = \"tidyr\")\ndbWriteTable(conn = tmpcon, name = \"table5\", value = table5)\n\nHere they are:\nSQL\n\nDESCRIBE;\n\n\n6 records\n\n\n\n\n\n\n\n\n\n\ndatabase\nschema\nname\ncolumn_names\ncolumn_types\ntemporary\n\n\n\n\nmemory\nmain\ntable1\ncountry , year , cases , population\nVARCHAR, DOUBLE , DOUBLE , DOUBLE\nFALSE\n\n\nmemory\nmain\ntable2\ncountry, year , type , count\nVARCHAR, DOUBLE , VARCHAR, DOUBLE\nFALSE\n\n\nmemory\nmain\ntable3\ncountry, year , rate\nVARCHAR, DOUBLE , VARCHAR\nFALSE\n\n\nmemory\nmain\ntable4a\ncountry, 1999 , 2000\nVARCHAR, INTEGER, INTEGER\nFALSE\n\n\nmemory\nmain\ntable4b\ncountry, 1999 , 2000\nVARCHAR, DOUBLE , DOUBLE\nFALSE\n\n\nmemory\nmain\ntable5\ncountry, century, year , rate\nVARCHAR, VARCHAR, VARCHAR, VARCHAR\nFALSE"
  },
  {
    "objectID": "07_sql/07_sql.html#tidyr-stuff",
    "href": "07_sql/07_sql.html#tidyr-stuff",
    "title": "SQL",
    "section": "Tidyr stuff",
    "text": "Tidyr stuff\n\nRecall the four main verbs of tidying: gathering, spreading, separating, and uniting.\nThese are very rarely used in SQL.\nSQL is used mostly to query a subset of data. You would then load in that data using more advanced methods (R, Python, Excel, etc).\nIt’s still possible to spread and gather (but not separate and unite), but a huge pain and folks don’t typically do it."
  },
  {
    "objectID": "07_sql/07_sql.html#writing-tables-to-a-csv-file",
    "href": "07_sql/07_sql.html#writing-tables-to-a-csv-file",
    "title": "SQL",
    "section": "Writing Tables to a CSV File",
    "text": "Writing Tables to a CSV File\n\nYou can use COPY TO to write the outputs of a SQL query to a CSV file.\nThe syntax for this is\nSQL\nCOPY\n(\nYour SQL query goes here\n)\nTO 'myfile.csv' (HEADER, DELIMITER ',');\nLet’s write some group summaries to a CSV file\nSQL\n\nCOPY\n(\nSELECT \"origin\", AVG(\"dep_delay\")\nFROM flights\nGROUP BY \"origin\"\n)\nTO 'summaries.csv' (HEADER, DELIMITER ',');\n\nThis is what the resulting file looks like:\n\n\norigin,avg(dep_delay)\nJFK,12.112159099217665\nEWR,15.10795435218885\nLGA,10.3468756464944"
  },
  {
    "objectID": "06_python/06_numpy.html",
    "href": "06_python/06_numpy.html",
    "title": "Numpy",
    "section": "",
    "text": "Python basics.\nNumpy arrays.\nChapter 2 of Python Data Science Handbook."
  },
  {
    "objectID": "06_python/06_numpy.html#useful-functions-over-vectors",
    "href": "06_python/06_numpy.html#useful-functions-over-vectors",
    "title": "Numpy",
    "section": "Useful functions over vectors",
    "text": "Useful functions over vectors\n\nIn R, we have functions operate on objects (e.g. log(x), sort(x), etc).\nPython also has functions that operate on objects. But objects usually have functions associated with them directly. You access these functions by a period after the object name. These functions are called “methods”. Use tab completion to scroll through the available methods of an object.\nPython\n\nvec.sort() # sort\nvec.min() # minimum\nvec.max() # maximum\nvec.mean() # mean\nvec.sum() # sum\nvec.var() # variance\n\nBut there are still loads of useful functions that operate on objects.\nPython\n\nnp.sort(vec)\nnp.min(vec)\nnp.max(vec)\nnp.mean(vec)\nnp.sum(vec)\nnp.var(vec)\nnp.size(vec)\nnp.exp(vec)\nnp.log(vec)"
  },
  {
    "objectID": "06_python/06_seaborn_objects.html",
    "href": "06_python/06_seaborn_objects.html",
    "title": "The seaborn.objects Interface",
    "section": "",
    "text": "Learning Objectives\n\nData visualization with seaborn.objects.\nhttps://seaborn.pydata.org/tutorial/objects_interface.html\n\n\n\nPython Overview\n\n\n\nIn R I Want\nIn Python I Use\n\n\n\n\nBase R\nnumpy\n\n\ndplyr/tidyr\npandas\n\n\nggplot2\nmatplotlib/seaborn\n\n\n\n\nI previously taught how to use the basic seaborn interface. But in 2022 the author introduced an interface more similar to ggplot which seems to be the future of the package. So here is a lecture on the seaborns.objects interface.\nThis interface is experimental, doesn’t have all of the features you would need for data analysis, and will likely change. But I think it looks cool.\n\n\n\nImport Matplotlib and Seaborn, and Load Dataset\nR\n\nlibrary(ggplot2)\ndata(\"mpg\")\n\nAll other code will be Python unless otherwise marked.\n\nimport matplotlib.pyplot as plt # base plotting functionality\nimport seaborn as sns           # Original interface\nimport seaborn.objects as so    # ggplot2-like interface\nmpg = r.mpg\n\n\n\nBasics\n\nUse so.Plot() to instantiate a Plot object and define asthetic mappings.\n\nJust like ggplot().\n\nUse the .add() method of the Plot object to add geometric objects and a statistical transformation.\nAs in ggplot, each aesthetic mapping is followed by a statistical transformation before plotting. But unlike in ggplot, you need to specify this statistical transformation manually.\n\nE.g. aggregating a categorical variable into a counts before plotting those counts on the y-axis.\nE.g. binning a quantitative variable to make a histogram.\n\nYou specify the statistical transformation as the second argument in .add().\n\n\n\nOne Quantitative Variable: Histogram\n\nNotice that we need to specify the so.Hist() statistical transformation to generate a histogram.\nWe use the so.Bars() geometric object to plot it after the statistical transformation.\n\npl = (\n  so.Plot(mpg, x = \"hwy\")\n  .add(so.Bars(), so.Hist(bins = 10))\n)\npl.show()\n\n\n\n\n\n\n\n\n\n\n\nOne Categorical Variable: Barplot\n\nWe use the so.Bar() geometric object after the so.Count() statistical transformation.\n\npl = (\n  so.Plot(mpg, x = \"class\")\n  .add(so.Bar(), so.Count())\n)\npl.show()\n\n\n\n\n\n\n\n\nso.Bar() (for categorical data) and so.Bars() (for quantitative data) seem to be only slightly different based on the defaults.\n\n\n\nDodging Barplots\n\nIf you are creating two barplots, annotated by color, you need to be explicit that the bars should dodge eachother with a so.Dodge() transformation.\n\npl = (\n  so.Plot(mpg, x = \"class\", color = \"drv\")\n  .add(so.Bar(), so.Count(), so.Dodge())\n)\npl.show()\n\n\n\n\n\n\n\n\n\n\n\nOne Quantitative Variable, One Categorical Variable: Boxplot\n\nThis interface is currently (November 2022) missing boxplotting functions, so you need to use the old interface.\n\nplt.clf()\nsns.boxplot(data = mpg, x = \"class\", y = \"hwy\")\nplt.show()\n\n\n\n\n\n\n\nplt.clf()\n\n\n\n\n\n\n\n\nI think this is the closest thing to a boxplot you can get right now:\n\npl = (\n  so.Plot(mpg, x = \"class\", y = \"hwy\")\n  .add(so.Dash(width = 0.4), so.Perc())\n  .add(so.Range())\n  .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2))\n  .add(so.Range(), so.Perc([25, 75]), so.Shift(x=-0.2))\n)\npl.show()\n\n\n\n\n\n\n\n\n\n\n\nTwo Quantitative Variables: Scatterplot\n\nBase scatterplot uses the so.Dots() geometric object:\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .add(so.Dots())\n)\npl.show()\n\n\n\n\n\n\n\n\nUse the so.Jitter() statistical transformation to make a jittered scatterplot.\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .add(so.Dots(), so.Jitter(1))\n)\npl.show()\n\n\n\n\n\n\n\n\nUse so.Line() (geometric object) and so.PolyFit() (statistical transformation) to add a smoother.\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .add(so.Dots())\n  .add(so.Line(), so.PolyFit())\n)\npl.show()\n\n\n\n\n\n\n\n\n\nI don’t think it does lowess or gam or splines yet, but just a polynomial, which is not optimal. You can control the order of the polynomial by the order argument.\n\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .add(so.Dots())\n  .add(so.Line(), so.PolyFit(order = 1))\n)\npl.show()\n\n\n\n\n\n\n\n\nAnnotate by a third variable by adding a color mapping:\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\", color = \"drv\")\n  .add(so.Dots())\n  .add(so.Line(), so.PolyFit(order = 1))\n)\npl.show()\n\n\n\n\n\n\n\n\n\n\n\nFaceting\n\nFacet by the .facet() method.\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .facet(row = \"drv\")\n  .add(so.Dots())\n)\npl.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Look\n\nYou can change the scaling using the .scale() method. E.g. here is a \\(\\log_2\\) transformation for the \\(x\\)-axis.\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .add(so.Dots())\n  .add(so.Line(), so.PolyFit(order = 1))\n  .scale(x = \"log2\")\n)\npl.show()\n\n\n\n\n\n\n\n\nYou can change the labels by .label().\n\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .add(so.Dots())\n  .label(x = \"Displacement (L)\", y = \"Highway MPG\")\n)\npl.show()\n\n\n\n\n\n\n\n\nYou can change the theme using .theme(). But it is a little verbose right now.\n\nfrom seaborn import axes_style\npl = (\n  so.Plot(mpg, x = \"displ\", y = \"hwy\")\n  .add(so.Dots())\n  .add(so.Line(), so.PolyFit(order = 1))\n  .theme({**axes_style(\"whitegrid\"), \"grid.linestyle\": \":\"})\n)\npl.show()\n\n\n\n\n\n\n\n\n\n\n\nExercises\nConsider the palmer penguins data, which you can load via\n\npenguins = sns.load_dataset(\"penguins\")\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    object \n 1   island             344 non-null    object \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    object \ndtypes: float64(4), object(3)\nmemory usage: 18.9+ KB\n\n\n\nMake a visualization of bill length versus bill depth, annotated by species.\nAdd OLS lines to for each species to the same plot object you created in part 1 (don’t rerun so.Plot()).\nUse pandas.cut() to convert body mass into five equally spaced levels.\nFacet your plot from part 2 by the above transformation. You will have to redo the object since we are using a different data frame here.\nMake a visualization for the number of each species in the dataset. Make sure you have good labels."
  },
  {
    "objectID": "06_python/06_matplotlib.html",
    "href": "06_python/06_matplotlib.html",
    "title": "Plotting with Matplotlib and Seaborn",
    "section": "",
    "text": "Data visualization with seaborn and matplotlib\nChapter 4 of Python Data Science Handbook."
  },
  {
    "objectID": "06_python/06_matplotlib.html#linessmoothers",
    "href": "06_python/06_matplotlib.html#linessmoothers",
    "title": "Plotting with Matplotlib and Seaborn",
    "section": "Lines/Smoothers",
    "text": "Lines/Smoothers\n\nUse sns.regplot() to make a scatterplot with a regression line or a loess smoother.\nRegression line with 95% Confidence interval\n\nsns.regplot(x='displ', y='hwy', data=mpg)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nLoess smoother with confidence interval removed.\n\nsns.regplot(x='displ', y='hwy', data=mpg, lowess=True, ci='None')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()"
  },
  {
    "objectID": "06_python/06_matplotlib.html#annotating-by-third-variable",
    "href": "06_python/06_matplotlib.html#annotating-by-third-variable",
    "title": "Plotting with Matplotlib and Seaborn",
    "section": "Annotating by Third Variable",
    "text": "Annotating by Third Variable\n\nUse the hue or style arguments to annotate by a categorical variable:\n\nsns.scatterplot(x='displ', y='hwy', hue='class', data=mpg)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\n\n\nsns.scatterplot(x='displ', y='hwy', style='class', data=mpg)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\n\nUse the hue or size arguments to annotate by a quantitative variable:\n\nsns.scatterplot(x='cty', y='hwy', hue='displ', data=mpg)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()\n\n\nsns.scatterplot(x='cty', y='hwy', size='displ', data=mpg)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.clf()"
  },
  {
    "objectID": "06_python/windows_setup.html",
    "href": "06_python/windows_setup.html",
    "title": "DATA 413/613 Data Science",
    "section": "",
    "text": "Install the latest version of Python via the Microsoft Store App. 2 Make sure that you Add Python to PATH if that option is available during install.\nOpen the Windows PowerShell by searching for it in the “Type here to search” box.\nIn the Windows PowerShell, upgrade pip by running:\npython -m pip install --upgrade pip\nInstall jupyterlab via running the following in Windows PowerShell:\npip install jupyterlab\nYou should now be able to run jupyterlab by running the following in Windows PowerShell:\npython -m jupyterlab\n\nI was able to add jupyter-lab to the PATH variable via: 1. Type where python to find the basic location of Python. 2. Search for the location of jupyter.exe. 3. Add this location to the PATH. For example, jupyter.exe was located in C:\\Users\\dgerard\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\Scripts So I added that to the path via (in PowerShell): dos    [Environment]::SetEnvironmentVariable(\"Path\", $env:Path + \";C:\\Users\\dgerard\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\Scripts\", [EnvironmentVariableTarget]::User) 4. Check it is now in the Path by running in PowerShell dos    jupyter --version 5. You should then be able to run jypterlab via the PowerShell with dos    jupyter-lab\nOnce you add jupyter-lab to the Path, you can then install the R kernel via: 1. Install the IRkernel in R: r    install.packages('IRkernel') 2. Run the following in R using sudo r    IRkernel::installspec(user = FALSE)\nBut the above doesn’t work for R… hmm\nYou can install jupyter-lab for windows subystem for linux by following these commands: https://medium.com/@sayanghosh_49221/jupyter-notebook-in-windows-subsystem-for-linux-wsl-f075f7ec8691\nThe trick, after install python3 and python3-pip, is to open up\nemacs ~/.bashrc\nThen adding this to the PATH\nalias jupyter-notebook=\"~/.local/bin/jupyter-notebook --no-browser\"\nYou can now open up the location in your windows browser and it works!"
  },
  {
    "objectID": "06_python/ubuntu_setup.html",
    "href": "06_python/ubuntu_setup.html",
    "title": "DATA 413/613 Data Science",
    "section": "",
    "text": "Install pip3 (which should automatically install python3)\nsudo apt-get install python3-pip\nUse pip3 to install useful packages\npip3 install \n   pandas \\\n   fastparquet \\\n   pyarrow \\\n   tables \\\n   plotly \\\n   seaborn \\\n   xlrd\nInstall JupyterLab\npip3 install jupyterlab\nAdd user-level bin directy to PATH\nexport PATH=$PATH:~/.local/bin\nRestart computer\nInstall JupyterLab Extensions\njupyter labextension install \\\n   @jupyterlab/toc \\\n   jupyterlab-chart-editor \\\n   jupyterlab-spreadsheet\nYou should now be able to run JupyterLab with Python3 kernel via\njupyter lab\n\nIf you want to run the R kernel in JupyterLab:\n\nInstall the IRkernel in R:\ninstall.packages('IRkernel')\nRun the following in R using sudo\nIRkernel::installspec(user = FALSE)\nIf that doesn’t work, try aliasing sudo to take your local path:\nalias sudo='sudo -E env \"PATH=$PATH\"'\nthen rerun 2.\n\nIf you want to run the Julia kernel in JupyterLab:\n\nInstall julia\nsudo apt-get install julia\nInstall IJulia inside julia\nimport Pkg; Pkg.add(\"IJulia\")\nRun in julia:\nusing IJulia\njupyterlab()"
  },
  {
    "objectID": "06_python/06_pandas.html",
    "href": "06_python/06_pandas.html",
    "title": "Data Manipulation with Pandas",
    "section": "",
    "text": "Learning Objectives\n\nRead in and manipulate data with pandas.\nChapter 3 of Python Data Science Handbook.\n\n\n\nPython Overview\n\n\n\nIn R I Want\nIn Python I Use\n\n\n\n\nBase R\nnumpy\n\n\ndplyr/tidyr\npandas\n\n\nggplot2\nmatplotlib/seaborn\n\n\n\n\n\nPandas versus Tidyverse\n\nThese are the equivalencies you should have in mind.\n&lt;DataFrame&gt;.fun() means that fun() is a method of the &lt;DataFrame&gt; object.\n&lt;Series&gt;.fun() means that fun() is a method of the &lt;Series&gt; object.\n\n\n\n\n\n\n\ntidyverse\npandas\n\n\n\n\narrange()\n&lt;DataFrame&gt;.sort_values()\n\n\nbind_rows()\npandas.concat()\n\n\nfilter()\n&lt;DataFrame&gt;.query()\n\n\ngather() and pivot_longer()\n&lt;DataFrame&gt;.melt()\n\n\nglimpse()\n&lt;DataFrame&gt;.info() and &lt;DataFrame&gt;.head()\n\n\ngroup_by()\n&lt;DataFrame&gt;.groupby()\n\n\nif_else()\nnumpy.where()\n\n\nleft_join()\npandas.merge()\n\n\nlibrary()\nimport\n\n\nmutate()\n&lt;DataFrame&gt;.eval() and &lt;DataFrame&gt;.assign()\n\n\nread_csv()\npandas.read_csv()\n\n\nrecode()\n&lt;DataFrame&gt;.replace()\n\n\nrename()\n&lt;DataFrame&gt;.rename()\n\n\nselect()\n&lt;DataFrame&gt;.filter() and &lt;DataFrame&gt;.drop()\n\n\nseparate()\n&lt;Series&gt;.str.split()\n\n\nslice()\n&lt;DataFrame&gt;.iloc()\n\n\nspread() and pivot_wider()\n&lt;DataFrame&gt;.pivot_table().reset_index()\n\n\nsummarize()\n&lt;DataFrame&gt;.agg()\n\n\nunite()\n&lt;Series&gt;.str.cat()\n\n\n|&gt;\nEnclose pipeline in ()\n\n\n\n\n\n\nImporting libraries\n\nPython: import &lt;package&gt; as &lt;alias&gt;.\nPython\n\nimport numpy as np\nimport pandas as pd\n\nYou can use the alias that you define in place of the package name. In Python we write down the package name a lot, so it is nice for it to be short.\nR equivalent\nR\n\nlibrary(tidyverse)\n\n\n\n\nReading in and Printing Data\n\nWe’ll demonstrate most methods with the “estate” data that we’ve seen before: https://data-science-master.github.io/lectures/data/estate.csv\nYou can read about these data here: https://data-science-master.github.io/lectures/data.html\nPython: pd.read_csv(). There is a family of reading functions in pandas (fixed width files, e.g.). Use tab-completion to scroll through them.\nPython\n\nestate = pd.read_csv(\"../data/estate.csv\")\n\nR equivalent:\nR\n\nestate &lt;- read_csv(\"../data/estate.csv\")\n\nUse the info() and head() methods to get a view of the data.\nPython\n\nestate.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 522 entries, 0 to 521\nData columns (total 12 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   Price    522 non-null    int64 \n 1   Area     522 non-null    int64 \n 2   Bed      522 non-null    int64 \n 3   Bath     522 non-null    int64 \n 4   AC       522 non-null    int64 \n 5   Garage   522 non-null    int64 \n 6   Pool     522 non-null    int64 \n 7   Year     522 non-null    int64 \n 8   Quality  522 non-null    object\n 9   Style    522 non-null    int64 \n 10  Lot      522 non-null    int64 \n 11  Highway  522 non-null    int64 \ndtypes: int64(11), object(1)\nmemory usage: 49.1+ KB\n\n\nPython\n\nestate.head()\n\n    Price  Area  Bed  Bath  AC  ...  Year  Quality  Style    Lot  Highway\n0  360000  3032    4     4   1  ...  1972   Medium      1  22221        0\n1  340000  2058    4     2   1  ...  1976   Medium      1  22912        0\n2  250000  1780    4     3   1  ...  1980   Medium      1  21345        0\n3  205500  1638    4     2   1  ...  1963   Medium      1  17342        0\n4  275500  2196    4     3   1  ...  1968   Medium      7  21786        0\n\n[5 rows x 12 columns]\n\n\nR equivalent:\nR\n\nglimpse(estate)\n\nRows: 522\nColumns: 12\n$ Price   &lt;dbl&gt; 360000, 340000, 250000, 205500, 275500, 248000, 229900, 150000…\n$ Area    &lt;dbl&gt; 3032, 2058, 1780, 1638, 2196, 1966, 2216, 1597, 1622, 1976, 28…\n$ Bed     &lt;dbl&gt; 4, 4, 4, 4, 4, 4, 3, 2, 3, 3, 7, 3, 5, 5, 3, 5, 2, 3, 4, 3, 4,…\n$ Bath    &lt;dbl&gt; 4, 2, 3, 2, 3, 3, 2, 1, 2, 3, 5, 4, 4, 4, 3, 5, 2, 4, 3, 3, 3,…\n$ AC      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Garage  &lt;dbl&gt; 2, 2, 2, 2, 2, 5, 2, 1, 2, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ Pool    &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Year    &lt;dbl&gt; 1972, 1976, 1980, 1963, 1968, 1972, 1972, 1955, 1975, 1918, 19…\n$ Quality &lt;chr&gt; \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"M…\n$ Style   &lt;dbl&gt; 1, 1, 1, 1, 7, 1, 7, 1, 1, 1, 7, 1, 7, 5, 1, 6, 1, 7, 7, 1, 2,…\n$ Lot     &lt;dbl&gt; 22221, 22912, 21345, 17342, 21786, 18902, 18639, 22112, 14321,…\n$ Highway &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\nDataFrames and Series\n\nPandas reads in tabular data as a DataFrame object.\nJust as R’s data.frame is a list of a bunch of vectors, Panda’s DataFrame contains a bunch of Series objects.\nA Series object is a generalization of a numpy array. So you can use numpy functions on it.\nPython\n\nx = pd.Series([1, 4, 2, 1])\nx[2:3]\nx[pd.Series([0, 2])]\nx[x &gt;= 2]\nnp.sum(x)\n\n\n\n\nExtract Variables\n\nPython: Use a period. This extracts the column as a Pandas Series.\nPython\n\nestate.Price\n\nThen you can use all of those numpy functions on the Series\nPython\n\nnp.mean(estate.Price)\nnp.max(estate.Price)\n\nR equivalent: Use a $:\nR\n\nestate$Price\n\n\n\n\nFiltering/Arranging Rows (Observations)\n\nFilter rows based on booleans (logicals) with query(). The queries need to be in quotes.\nPython\n\nestate.query('(Price &gt; 300000) & (Area &lt; 2500)')\n\nSome folks use bracket notation, which is more similar to base R\nPython\n\nestate[(estate.Price &gt; 300000) & (estate.Area &lt; 2500)]\n\nR equivalent:\nR\n\nfilter(estate, Price &gt; 300000, Area &lt; 2500)\n\nSelect rows by numerical indices with iloc()\nPython\n\nestate.iloc[[1, 4, 10]]\n\n     Price  Area  Bed  Bath  AC  ...  Year  Quality  Style    Lot  Highway\n1   340000  2058    4     2   1  ...  1976   Medium      1  22912        0\n4   275500  2196    4     3   1  ...  1968   Medium      7  21786        0\n10  190000  2812    7     5   0  ...  1966      Low      7  56639        0\n\n[3 rows x 12 columns]\n\n\nR equivalent:\nR\n\nslice(estate, 1, 4, 10)\n\n# A tibble: 3 × 12\n   Price  Area   Bed  Bath    AC Garage  Pool  Year Quality Style   Lot Highway\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 360000  3032     4     4     1      2     0  1972 Medium      1 22221       0\n2 205500  1638     4     2     1      2     0  1963 Medium      1 17342       0\n3 160000  1976     3     3     0      1     0  1918 Low         1 32358       0\n\n\nArrange rows by sort_values().\nPython\n\nestate.sort_values(by=\"Price\", ascending=False)\n\nR equivalent\nR\n\narrange(estate, desc(Price))\n\nExercise: Use both the tidyverse and pandas to extract all medium quality homes that have a pool and arrange the rows in increasing order of price.\n\n\n\nSelecting Columns (Variables)\n\nVariables are selected using filter().\nPython\n\nestate.filter([\"Price\"])\nestate.filter([\"Price\", \"Area\"])\n\nSome folks use bracket notation, which is more similar to Base R.\nPython\n\nestate[[\"Price\"]]\nestate[[\"Price\", \"Area\"]]\n\nThe inner brackets [] just creates a Python list. The outer brackets [] says that we are subsetting the columns.\nR equivalent:\nR\n\nselect(estate, Price)\nselect(estate, Price, Area)\n\nDropping a column is done by drop(). The axis=1 argument says to drop by columns (rather than by “index”, which is something we haven’t covered).\nPython\n\nestate.drop([\"Price\", \"Area\"], axis=1)\n\nR: just use select() with a minus sign.\nR\n\nselect(estate, -Price, -Area)\n\nRenaming variables is done with rename().\nPython\n\nestate.rename({'Price': 'price', 'Area': 'area'}, axis = 'columns')\n\nR equivalence:\nR\n\nrename(estate, price = Price, area = Area)\n\n# A tibble: 522 × 12\n    price  area   Bed  Bath    AC Garage  Pool  Year Quality Style   Lot Highway\n    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1 360000  3032     4     4     1      2     0  1972 Medium      1 22221       0\n 2 340000  2058     4     2     1      2     0  1976 Medium      1 22912       0\n 3 250000  1780     4     3     1      2     0  1980 Medium      1 21345       0\n 4 205500  1638     4     2     1      2     0  1963 Medium      1 17342       0\n 5 275500  2196     4     3     1      2     0  1968 Medium      7 21786       0\n 6 248000  1966     4     3     1      5     1  1972 Medium      1 18902       0\n 7 229900  2216     3     2     1      2     0  1972 Medium      7 18639       0\n 8 150000  1597     2     1     1      1     0  1955 Medium      1 22112       0\n 9 195000  1622     3     2     1      2     0  1975 Low         1 14321       0\n10 160000  1976     3     3     0      1     0  1918 Low         1 32358       0\n# ℹ 512 more rows\n\n\nExercise: Use the tidyverse and pandas to select year, price, and area.\n\n\n\nCreating New Variables (Mutate)\n\nNew variables are created in Python using eval(). Note that we need to place the expression in quotes.\nPython\n\nestate.eval('age = 2013 - Year')\n\nYou can use assign(), but then you need to reference the DataFrame as you extract variables:\nPython\n\nestate.assign(age = 2013 - estate.Year)\n\nR equivalent:\nR\n\nmutate(estate, age = 2013 - Year)\n\nExercise: Use the tidyverse and pandas to calculate the price per unit area.\n\n\n\nPiping\n\nAll of these pandas functions return DataFrames. So, we can apply methods to these DataFrames by just appending methods to the end.\n\nThis is called “method chaining”.\n\nE.g., suppose we want to find the total number of beds/baths and only select the price and this total number to print. Then the following code would work.\nPython\n\nestate.eval('tot = Bed + Bath').filter([\"Price\", \"tot\"])\n\nIf you want to place these operations on different lines, then just place the whole operation within parentheses.\nPython\n\n(\nestate.eval('tot = Bed + Bath')\n  .filter([\"Price\", \"tot\"])\n)\n\nThis looks similar to piping in the tidyverse\nR\n\nestate |&gt;\n  mutate(tot = Bed + Bath) |&gt;\n  select(Price, tot)\n\nExercise: Use pandas to extract all medium quality homes that have a pool and arrange the rows in increasing order of price. Use piping.\n\n\n\nGroup Summaries\n\nSummaries can be calculated by the agg() method. You usually first select the columns whose summaries you want before running agg().\nPython\n\n(\nestate.filter([\"Price\", \"Area\"])\n  .agg(np.mean)\n)\n\n&lt;string&gt;:3: FutureWarning: The provided callable &lt;function mean at 0x1210c3130&gt; is currently using DataFrame.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\nPrice    277894.147510\nArea       2260.626437\ndtype: float64\n\n\nR equivalent\nR\n\nsummarize(estate, Price = mean(Price), Area = mean(Area))\n\n# A tibble: 1 × 2\n    Price  Area\n    &lt;dbl&gt; &lt;dbl&gt;\n1 277894. 2261.\n\n\nUse groupby() to create group summaries.\nPython\n\n(\nestate.filter([\"Price\", \"Area\", \"Bed\", \"Bath\"])\n  .groupby([\"Bed\", \"Bath\"])\n  .agg(np.mean)\n)\n\nR equivalent\nR\n\nestate |&gt;\n  group_by(Bed, Bath) |&gt;\n  summarize(Price = mean(Price), Area = mean(Area))\n\nYou can get multiple summaries out by passing a list of functions:\nPython\n\n(\nestate.filter([\"Price\", \"Area\", \"Quality\"])\n  .groupby(\"Quality\")\n  .agg([np.mean, np.var])\n)\n\nYou can create your own functions and pass those\nPython\n\ndef cv(x):\n  \"\"\"Calculate coefficient of variation\"\"\"\n  return(np.sqrt(np.var(x)) / np.mean(x))\n\n(\nestate.filter([\"Price\", \"Area\"])\n  .agg(cv)\n)\n\nPrice    0.495841\nArea     0.314242\ndtype: float64\n\n\n\n\n\nRecoding\n\nUse replace() with a dict object to recode variable values.\nPython\n\nestate.replace({'AC' : {0: \"No AC\", 1: \"AC\"}})\n\nR equivalent:\nR\n\nestate |&gt;\n  mutate(AC = recode(AC,\n                     \"0\" = \"No AC\",\n                     \"1\" = \"AC\"))\n\nTo recode values based on logical conditions, use np.where().\nPython\n\nestate.assign(isbig = np.where(estate.Price &gt; 300000, \"expensive\", \"cheap\"))\n\nR equivalence:\nR\n\nmutate(estate, isbig = if_else(Price &gt; 300000, \"expensive\", \"cheap\"))\n\n\n\n\nGathering\n\nProblem: One variable spread across multiple columns.\nColumn names are actually values of a variable\nRecall table4a from the tidyr package\nR\n\ndata(\"table4a\")\n\nPython\n\ntable4a = pd.DataFrame({'country': ['Afghanistan', 'Brazil', 'China'],\n                        '1999': [745, 37737, 212258],\n                        '2000': [2666, 80488, 213766]})\ntable4a\n\n       country    1999    2000\n0  Afghanistan     745    2666\n1       Brazil   37737   80488\n2        China  212258  213766\n\n\nSolution: melt().\nPython\n\ntable4a.melt(id_vars='country', value_vars=['1999', '2000'])\n\n       country variable   value\n0  Afghanistan     1999     745\n1       Brazil     1999   37737\n2        China     1999  212258\n3  Afghanistan     2000    2666\n4       Brazil     2000   80488\n5        China     2000  213766\n\n\nR equivalences:\nR\n\ngather(table4a, key = \"variable\", value = \"value\", `1999`, `2000`)\n\n# A tibble: 6 × 3\n  country     variable  value\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;\n1 Afghanistan 1999        745\n2 Brazil      1999      37737\n3 China       1999     212258\n4 Afghanistan 2000       2666\n5 Brazil      2000      80488\n6 China       2000     213766\n\n\nR\n\npivot_longer(table4a, cols = c(\"1999\", \"2000\"), \n             names_to = \"variable\",\n             values_to = \"value\")\n\n# A tibble: 6 × 3\n  country     variable  value\n  &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;\n1 Afghanistan 1999        745\n2 Afghanistan 2000       2666\n3 Brazil      1999      37737\n4 Brazil      2000      80488\n5 China       1999     212258\n6 China       2000     213766\n\n\nRDS visualization:\n\n\nExercise: Use pandas to gather the monkeymem data frame (available at https://data-science-master.github.io/lectures/data/tidy_exercise/monkeymem.csv). The cell values represent identification accuracy of some objects (in percent of 20 trials).\n\n\n\nSpreading\n\nProblem: One observation is spread across multiple rows.\nOne column contains variable names. One column contains values for the different variables.\nRecall table2 from the tidyr package\nR\n\ndata(\"table2\")\n\nPython\n\ntable2 = pd.DataFrame({'country': ['Afghanistan', 'Afghanistan', \n                                   'Afghanistan', 'Afghanistan', \n                                   'Brazil', 'Brazil', 'Brazil', \n                                   'Brazil', 'China', 'China', \n                                   'China', 'China'],\n                       'year': [1999, 1999, 2000, 2000, 1999, 1999, \n                                2000, 2000, 1999, 1999, 2000, 2000],\n                       'type': ['cases', 'population', 'cases', \n                                'population', 'cases', 'population', \n                                'cases', 'population', 'cases', \n                                'population', 'cases', 'population'],\n                       'count': [745, 19987071, 2666, 20595360, 37737,\n                                 172006362, 80488, 174504898, 212258, \n                                 1272915272, 213766, 1280428583]})\ntable2\n\n        country  year        type       count\n0   Afghanistan  1999       cases         745\n1   Afghanistan  1999  population    19987071\n2   Afghanistan  2000       cases        2666\n3   Afghanistan  2000  population    20595360\n4        Brazil  1999       cases       37737\n5        Brazil  1999  population   172006362\n6        Brazil  2000       cases       80488\n7        Brazil  2000  population   174504898\n8         China  1999       cases      212258\n9         China  1999  population  1272915272\n10        China  2000       cases      213766\n11        China  2000  population  1280428583\n\n\nSolution: pivot_table() followed by reset_index().\nPython\n\n(\ntable2.pivot_table(index=['country', 'year'], columns='type', values='count')\n  .reset_index()\n)\n\ntype      country  year     cases    population\n0     Afghanistan  1999     745.0  1.998707e+07\n1     Afghanistan  2000    2666.0  2.059536e+07\n2          Brazil  1999   37737.0  1.720064e+08\n3          Brazil  2000   80488.0  1.745049e+08\n4           China  1999  212258.0  1.272915e+09\n5           China  2000  213766.0  1.280429e+09\n\n\npivot_table() creates a table with an index attribute defined by the columns you pass to the index argument. The reset_index() converts that attribute to columns and changes the index attribute to a sequence [0, 1, ..., n-1].\nR equivalences\nR\n\nspread(table2, key = \"type\", value = \"count\")\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nR\n\npivot_wider(table2, id_cols = c(\"country\", \"year\"), \n            names_from = \"type\", \n            values_from = \"count\")\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nRDS visualization:\n \nExercise: Use pandas to spread the flowers1 data frame (available at https://data-science-master.github.io/lectures/data/tidy_exercise/flowers1.csv).\n\n\n\nSeparating\n\nSometimes we want to split a column based on a delimiter:\nR\n\ndata(\"table3\")\n\nPython\n\ntable3 = pd.DataFrame({'country': ['Afghanistan', 'Afghanistan', 'Brazil', \n                                   'Brazil', 'China', 'China'],\n                       'year': [1999, 2000, 1999, 2000, 1999, 2000],\n                       'rate': ['745/19987071', '2666/20595360', \n                                '37737/172006362', '80488/174504898', \n                                '212258/1272915272', '213766/1280428583']})\ntable3\n\n       country  year               rate\n0  Afghanistan  1999       745/19987071\n1  Afghanistan  2000      2666/20595360\n2       Brazil  1999    37737/172006362\n3       Brazil  2000    80488/174504898\n4        China  1999  212258/1272915272\n5        China  2000  213766/1280428583\n\n\nPython\n\ntable3[['cases', 'population']] = table3.rate.str.split(pat = '/', expand = True)\ntable3.drop('rate', axis=1)\n\n       country  year   cases  population\n0  Afghanistan  1999     745    19987071\n1  Afghanistan  2000    2666    20595360\n2       Brazil  1999   37737   172006362\n3       Brazil  2000   80488   174504898\n4        China  1999  212258  1272915272\n5        China  2000  213766  1280428583\n\n\nR equivalence\nR\n\nseparate(table3, col = \"rate\", sep = \"/\", into = c(\"cases\", \"population\"))\n\n# A tibble: 6 × 4\n  country      year cases  population\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     \n1 Afghanistan  1999 745    19987071  \n2 Afghanistan  2000 2666   20595360  \n3 Brazil       1999 37737  172006362 \n4 Brazil       2000 80488  174504898 \n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\nExercise: Use pandas to separate the flowers2 data frame (available at https://data-science-master.github.io/lectures/data/tidy_exercise/flowers2.csv).\n\n\n\nUniting\n\nSometimes we want to combine two columns of strings into one column.\nR\n\ndata(\"table5\")\n\nPython\n\ntable5 = pd.DataFrame({'country': ['Afghanistan', 'Afghanistan', 'Brazil',\n                                   'Brazil', 'China', 'China'], \n                       'century': ['19', '20', '19', '20', '19', '20'], \n                       'year': ['99', '00', '99', '00', '99', '00'], \n                       'rate': ['745/19987071', '2666/20595360', \n                                '37737/172006362', '80488/174504898', \n                                '212258/1272915272', '213766/1280428583']})\ntable5\n\n       country century year               rate\n0  Afghanistan      19   99       745/19987071\n1  Afghanistan      20   00      2666/20595360\n2       Brazil      19   99    37737/172006362\n3       Brazil      20   00    80488/174504898\n4        China      19   99  212258/1272915272\n5        China      20   00  213766/1280428583\n\n\nYou can use str.cat() to combine two columns.\nPython\n\n(\ntable5.assign(year = table5.century.str.cat(table5.year))\n  .drop('century', axis = 1)\n)\n\n       country  year               rate\n0  Afghanistan  1999       745/19987071\n1  Afghanistan  2000      2666/20595360\n2       Brazil  1999    37737/172006362\n3       Brazil  2000    80488/174504898\n4        China  1999  212258/1272915272\n5        China  2000  213766/1280428583\n\n\nR equivalence:\nR\n\nunite(table5, century, year, col = \"year\", sep = \"\")\n\n# A tibble: 6 × 3\n  country     year  rate             \n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;            \n1 Afghanistan 1999  745/19987071     \n2 Afghanistan 2000  2666/20595360    \n3 Brazil      1999  37737/172006362  \n4 Brazil      2000  80488/174504898  \n5 China       1999  212258/1272915272\n6 China       2000  213766/1280428583\n\n\nExercise: Use pandas to re-unite the data frame you separated from the flowers2 exercise. Use a comma for the separator.\n\n\n\nJoining\n\nWe will use these DataFrames for the examples below.\nPython\n\nxdf = pd.DataFrame({\"mykey\": np.array([1, 2, 3]), \n                    \"x\": np.array([\"x1\", \"x2\", \"x3\"])})\nydf = pd.DataFrame({\"mykey\": np.array([1, 2, 4]), \n                    \"y\": np.array([\"y1\", \"y2\", \"y3\"])})\nxdf\nydf\n\nR\n\nxdf &lt;- tibble(mykey = c(\"1\", \"2\", \"3\"),\n              x_val = c(\"x1\", \"x2\", \"x3\"))\nydf &lt;- tibble(mykey = c(\"1\", \"2\", \"4\"),\n              y_val = c(\"y1\", \"y2\", \"y3\"))\nxdf\nydf\n\nBinding rows is done with pd.concat().\nPython\n\npd.concat([xdf, ydf])\n\nR equivalence:\nR\n\nbind_rows(xdf, ydf)\n\nAll joins use pd.merge().\nInner Join (visualization from RDS):\n \nPython\n\npd.merge(left=xdf, right=ydf, how=\"inner\", on=\"mykey\")\n\nR\n\ninner_join(xdf, ydf, by = \"mykey\")\n\nOuter Joins (visualization from RDS):\n \nLeft Join\nPython\n\npd.merge(left=xdf, right=ydf, how=\"left\", on=\"mykey\")\n\nR\n\nleft_join(xdf, ydf, by = \"mykey\")\n\nRight Join\nPython\n\npd.merge(left=xdf, right=ydf, how=\"right\", on=\"mykey\")\n\nR\n\nright_join(xdf, ydf, by = \"mykey\")\n\nFull Join\nPython\n\npd.merge(left=xdf, right=ydf, how=\"outer\", on=\"mykey\")\n\nR\n\nfull_join(xdf, ydf, by = \"mykey\")\n\nUse the left_on and right_on arguments if the keys are named differently.\nThe on argument can take a list of key names if your key is multiple columns.\n\n\n\nExtra Resources\n\nI am not an expert in Python, and there is so much more to Python than what I am presenting here. Here are some resources if you want to learn more:\nPython Data Science Handbook\nPython for Data Analysis\nAnother Book on Data Science"
  },
  {
    "objectID": "06_python/06_python_setup.html",
    "href": "06_python/06_python_setup.html",
    "title": "Python and Reticulate",
    "section": "",
    "text": "Learning Objectives\n\nSet up R Studio environment for Python.\nPython REPL.\nPython chunks and Python scripts.\nGetting Started with Reticulate\nInstalling Python Packages\n\n\n\nSetting Up Python for R Studio\n\nMost Python pros use Jupyter Notebook as their IDE when developing in Python.\nBut for R programmers who only need to dabble in Python (me, for example), we can use R Studio and the reticulate package.\nR\n\ninstall.packages(\"reticulate\")\n\nYou might need to accept the conda terms of service to use conda. So run the following code to load reticulate.\nR\n\nSys.setenv(CONDA_PLUGINS_AUTO_ACCEPT_TOS = \"yes\")\nlibrary(reticulate)\n\nYou can also put this in your R environment (opened via usethis::edit_r_environ()). Then you don’t need to set the enviornment variable every time you load reticulate.\nCONDA_PLUGINS_AUTO_ACCEPT_TOS = \"yes\"\nTo install a version of Python that reticulate will use, run in the terminal:\nR\n\nreticulate::install_miniconda()\n\nTo validate that Python was installed and is available, run\nR\n\nreticulate::py_available()\n\nPython programmers typically modularize their projects into environments, which are separate installations of python and packages. That way, if you go back to a project, you go back to the exact same version of python and packages that you know work for that project. This reduces chances for breaking changes to affect old projects.\nUse conda_list() to list all of the environments.\nR\n\nreticulate::conda_list()\n\n          name\n1         base\n2 r-reticulate\n3    anaconda3\n4      msprime\n                                                               python\n1                   /home/dgerard/.local/share/r-miniconda/bin/python\n2 /home/dgerard/.local/share/r-miniconda/envs/r-reticulate/bin/python\n3                                  /home/dgerard/anaconda3/bin/python\n4                     /home/dgerard/anaconda3/envs/msprime/bin/python\n\n\nUse conda_create() to create a new environment. You can choose the version of python that this environment uses.\nR\n\nreticulate::conda_create(envname = \"objects\", python_version = \"3.13.3\")\n\nUse use_condaenv() to specify the environment you want to use.\nR\n\nreticulate::use_condaenv(\"objects\")\n\nTo install Python packages, use py_install(). You need to specify the environment to which the packages are installed. For example, we can install the numpy, pandas, and matplotlib packages via\nR\n\nreticulate::py_install(envname = \"objects\", packages = c(\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\"))\n\nNote: We used conda environments here, but there is a competing system of environments called “virtualenv” which some folks use.\nIf you want to update the python version of a specific environment, I’ve found the easiest way was to just remove it with conda_remove() and recreate it with the specified python version.\n\n\n\nPython REPL’s, Chunks, and Scripts\n\nTo start an IPython shell — similar to the R command promp — run the following in R:\nR\n\nreticulate::repl_python()\n\n \n“REPL” stands for “read–eval–print loop”, which is an interactive programming environment (like the R command prompt or the IPython shell).\nYou can exit the REPL by typing the following:\nPython\n\nexit\n\nYou can have Python chunks in R Markdown files by replacing the “r” at the beginning of the chunk by “python”:\n\n\n  ```{python}\n  # Code goes here\n  ```\n\n\nYou can access R objects in Python using the r object. That is, r.x will access, in Python, the x variable defined using R.\nR\n\nx &lt;- c(1, 4, 6, 2)\n\nPython\n\nr.x\n\n[1.0, 4.0, 6.0, 2.0]\n\n\nYou can access Python objects in R using the py object. That is, py$x will access, in R, the x variable defined using Pythong.\nPython\n\nx = [8, 9, 11, 3]\n\nR\n\npy$x\n\n[1]  8  9 11  3\n\n\nSometimes it’s buggy, but you can usually begin a Python REPL by also hitting Control/Command + Enter inside the Python chunk:\n \nPython scripts (where there is only Python code and no plain text) end in “.py”. You can create a Python script in R Studio:\n \nHitting Control/Command + Enter inside a Python script will also start a Python REPL."
  },
  {
    "objectID": "07_sql/07_dbeaver.html",
    "href": "07_sql/07_dbeaver.html",
    "title": "DBeaver IDE for SQL Queries",
    "section": "",
    "text": "Learning Objectives\n\nUsing the DBeaver IDE.\nDuckDB tutorial on DBeaver: https://duckdb.org/docs/guides/sql_editors/dbeaver\n\n\n\nMotivation\n\nNo job will have you use R Studio to do SQL.\nLet’s try out another IDE.\n\n\n\nSetup\n\nThe instructions here are only for DuckDB. But DBeaver works with other DBMS’s pretty easily.\nDownload and install DBeaver Community here: https://dbeaver.io/download/\nWhen you open it up, select “DuckDB”\n\n\n\n\n\n\n\n\n\nWith “Host” selected, navigate to your flights.duckdb database. Click “Finish”.\n\n\n\n\n\n\n\n\n\nClick on the database name “flights.duckdb” in the top right. If you haven’t used it before, it will ask you to install a driver to use connect to a DuckDB database.\n\n\n\n\n\n\n\n\n\nYou should now see the database. Right click on the connection and create a new SQL file.\n\n\n\n\n\n\n\n\n\nType some SQL and hit the execute button  or just hit Control/Command + Enter.\n\n\n\n\n\n\n\n\n\nYou can now play around with SQL. When you want to save the data to a CSV file, just hit “Export data”.\nThere are a lot of features in DBeaver, but we won’t cover them. They are pretty intuitive. Just play with it for awhile.\n\n\n\nExercises\nUse DBeaver for all of these exercises.\n\nWhat is the mean temperature for flights from each airport?\nWhat is the average number of flights from each airport per day in January?\nWhat are the top destinations for each airport?"
  },
  {
    "objectID": "15_spark/15_spark.html",
    "href": "15_spark/15_spark.html",
    "title": "Spark",
    "section": "",
    "text": "Learn some Spark\nMastering Spark in R"
  },
  {
    "objectID": "15_spark/15_spark.html#install",
    "href": "15_spark/15_spark.html#install",
    "title": "Spark",
    "section": "Install",
    "text": "Install\n\nMake sure you have java installed with (otherwise install it here).\n\nsystem(\"java -version\")\n\nInstall {sparklyr}\ninstall.packages(\"sparklyr\")\nInstall Spark\nsparklyr::spark_install()\nCheck that Spark is installed.\n\nsparklyr::spark_installed_versions()"
  },
  {
    "objectID": "15_spark/15_spark.html#connect",
    "href": "15_spark/15_spark.html#connect",
    "title": "Spark",
    "section": "Connect",
    "text": "Connect\n\nLoad {sparklyr} into R\n\nlibrary(sparklyr)\n\nConnect to Spark with spark_connect()\n\nsc &lt;- spark_connect(master = \"local\", version = \"3.3.1\")\n\nYou change the master argument to connect to clusters. The \"local\" value says to use the Spark that is installed on your computer.\nYou use sc to run Spark commands."
  },
  {
    "objectID": "15_spark/15_spark.html#disconnect",
    "href": "15_spark/15_spark.html#disconnect",
    "title": "Spark",
    "section": "Disconnect",
    "text": "Disconnect\n\nDisconnect using spark_disconnect().\n\nspark_disconnect(sc)"
  },
  {
    "objectID": "11_websites/11_websites_github.html",
    "href": "11_websites/11_websites_github.html",
    "title": "Create your own Website using GitHub Pages",
    "section": "",
    "text": "Create your own personal website using GitHub pages.\nSetting up a GitHub Pages site with Jekyll.\n\nIn particular, the pages on adding content and troubleshooting build errors.\n\nJekyll Themes"
  },
  {
    "objectID": "11_websites/11_websites_github.html#configuring-your-website",
    "href": "11_websites/11_websites_github.html#configuring-your-website",
    "title": "Create your own Website using GitHub Pages",
    "section": "Configuring your website",
    "text": "Configuring your website\n\nMost of the configuration settings are in “_config.yml”.\nMake sure you use spaces instead of tabs in side “_config.yml”.\nMake sure you use a space after : when setting key-value pairs.\nMost of these settings are self explanatory, and you can edit them to suit your needs. For example, here is the original “_config.yml” file:\n \nAnd here is the one I did after I edited it:\n \nMore info on Configurations: https://jekyllrb.com/docs/configuration/"
  },
  {
    "objectID": "11_websites/11_websites_github.html#adding-content-to-your-website",
    "href": "11_websites/11_websites_github.html#adding-content-to-your-website",
    "title": "Create your own Website using GitHub Pages",
    "section": "Adding content to your website",
    "text": "Adding content to your website\n\nThe main types of content of a Jekyll website are pages and posts.\n\nA page contains standalone content not associated with a particular date. For example, the home page, an “about” page, or a license.\n\nInformation on pages: https://jekyllrb.com/docs/pages/\n\nA post is a blog post, that is associated with a particular date.\n\nInformation on posts: https://jekyllrb.com/docs/posts/\n\n\n\n\nPages\n\nTo add a page, just include a markdown file (ends with “.md”).\nThis markdown file will be converted to an HTML file when uploaded to GitHub.\nEach markdown file has a YAML header (just like in R Markdown files) and what goes into this header depends on the theme you chose. Just look at a few examples from that theme.\n\nMore information on the YAML header: https://jekyllrb.com/docs/front-matter/\n\nYou can edit the homepage by editing “index.md”\n\n\n\nPosts\n\nAll posts go in the _posts directory. These are also markdown files (ends in “.md”). Your theme will have a default way of displaying posts on the homepage that I would not mess with until you get more experience.\nAll posts should be titled with the following format:\nYYYY-MM-DD-title.md\nThe theme will generally display links to these posts in reverse-chronological order.\nAll posts will again have a YAML header. Just look at a few examples from the theme for what should go there.\n\nMore information on the YAML header: https://jekyllrb.com/docs/front-matter/"
  },
  {
    "objectID": "11_websites/11_websites_github.html#troubleshooting-build-errors",
    "href": "11_websites/11_websites_github.html#troubleshooting-build-errors",
    "title": "Create your own Website using GitHub Pages",
    "section": "Troubleshooting Build Errors",
    "text": "Troubleshooting Build Errors\n\nSee here for helpful hints on fixing build errors.\nGitHub will send you an email if your website fails to build. Go to the above link and scroll through the types of errors to see hints on where to look for the error in your source."
  },
  {
    "objectID": "11_websites/11_websites_github.html#my-website",
    "href": "11_websites/11_websites_github.html#my-website",
    "title": "Create your own Website using GitHub Pages",
    "section": "My Website",
    "text": "My Website\n\nAfter playing around with the source, this is what I came up with:\n\nhttps://github.com/dcgerard/no-style-please\nhttps://dcgerard.github.io/no-style-please/\n\nWhich I think looks pretty good!"
  },
  {
    "objectID": "14_dimension_reduction/14_dimension_reduction.html",
    "href": "14_dimension_reduction/14_dimension_reduction.html",
    "title": "Dimension Reduction",
    "section": "",
    "text": "Learning Objectives\n\nGoals of dimension reduction.\nUses of dimension reduction.\nPerils of dimension reduction.\nMost common dimension reduction techniques.\nDimension Reduction with R\n\n\n\nMotivation"
  },
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "TODO",
    "section": "",
    "text": "TODO\n\nModify git notes to:\n\nInitiate repos on GitHub rather than locally. This seems more intuitive for students than linking the repos manually.\nRemove all instances of forking. Just getting them to clone and push I think is enough.\n\nI decided that initiating on local is better for generalizability (not every project is on GitHub). Instead of removing forking, I added branching. This will make group projects run smoother."
  },
  {
    "objectID": "A2_linear_regression/A2_many_models.html",
    "href": "A2_linear_regression/A2_many_models.html",
    "title": "Many Models",
    "section": "",
    "text": "Learning Objectives\n\nUse a list-column to run many models at once.\nChapter 25 from RDS.\n\n\n\nMotivation\n\nThe gapminder data\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(gapminder)\ndata(\"gapminder\")\nglimpse(gapminder)\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.80, 30.33, 32.00, 34.02, 36.09, 38.44, 39.85, 40.82, 41.6…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4, 820.9, 853.1, 836.2, 740.0, 786.1, 978.0, 852.4, 649.…\n\n\nSuppose we want to look at how life expectancy has changed over time in each country:\n\ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp, group = country)) +\n  geom_line(alpha = 1/3) +\n  xlab(\"Year\") +\n  ylab(\"Life Expectancy\")\n\n\n\n\n\n\n\n\nGeneral trend is going up. But there are some countries where this doesn’t happen. How do we quantify the trend? We can do this with one country using a linear model:\n\ngapminder %&gt;%\n  filter(country == \"United States\") -&gt;\n  usdf\n\nggplot(usdf, aes(x = year, y = lifeExp)) +\n  geom_line() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_line(alpha = 1/3) +\n  xlab(\"Year\") +\n  ylab(\"Life Expectancy\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nus_lmout &lt;- lm(lifeExp ~ year, data = usdf)\ntidy_uslm &lt;- tidy(us_lmout)\ntidy_uslm\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) -291.     13.8         -21.1 1.25e- 9\n2 year           0.184   0.00696      26.5 1.37e-10\n\n\nSo each year, the US has been increasing its life expectancy by 0.1842 years.\nHow can we get these coefficient estimates for each country?\nFor the exercises in this lesson plan, you should log the pop variable now:\n\ngapminder %&gt;%\n  mutate(logpop = log2(pop)) -&gt;\n  gapminder\n\nExercise: Create a line plot for year vs log-population for each country.\nExercise: For China, fit a linear model for log-population on year. Make sure the assumptions of the linear model are fulfilled. Interpret the coefficients\n\n\n\nNest\n\nThe nest() function from the tidyr package will turn a grouped data frame into a data frame where each group is a row. All of the observations are placed in the data variable as a data frame. Let’s look at an example.\n\ngapminder %&gt;%\n  group_by(country, continent) %&gt;%\n  nest() -&gt;\n  nested_gap_df\n\nnested_gap_df\n\n# A tibble: 142 × 3\n# Groups:   country, continent [142]\n   country     continent data             \n   &lt;fct&gt;       &lt;fct&gt;     &lt;list&gt;           \n 1 Afghanistan Asia      &lt;tibble [12 × 5]&gt;\n 2 Albania     Europe    &lt;tibble [12 × 5]&gt;\n 3 Algeria     Africa    &lt;tibble [12 × 5]&gt;\n 4 Angola      Africa    &lt;tibble [12 × 5]&gt;\n 5 Argentina   Americas  &lt;tibble [12 × 5]&gt;\n 6 Australia   Oceania   &lt;tibble [12 × 5]&gt;\n 7 Austria     Europe    &lt;tibble [12 × 5]&gt;\n 8 Bahrain     Asia      &lt;tibble [12 × 5]&gt;\n 9 Bangladesh  Asia      &lt;tibble [12 × 5]&gt;\n10 Belgium     Europe    &lt;tibble [12 × 5]&gt;\n# ℹ 132 more rows\n\n\nThe data variable is a list. Each element is a data frame that contains all of the observations in the group we created by country.\nObservations from Afghanistan\n\nnested_gap_df$data[[1]]\n\n# A tibble: 12 × 5\n    year lifeExp      pop gdpPercap logpop\n   &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1  1952    28.8  8425333      779.   23.0\n 2  1957    30.3  9240934      821.   23.1\n 3  1962    32.0 10267083      853.   23.3\n 4  1967    34.0 11537966      836.   23.5\n 5  1972    36.1 13079460      740.   23.6\n 6  1977    38.4 14880372      786.   23.8\n 7  1982    39.9 12881816      978.   23.6\n 8  1987    40.8 13867957      852.   23.7\n 9  1992    41.7 16317921      649.   24.0\n10  1997    41.8 22227415      635.   24.4\n11  2002    42.1 25268405      727.   24.6\n12  2007    43.8 31889923      975.   24.9\n\n\nObservations from Albania\n\nnested_gap_df$data[[2]]\n\n# A tibble: 12 × 5\n    year lifeExp     pop gdpPercap logpop\n   &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n 1  1952    55.2 1282697     1601.   20.3\n 2  1957    59.3 1476505     1942.   20.5\n 3  1962    64.8 1728137     2313.   20.7\n 4  1967    66.2 1984060     2760.   20.9\n 5  1972    67.7 2263554     3313.   21.1\n 6  1977    68.9 2509048     3533.   21.3\n 7  1982    70.4 2780097     3631.   21.4\n 8  1987    72   3075321     3739.   21.6\n 9  1992    71.6 3326498     2497.   21.7\n10  1997    73.0 3428038     3193.   21.7\n11  2002    75.7 3508512     4604.   21.7\n12  2007    76.4 3600523     5937.   21.8\n\n\nNotice that I used double brackets to extract the elements of data because it is a list.\nExercise: Extract the United States data frame from nested_gap_df. This code should be general so that the location of the US in the data frame does not matter.\n\n\n\nFit a model\n\nWe can now use purrr to fit a model on all of the elements of data.\n\nnested_gap_df %&gt;%\n  mutate(lmout = map(data, ~lm(lifeExp ~ year, data = .))) -&gt;\n  nested_gap_df\n\nRecall: the data = . argument says that we should place all of the elements from the data column in the nested_gap_df data frame as the data argument in lm().\nLet’s look at the results\n\nnested_gap_df\n\n# A tibble: 142 × 4\n# Groups:   country, continent [142]\n   country     continent data              lmout \n   &lt;fct&gt;       &lt;fct&gt;     &lt;list&gt;            &lt;list&gt;\n 1 Afghanistan Asia      &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 2 Albania     Europe    &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 3 Algeria     Africa    &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 4 Angola      Africa    &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 5 Argentina   Americas  &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 6 Australia   Oceania   &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 7 Austria     Europe    &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 8 Bahrain     Asia      &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n 9 Bangladesh  Asia      &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n10 Belgium     Europe    &lt;tibble [12 × 5]&gt; &lt;lm&gt;  \n# ℹ 132 more rows\n\n\nEach element of the lmout column is the model output from fitting lm() to each data frame in the data column.\n\n## Afghanistan fit\nnested_gap_df$lmout[[1]]\n\n\nCall:\nlm(formula = lifeExp ~ year, data = .)\n\nCoefficients:\n(Intercept)         year  \n   -507.534        0.275  \n\n## Albania fit\nnested_gap_df$lmout[[2]]\n\n\nCall:\nlm(formula = lifeExp ~ year, data = .)\n\nCoefficients:\n(Intercept)         year  \n   -594.073        0.335  \n\n\nExercise: For each country, fit a linear model of log-population on year. Save this fit as the lmpop column in nested_gap_df.\n\n\n\nGet model summaries\n\nAlso use map to create columns of model summaries using tidy(), augment(), or glance().\n\nnested_gap_df %&gt;%\n  mutate(tidyout = map(lmout, ~tidy(., conf.int = TRUE)),\n         augmentout = map(lmout, augment),\n         glanceout = map(lmout, glance)) -&gt;\n  nested_gap_df\n\nWe now have model summaries within all groups\n\n## Afghanistan summary\nnested_gap_df$tidyout[[1]]\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic      p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -508.      40.5        -12.5 0.000000193  -598.     -417.   \n2 year           0.275    0.0205      13.5 0.0000000984    0.230     0.321\n\n## Albania summary\nnested_gap_df$tidyout[[2]]\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic    p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) -594.      65.7        -9.05 0.00000394 -740.     -448.   \n2 year           0.335    0.0332     10.1  0.00000146    0.261     0.409\n\n\nExercise: Use tidy() to get model summaries of your linear model fits of log-population on year. Add these summaries as another column in nested_gap_df.\n\n\n\nUnnest\n\nUse the unnest() function, followed by the ungroup() function, to unnest a single list column.\n\nunnest(nested_gap_df, tidyout) %&gt;%\n  ungroup() -&gt;\n  model_summary_df\n\nmodel_summary_df\n\n# A tibble: 284 × 13\n   country  continent data     lmout term  estimate std.error statistic  p.value\n   &lt;fct&gt;    &lt;fct&gt;     &lt;list&gt;   &lt;lis&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Afghani… Asia      &lt;tibble&gt; &lt;lm&gt;  (Int… -5.08e+2  40.5        -12.5  1.93e- 7\n 2 Afghani… Asia      &lt;tibble&gt; &lt;lm&gt;  year   2.75e-1   0.0205      13.5  9.84e- 8\n 3 Albania  Europe    &lt;tibble&gt; &lt;lm&gt;  (Int… -5.94e+2  65.7         -9.05 3.94e- 6\n 4 Albania  Europe    &lt;tibble&gt; &lt;lm&gt;  year   3.35e-1   0.0332      10.1  1.46e- 6\n 5 Algeria  Africa    &lt;tibble&gt; &lt;lm&gt;  (Int… -1.07e+3  43.8        -24.4  3.07e-10\n 6 Algeria  Africa    &lt;tibble&gt; &lt;lm&gt;  year   5.69e-1   0.0221      25.7  1.81e-10\n 7 Angola   Africa    &lt;tibble&gt; &lt;lm&gt;  (Int… -3.77e+2  46.6         -8.08 1.08e- 5\n 8 Angola   Africa    &lt;tibble&gt; &lt;lm&gt;  year   2.09e-1   0.0235       8.90 4.59e- 6\n 9 Argenti… Americas  &lt;tibble&gt; &lt;lm&gt;  (Int… -3.90e+2   9.68       -40.3  2.14e-12\n10 Argenti… Americas  &lt;tibble&gt; &lt;lm&gt;  year   2.32e-1   0.00489     47.4  4.22e-13\n# ℹ 274 more rows\n# ℹ 4 more variables: conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, augmentout &lt;list&gt;,\n#   glanceout &lt;list&gt;\n\n\nYou do this so you can plot some cool things.\n\nlibrary(ggthemes)\nmodel_summary_df %&gt;%\n  filter(term == \"year\") %&gt;%\n  mutate(country = fct_reorder(country, estimate)) %&gt;%\n  ggplot(aes(x = country, y = estimate, color = continent)) +\n  geom_point() +\n  geom_segment(aes(x = country, xend = country, y = conf.low, yend = conf.high), \n               color = \"black\", \n               alpha = 1/3) +\n  ylab(\"Estimate of Rate of Life Expectancy Increase\") +\n  xlab(\"Country\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()) +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\nYou can check all of the models’ qualities by looking at the residuals\n\nunnest(nested_gap_df, augmentout) -&gt;\n  augment_df\n\naugment_df %&gt;%\n  ggplot(aes(x = year, y = .resid, group = country)) +\n  geom_line(alpha = 1/3) +\n  facet_wrap(.~continent) +\n  geom_hline(yintercept = 0, lty = 2, col = \"blue\") +\n  xlab(\"Residuals\") +\n  ylab(\"Year\")\n\n\n\n\n\n\n\n\nAbove, we see there is a lot of curvature we are not accounting for.\nExercise: Unnest the tidy() output from the linear model fit of log-population on year. Which countries have seen the greatest increase in population? Have any seen a decline?\n\n\n\nmtcars exercise\nRecall how we used the split() function in the iterators worksheet on the mtcars dataset to get the regression coefficient estimates of a regression of mpg on wt.\n\ndata(\"mtcars\")\nmtcars %&gt;% \n  split(.$cyl) %&gt;% \n  map(~lm(mpg ~ wt, data = .)) %&gt;%\n  map(~tidy(.)) %&gt;%\n  map_dbl(~.$estimate[2])\n\n     4      6      8 \n-5.647 -2.780 -2.192 \n\n\nRedo this exercise using the nest()-map()-unnest() workflow we just went through."
  },
  {
    "objectID": "10_maps/10_maps.html",
    "href": "10_maps/10_maps.html",
    "title": "Simple Features and Graphing Maps",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand the concept and utility of spatial data visualization in R.\nGet acquainted with the basic functions and data types in the {sf} package.\nCreate simple maps using the {sf} package.\nSimple Features\n\nhttps://r-spatial.github.io/sf/articles/\n\nMap Plotting\n\nhttps://ggplot2.tidyverse.org/reference/ggsf.html\n\n\n\n\nMotivation\n\nFor some reason, people are really impressed with data visualizations that include maps (e.g. of D.C., the U.S., the Earth, etc).\nLet’s go over how to build simple map visualizations in R.\nFor this lecture, we will try to visualize the 2020 presidential election elections from (https://github.com/TheUpshot/presidential-precinct-map-2020).\n\n\n\nIntroduction to Simple Features in R\n\nThe sf package, implements the “simple features” standard, making it easier to manipulate geographical data in R.\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "08_make/08_make.html",
    "href": "08_make/08_make.html",
    "title": "Managing Workflows with GNU Make",
    "section": "",
    "text": "Automated workflow with GNU make.\nRequired Reading:\n\nMinimal Make\nMakefile Wiki Page\n\nAdditional Resources\n\nOfficial Manual\nManaging Projects with GNU Make\nUsing GNU Make to Manage the Workflow of Data Analysis Projects\nStat 545 Chapter on Make"
  },
  {
    "objectID": "08_make/08_make.html#use-rtools-light",
    "href": "08_make/08_make.html#use-rtools-light",
    "title": "Managing Workflows with GNU Make",
    "section": "Use Rtools (light)",
    "text": "Use Rtools (light)\n\nDownload and install Rtools: https://cran.r-project.org/bin/windows/Rtools/\nRestart R.\nChange R Studio’s settings “Global Options… &gt; Terminal &gt; New terminals open with” so that it reads “Windows Powershell”. This will make it so that the terminal is Windows PowerShell, not git bash (so it will look a little weird and act a little differently).\nOpen a new terminal. Again, this is a Windows PowerShell.\nAlways run make in the Windows PowerShell from R Studio. This pipeline won’t work if you try to directly open the PowerShell outside of R Studio.\nYou can always convert the terminal back to git bash later.\nTo verify make is installed, run make --version in the Windows PowerShell.\nI am not a big fan of RStudio Projects, but if you like them then there is some R Studio make functionality when you use an RStudio Project (https://stat545.com/make-test-drive.html). This lets you avoid using Windows PowerShell."
  },
  {
    "objectID": "08_make/08_make.html#windows-subsystem-for-linux-heavy",
    "href": "08_make/08_make.html#windows-subsystem-for-linux-heavy",
    "title": "Managing Workflows with GNU Make",
    "section": "Windows Subsystem for Linux (heavy)",
    "text": "Windows Subsystem for Linux (heavy)\n\nOnly start this if you are willing to spend a few hours fiddling around your computer.\nYou can install Ubuntu on Windows to use all of its powerful features. This is what you would need to do for more advanced computational operations. This is how I use Windows.\nBut on Ubuntu, you’ll need to install a separate version of R, all of the R packages that you usually use, and git.\nNOTE: This strategy could take up to a gigabyte of storage.\nInstall Ubuntu using these directions: https://ubuntu.com/wsl\nOpen up Ubuntu (this will be a shell).\nFollow the instructions on CRAN (https://cran.r-project.org/) to install R for Ubuntu inside the Ubuntu shell.\nIn Ubuntu open up the command line for R\nR\nIn the R command prompt, install all of the R packages you will need with install.packages(). You can exit R afterwards with q().\nChange R Studio’s settings “Global Options… &gt; Terminal &gt; New terminals open with” so that it reads “Bash (Windows Subsystem for Linux)”.\nOpen up a new terminal, and you should now be using Ubuntu for your terminal.\nTo verify make is installed, run make --version in the Ubuntu terminal."
  },
  {
    "objectID": "08_make/08_make.html#rules",
    "href": "08_make/08_make.html#rules",
    "title": "Managing Workflows with GNU Make",
    "section": "Rules",
    "text": "Rules\n\nInside the Makefile, you prepare a series of rules of the form\ntarget: prereq_1 prereq_2 prereq_3 ...\n  first bash command to make target\n  second bash command and so on\nEach rule contains three things: a target, prerequisites, and commands.\ntarget is the name of the file that will be generated.\nprereq_1, prereq_2, prereq_3, etc are the names of the files which are used to generate target. These can be datasets, R scripts, etc.\nEach subsequent line is a bash command that will be evaluated in the terminal in the order listed. This sequence of commands is sometimes called a recipe.\n\nIMPORTANT: Make sure each bash command has one tab (not spaces) at the start of the line. If you copy and paste a Makefile from a web site then usually tabs are converted to spaces and produce an error!\nFrom the terminal, it is possible to evaluate R scripts, python scripts, and knit R Markdown files.\nYou can also use the usual bash commands you are used do (touch, cp, mv, rm, etc…)\nThere are tons of other commands that you can install that allow you to do things like download files (curl and wget), unzip files (7z and tar), convert image files, compile LaTeX documents, etc…"
  },
  {
    "objectID": "08_make/08_make.html#useful-bash-commands-for-data-science",
    "href": "08_make/08_make.html#useful-bash-commands-for-data-science",
    "title": "Managing Workflows with GNU Make",
    "section": "Useful bash commands for data science",
    "text": "Useful bash commands for data science\n\nRun an R script\n\nR CMD BATCH --no-save --no-restore input_file.R output_file.Rout\n\nMake sure to change “input_file.R” and “output_file.Rout”\n\nThe --no-save --no-restore options make sure that you are working with a clean environment and that you don’t save this environment after the command is executed. This is a good thing for reproducibility.\n\nRender a Quarto  Document\n\nquarto render quarto_file.qmd\n\nUse Quarto  to Render an Jupyter Notebook (.ipynb Document):\n\nquarto render notebook.ipynb --execute\n\nKnit an R Markdown file\n\nRscript -e \"library(rmarkdown);render('rmarkdown_file.Rmd')\"\n\nMake sure to change “rmarkdown_file.Rmd”\nRun python script\n\npython3 input_file.py\n\nMake sure to change “input_file.py”\nDownload data from the web\n\nwget --no-clobber url_to_data"
  },
  {
    "objectID": "08_make/08_make.html#phony-targets",
    "href": "08_make/08_make.html#phony-targets",
    "title": "Managing Workflows with GNU Make",
    "section": "Phony Targets",
    "text": "Phony Targets\n\nIf you have multiple, related, final outputs, it is common to place these as prerequisites to “phony” targets:\n.PHONY : phony_target\nphony_target : target1 target2 target3 ...\nwhere “phony_target” is a name you provide to represent the operation being performed.\n“Phony” targets are not real files. They are just convenient names to use to describe a collection of targets that should be generated.\nAt the top of the makefile, you then list the phony targets after all:\n.PHONY : all\nall : phony_target1 phony_target2 phony_target3 ...\nNOTE: It is important to have all : phony_target1 phony_target2 as the very first rule because by default make will only evaluate the very first rule in the file. So if all is first, then make will evaluate all targets."
  },
  {
    "objectID": "08_make/08_make.html#comments",
    "href": "08_make/08_make.html#comments",
    "title": "Managing Workflows with GNU Make",
    "section": "Comments",
    "text": "Comments\n\nUse a hashtag # for comments in a makefile."
  },
  {
    "objectID": "08_make/08_make.html#pseudo-code-for-makefile",
    "href": "08_make/08_make.html#pseudo-code-for-makefile",
    "title": "Managing Workflows with GNU Make",
    "section": "Pseudo-code for Makefile",
    "text": "Pseudo-code for Makefile\n.PHONY : all\nall : phony_target1 phony_target2\n\n.PHONY : phony_target1\nphony_target1 : target1 target2\n\n.PHONY : phony_target2\nphony_target2 : target3\n\n# Comment 1\ntarget1 : prereq1.R data0.csv data1.RDS\n  R CMD BATCH --no-save --no-restore prereq1.R prereq1_out.Rout\n\n# Comment 2\ntarget2 : prereq2.py data2.csv\n  python prereq2.py\n\n# Comment 3\ntarget3 : prereq3.Rmd\n  Rscript -e \"library(rmarkdown);render('prereq3.Rmd')\""
  },
  {
    "objectID": "08_make/08_make.html#evaluate-a-makefile",
    "href": "08_make/08_make.html#evaluate-a-makefile",
    "title": "Managing Workflows with GNU Make",
    "section": "Evaluate a makefile",
    "text": "Evaluate a makefile\n\nYou can generate all target files by running the following in the terminal\nmake\nYou can run just the targets in a phony target by specifying the phony target\nmake phony_target\nMake will check if any of the prerequisites have changes for each target and, if so, will re-run the bash commands of that rule.\nMake will not re-run commands if the prerequisites have not changed. That is, if no upstream files to target were modified, then target will not be re-generated. This makes make very efficient."
  },
  {
    "objectID": "08_make/08_make.html#working-directory-considerations",
    "href": "08_make/08_make.html#working-directory-considerations",
    "title": "Managing Workflows with GNU Make",
    "section": "Working directory considerations",
    "text": "Working directory considerations\n\ntl;dr\n\nFor R and the terminal: Assume the working directory is the location of the Makefile.\nFor R Markdown: Assume the working directory is the location of the Rmd file.\n\nSo if your file structure is\nMakefile\nanalysis/script.R\nanalysis/report.Rmd\ndata/data.csv\nThen you need specify your targets according to this structure\ndata/data.csv : analysis/script.R\n  R CMD BATCH --no-save --no-restore analysis/script.R\nNOTE that the following will not work because each command is executed in its own subshell (assuming the working directory is the location of the Makefile):\n# DOES NOT WORK\ndata/data.csv : analysis/script.R\n  cd analysis \n  R CMD BATCH --no-save --no-restore script.R\nBut you can get around this by putting these commands on one line, with each command separated by a semicolon:\n# Works, but not recommended\ndata/data.csv : analysis/script.R\n  cd analysis; R CMD BATCH --no-save --no-restore script.R\nAny file manipulation in “script.R” needs to be done assuming the working directory is where Makefile is (notice the single dot):\n\nlibrary(readr)\ndat &lt;- read_csv(\"./data/data.csv\")\n\nHowever, confusingly, when you render an R Markdown file using knitr, you need to assume the working directory is the location of the R Markdown file, not the location of Makefile. So in “report.Rmd” you would write (notice the double dots)\n\nlibrary(readr)\ndat &lt;- read_csv(\"../data/data.csv\")"
  },
  {
    "objectID": "08_make/08_make.html#automatic-variables",
    "href": "08_make/08_make.html#automatic-variables",
    "title": "Managing Workflows with GNU Make",
    "section": "Automatic Variables",
    "text": "Automatic Variables\n\nThere are a lot of automatic variables that you can use to make your Makefile more concise.\nHere are the ones I use:\n\n$@: The target of the rule.\n$&lt;: The first prerequisite.\n$^: All of the prerequisites, with spaces between them.\n$(@D): The directory part of the target, with the trailing slash removed.\n$(@F): The file part of the target\n$(&lt;D): The directory part of the first prerequisite.\n$(&lt;F): The file part of the first prerequisite.\n$(basename names): Extracts all but the suffix of each file name in names.\n\nExample: Suppose I have the following rule:\noutput/figs/foo.pdf : analysis/scripts/gaa.R data/hii.csv\n  R CMD BATCH --no-save --no-restore analysis/scripts/gaa.R\nThen the following are these automatic variable values:\n\n$@: output/figs/foo.pdf\n$&lt;: analysis/scripts/gaa.R\n$^: analysis/scripts/gaa.R data/hii.csv\n$(@D): output/figs\n$(@F): foo.pdf\n$(&lt;D): analysis/scripts\n$(&lt;F): gaa.R\n$(basename $(&lt;F)): gaa\n\nFor example, if you always place the R script first that generates the target, and the datasets that the R script uses second, then the following is typically how I evaluate the R script.\nrexec = R CMD BATCH --no-save --no-restore\nfigure.pdf : script.R data1.csv data2.csv\n  $(rexec) $&lt; $(basename $(&lt;F)).Rout\nThe variables and automatic variables would interpret this as\nfigure.pdf : script.R data1.csv data2.csv\n  R CMD BATCH --no-save --no-restore script.R script.Rout\nExercise: Re-write the following to use an automatic variable instead of the Rmd’s file name in the recipe.\nreport.html : report.Rmd\n  Rscript -e \"library(rmarkdown);render('report.Rmd')\""
  },
  {
    "objectID": "09_shiny/09_layouts.html",
    "href": "09_shiny/09_layouts.html",
    "title": "Modifying Layouts and Aesthetics",
    "section": "",
    "text": "Learn the basics layouts.\nApplication Layout Guide.\nShiny Cheatsheet.\nOptional Resources\n\nShiny Tutorial.\nShiny Video, Part 3.\nShiny Examples."
  },
  {
    "objectID": "09_shiny/09_layouts.html#title-panel",
    "href": "09_shiny/09_layouts.html#title-panel",
    "title": "Modifying Layouts and Aesthetics",
    "section": "Title Panel",
    "text": "Title Panel\n\nAdd a title to your app using the titlePanel() function.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n    titlePanel(\"My First Title\")\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:"
  },
  {
    "objectID": "09_shiny/09_layouts.html#sidebar-layout",
    "href": "09_shiny/09_layouts.html#sidebar-layout",
    "title": "Modifying Layouts and Aesthetics",
    "section": "Sidebar Layout",
    "text": "Sidebar Layout\n\nThe most basic layout is to have inputs in a left column and outputs in a right column. You use sidebarLayout() to specify that you want this structure.\nInside sidebarLayout(), you define the left column inputs via sidebarPanel() and the right column outputs via mainPanel().\nLet’s create an app that plots random normal draws:\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  titlePanel(\"Random Normal Histogram\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"nobs\", \"Number of Observations\", min = 1, max = 500, value = 100)\n    ),\n    mainPanel(\n      plotOutput(\"hist\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$hist &lt;- renderPlot({\n    rout &lt;- data.frame(x = rnorm(n = input$nobs))\n    ggplot(rout, aes(x = x)) +\n      geom_histogram(bins = 30) +\n      theme_bw()\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n \nsidebarPanel():\n\nAn argument of sidebarLayout().\nTakes as input an input element (such as sliderInput(), textInput(), etc).\nYou can include multiple input elements by separating them by a comma.\n\nmainPanel():\n\nAn argument of sidebarLayout().\nTakes as input an output element (such as plotOutput(), textOutput(), etc).\nYou can include multiple output elements by separating them by a comma.\n\nHadley’s Graphic:\n \nIf you put input elements in mainPanel() and output elements in sidebarPanel(), then the app won’t die (but it won’t look as nice).\nExercise: Create a Shiny app with the sidebar layout. The inputs should be the number of bins, the plot title, and which variable to plot from mtcars. The output should be a histogram. Add a nice shiny app title.\nYour final Shiny App should look like this:"
  },
  {
    "objectID": "09_shiny/09_layouts.html#grid-layout",
    "href": "09_shiny/09_layouts.html#grid-layout",
    "title": "Modifying Layouts and Aesthetics",
    "section": "Grid Layout",
    "text": "Grid Layout\n\nTo create a general layout, use the fluidRow() and column() functions.\nfluidRow()\n\nCreates a new row of panels.\nTakes column() calls as input.\nYou place as many column() calls as you want columns.\nIt can have a title.\n\ncolumn()\n\nUsed as an argument in fluidRow().\nThe first argument should be a number between 1 and 12 (the width).\nAll column() calls should have widths that sum to 12.\nThe rest of the arguments are input/output elements to include in that column.\n\nHadley’s Graphic:\n \nLet’s make a shiny app with two input columns in one row, and three plot columns in a second row.\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  fluidRow(title = \"Inputs\",\n           column(6,\n                  selectInput(\"var1\", \"Variable 1\", choices = names(mtcars)),\n                  selectInput(\"var2\", \"Variable 2\", choices = names(mtcars))\n           ),\n           column(6,\n                  sliderInput(\"bins\", \"Number of Bins\", min = 1, max = 50, value = 20)\n           )\n  ),\n  fluidRow(title = \"Outputs\",\n           column(4,\n                  plotOutput(\"plot1\")\n           ),\n           column(4,\n                  plotOutput(\"plot2\")\n           ),\n           column(4,\n                  plotOutput(\"plot3\")\n           )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$plot1 &lt;- renderPlot({\n    ggplot(mtcars, aes(x = .data[[input$var1]], y = .data[[input$var2]])) +\n      geom_point()\n  })\n\n  output$plot2 &lt;- renderPlot({\n    ggplot(mtcars, aes(x = .data[[input$var1]])) +\n      geom_histogram(bins = input$bins)\n  })\n\n  output$plot3 &lt;- renderPlot({\n    ggplot(mtcars, aes(x = .data[[input$var2]])) +\n      geom_histogram(bins = input$bins)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n \nNote: You can nest fluidRow()’s inside fluidRow()’s. It can get quite complicated.\nExercise: Create a grid layout of four squares where the top left square takes as input the variables of the palmerpenguins::penguins dataset to include in a scatterplot and the bottom right contains the resulting scatterplot, color-coded by species. The top right square and bottom left squares should remain empty.\nYour final app should look like this:"
  },
  {
    "objectID": "09_shiny/09_layouts.html#tabsets",
    "href": "09_shiny/09_layouts.html#tabsets",
    "title": "Modifying Layouts and Aesthetics",
    "section": "Tabsets",
    "text": "Tabsets\n\nYou can have outputs subdivided by tabs with tabsetPanel() and tabPanel().\ntabsetPanel()\n\nTakes as input tabPanel() calls.\nYou place it as an argument in either mainPanel() in the sidebar layout, or in one of the column() calls in the grid layout.\n\ntabPanel()\n\nTakes as input different input/output elements, separated by a comma. Each element will get its own tab.\nNeeds to be placed in tabsetPanel().\n\nHere is an example from the mtcars dataset, where the tabs have different plots for the variables we select.\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  sidebarLayout(\n    sidebarPanel(\n      selectInput(\"var1\", \"Variable 1\", choices = names(mtcars)),\n      selectInput(\"var2\", \"Variable 2\", choices = names(mtcars)),\n      sliderInput(\"bins\", \"Number of Bins\", min = 1, max = 50, value = 20)\n    ),\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Scatterplot\",\n                 plotOutput(\"plot1\")\n        ),\n        tabPanel(\"Histogram of Variable 1\",\n                 plotOutput(\"plot2\")\n        ),\n        tabPanel(\"Histogram of Variable 2\",\n                 plotOutput(\"plot3\")\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$plot1 &lt;- renderPlot({\n    ggplot(mtcars, aes(x = .data[[input$var1]], y = .data[[input$var2]])) +\n      geom_point()\n  })\n\n  output$plot2 &lt;- renderPlot({\n    ggplot(mtcars, aes(x = .data[[input$var1]])) +\n      geom_histogram(bins = input$bins)\n  })\n\n  output$plot3 &lt;- renderPlot({\n    ggplot(mtcars, aes(x = .data[[input$var2]])) +\n      geom_histogram(bins = input$bins)\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n \nExercise: Create a basic Shiny app that has a tab for a density plot, a histogram, and a boxplot for a variable from the palmerpenguins::penguins dataset. The user should get to choose the variable plotted, the the number of bins for the histogram, and the bandwidth for the density plot (see the help page of geom_smooth()). A good default value for the bandwidth might be 0.25 in this case.\nYour app should look like this:"
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html",
    "href": "09_shiny/09_basic_shiny.html",
    "title": "The Basics of Shiny Apps",
    "section": "",
    "text": "Learn the basics of Shiny Apps.\nChapters 1 through 3 of Mastering Shiny.\n\nMost content discretely stolen from Hadley’s book.\n\nShiny Cheatsheet.\nOptional Resources\n\nShiny Tutorial.\nShiny Video, Part 1\nShiny Examples."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#text-inputs",
    "href": "09_shiny/09_basic_shiny.html#text-inputs",
    "title": "The Basics of Shiny Apps",
    "section": "Text Inputs",
    "text": "Text Inputs\n\nUse textInput() to collect one line of text.\nUse passwordInput() to collect one line of text which is not displayed on the screen as it is entered.\n\n\nNOTE: This is not a secure way to collect passwords by itself.\n\nUsetextAreaInput() to collect multiple lines of text.\nAdd this element to your tryouts app:\n\nlibrary(shiny)    \n\nui &lt;- fluidPage(\n  textInput(\"name\", \"What's your name?\"),\n  passwordInput(\"password\", \"What's your password?\"),\n  textAreaInput(\"story\", \"Tell me about yourself\")\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nLater on, we’ll demonstrate how to access these inputs in the server() function.\nExercise: Change the width and height arguments in textAreaInput(). What does it do?\nExercise: Change the value argument in textInput(). What does it do?"
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#numeric-inputs",
    "href": "09_shiny/09_basic_shiny.html#numeric-inputs",
    "title": "The Basics of Shiny Apps",
    "section": "Numeric Inputs",
    "text": "Numeric Inputs\n\nUse numericInput() to create a text box that only accepts numeric values.\nUse sliderInput() to create a number slider.\n\nGiving the value argument one number will result in a one-sided slider.\nGiving the value argument a vector of two numbers will result in a two-sided slider.\n\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  numericInput(\"num\", \"Number one\", value = 0, min = 0, max = 100),\n  sliderInput(\"num2\", \"Number two\", value = 50, min = 0, max = 100),\n  sliderInput(\"rng\", \"Range\", value = c(10, 20), min = 0, max = 100)\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nOnly use sliders for small ranges where the exact number is not important.\nExercise: What does the animate option do when you set it to TRUE?\nYou can see more on sliders at https://shiny.rstudio.com/articles/sliders.html."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#date-inputs",
    "href": "09_shiny/09_basic_shiny.html#date-inputs",
    "title": "The Basics of Shiny Apps",
    "section": "Date Inputs",
    "text": "Date Inputs\n\nUse dateInput() to collect a single date.\nUse dateRangeInput() to collect two dates.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  dateInput(\"dob\", \"When were you born?\"),\n  dateRangeInput(\"holiday\", \"When do you want to go on vacation next?\")\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nExercise: In dateInput(), try to disable the selection of Sundays, Mondays, Fridays, and Saturdays."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#multiple-choice",
    "href": "09_shiny/09_basic_shiny.html#multiple-choice",
    "title": "The Basics of Shiny Apps",
    "section": "Multiple Choice",
    "text": "Multiple Choice\n\nUse selectInput() to provide the user with a drop-down menu.\nUse radioButtons() to have a multiple choice button selection where only selection is possible.\nUse checkboxGroupInput() to have a multiple choice button selection where multiple selections are possible.\n\nlibrary(shiny)\n\nweekdays &lt;- c(\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\")\nui &lt;- fluidPage(\n  selectInput(\"state\", \"Where do you live?\", choices = state.name),\n  radioButtons(\"weekday\", \"What's your favorite day of the week?\", choices = weekdays),\n  checkboxGroupInput(\"weekday2\", \"What days do you work?\", choices = weekdays)\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nExercise: What does multiple = TRUE do in in selectInput()?"
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#columns-of-a-data-frame",
    "href": "09_shiny/09_basic_shiny.html#columns-of-a-data-frame",
    "title": "The Basics of Shiny Apps",
    "section": "Columns of a Data Frame",
    "text": "Columns of a Data Frame\n\nFor a data frame named df, just use selectInput(), with the choices = names(df) option.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  selectInput(\"carcol\", \"Which Column?\", choices = names(mtcars))\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nExercise: What does the selected argument do? Change it."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#binary-inputs",
    "href": "09_shiny/09_basic_shiny.html#binary-inputs",
    "title": "The Basics of Shiny Apps",
    "section": "Binary Inputs",
    "text": "Binary Inputs\n\nUse checkboxInput() to get a TRUE/FALSE or Yes/No answer.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  checkboxInput(\"startrek\", \"Like Star Trek?\")\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nExercise: What are the possible values for the value argument? Change it.\n\nIt can be either TRUE (so the box is checked) or FALSE (so the box is unchecked)."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#file-inputs",
    "href": "09_shiny/09_basic_shiny.html#file-inputs",
    "title": "The Basics of Shiny Apps",
    "section": "File Inputs",
    "text": "File Inputs\n\nUse fileInput() to have a user input a file name.\nRunning the app, you should get something like this:\n\nExercise: What does the buttonLabel argument do? Change it."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#action-buttons",
    "href": "09_shiny/09_basic_shiny.html#action-buttons",
    "title": "The Basics of Shiny Apps",
    "section": "Action Buttons",
    "text": "Action Buttons\n\nUse actionButton() to create a clickable button, or actionLink() to create a clickable link.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  actionButton(\"click\", \"Click me!\"),\n  actionLink(\"Link\", \"No, click me!\")\n)\n\nserver &lt;- function(input, output) {\n\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:"
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#text-output",
    "href": "09_shiny/09_basic_shiny.html#text-output",
    "title": "The Basics of Shiny Apps",
    "section": "Text Output",
    "text": "Text Output\n\nUse textOutput() to display text.\nUse verbatimTextOutput() to display code.\nYou create text in the server() function by either renderText() or renderPrint().\nrenderText() will display text returned by code. Functions can only return one thing.\nrenderPrint() will display text printed by code. Functions can print multiple things.\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  textOutput(\"text\"),\n  verbatimTextOutput(\"code\")\n)\n\nserver &lt;- function(input, output, session) {\n  output$text &lt;- renderText({\n    \"Hello World!\"\n  })\n\n  output$code &lt;- renderPrint({\n    summary(c(1, 2, 3, 4))\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nExercise: Change the label from “text” to something else. Make sure the Shiny App still works."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#output-tables",
    "href": "09_shiny/09_basic_shiny.html#output-tables",
    "title": "The Basics of Shiny Apps",
    "section": "Output Tables",
    "text": "Output Tables\n\nUse tableOutput() to print an entire table created in the server by renderTable().\n\nShould only be used for small tables.\n\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  tableOutput(\"static\")\n)\n\nserver &lt;- function(input, output, session) {\n  output$static &lt;- renderTable({\n    head(mtcars)\n    })\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nUse dataTableOutput() to output a dynamic table created in the server by renderDataTable().\n\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  dataTableOutput(\"dynamic\")\n)\n\nserver &lt;- function(input, output, session) {\n  output$dynamic &lt;- renderDataTable({\n    mtcars\n    })\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nYou can change the appearance of dataTableOutput() by passing arguments as a list to the options argument in renderDataTable().\n\nYou can find these options at: https://datatables.net/reference/option/\n\n\nrenderDataTable({ mtcars }, options = list(pageLength = 5))"
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#output-plots",
    "href": "09_shiny/09_basic_shiny.html#output-plots",
    "title": "The Basics of Shiny Apps",
    "section": "Output Plots",
    "text": "Output Plots\n\nUse plotOutput() to output plots created by renderPlot() in the server() function.\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n  plotOutput(\"plot\")\n)\n\nserver &lt;- function(input, output, session) {\n  output$plot &lt;- renderPlot({\n    ggplot(mpg, aes(x = displ, y = hwy)) +\n      geom_point() +\n      theme_bw() +\n      xlab(\"Displacement\") +\n      ylab(\"Highway MPG\")\n    })\n}\n\nshinyApp(ui = ui, server = server)\n\nRunning the app, you should get something like this:\n\nExercise: Change the height and width of the plot by changing the argument values in renderPlot().\nExercise: Change the height and width of the plot by changing the argument values in plotOutput()."
  },
  {
    "objectID": "09_shiny/09_basic_shiny.html#alternative-pipeline-to-.data",
    "href": "09_shiny/09_basic_shiny.html#alternative-pipeline-to-.data",
    "title": "The Basics of Shiny Apps",
    "section": "Alternative pipeline to .data",
    "text": "Alternative pipeline to .data\n\nYou can use selectInput() for variable selection, in which case you can use the .data object.\nBut if you use varSelectInput(), you need to use !!.\n\nlibrary(shiny)\nlibrary(ggplot2)\n\nui &lt;- fluidPage(\n    varSelectInput(\"var1\", \"Variable 1\", data = mtcars),\n    varSelectInput(\"var2\", \"Variable 2\", data = mtcars),\n    plotOutput(\"plot\")\n)\n\nserver &lt;- function(input, output) {\n    output$plot &lt;- renderPlot({\n        ggplot(mtcars, aes(x = !!input$var1, y = !!input$var2)) +\n            geom_point()\n    })\n}\n\nshinyApp(ui = ui, server = server)\n\nCompare UI:\n\nWay 1: varSelectInput(\"var1\", \"Variable 1\", data = mtcars)\nWay 2: selectInput(\"var1\", \"Variable 1\", choices = names(mtcars))\n\nCompare Server:\n\nWay 1: x = !!input$var1\nWay 2: x = .data[[input$var1]]\n\nExercise: Create a Shiny app that will take as input\n\nA column from mtcars\nA numeric value\n\nand it will print out an interactive table containing just the rows from mtcars that have a value of the selected variable above the provided numeric value. For example, if the user selects mpg as the column and 30 as the numeric value, then the code that would effectively be run would be:\n\nmtcars |&gt;\n  filter(mpg &gt; 30)"
  },
  {
    "objectID": "admin/syllabus.html#some-thoughts",
    "href": "admin/syllabus.html#some-thoughts",
    "title": "DATA-413/613 Data Science",
    "section": "Some Thoughts",
    "text": "Some Thoughts\n\nGenerative AI performs best on simple, well-documented problems—that is, problems like the ones you encounter in college.\n\nThese are overrepresented in its training data.\n\nIt performs worst on complex, messy, or proprietary problems—that is, problems you’ll encounter in the workplace.\n\nThese are underrepresented in its training data.\n\nTo solve real-world problems, you need to master the fundamentals first.\n\nIf you use generative AI to shortcut your way through college-level work, you’re undermining your own preparation.\n\nIn data science and statistics, a degree may get you an interview, but it won’t get you the job.\n\nEmployers want evidence that you understand the basics and can build from them.\n\nIf you rely too heavily on generative AI, you may struggle to develop the core skills needed to succeed in the field—and that will show during interviews and on the job."
  },
  {
    "objectID": "02_organization/02_files.html",
    "href": "02_organization/02_files.html",
    "title": "File Structure",
    "section": "",
    "text": "Understand file systems on Windows and Mac.\nBetter organize your files."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dgerard@american.edu. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at dgerard@american.edu. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "02_organization/02_projects.html",
    "href": "02_organization/02_projects.html",
    "title": "Project Organization",
    "section": "",
    "text": "Learning Objectives\n\nR Studio Projects\nChapter 6 of R for Data Science\n\nMost of my notes here are from Hadley. Thank you!\n\nMy project template: https://github.com/gerardlab/proj\nJim Hester’s project template: https://github.com/jimhester/analysis_framework\n\n\n\nWhat is Truth\n\nWhat is Truth?\n\nHadley uses this metaphore, where Truth = source of reproducibility.\n\nRight now, your Truth is whatever is in your global environment (the variables and functions that you, or R, have created in your current R session).\nThis is not reproducible. It’s hard to share it. It’s hard to exactly re-create it. You should never depend on your R environment for Truth.\nTruth should be what is written in your R scripts and Quartos.\nRun this to get a blank global environment each time you start up R studio:\nusethis::use_blank_slate()\nThat forces you to have the correct Truth.\n\n\n\nWhere does your project live?\n\nYour working directory is where R looks for files/folders.\nIt’s the same concept as from the bash lecture, but applied to R instead of bash.\nR’s working directory can be different from bash’s.\nIn R, you can see the current working directoy by\n\ngetwd()\n\n\n\n\nWhere should your project live?\n\nAll files/data/output for a single project should be in a single directory.\nYour working directory should be at the root of the project directory.\nYou can format this manually, but R Studio Projects do this for you automatically.\n\nSets the working directory to be the root of the project.\nYou can set default behavior for the project.\nR studio has tools that interface with projects (like using git or R environments inside a project).\n\nYou create a project with “File” &gt; “New Project…”\n\nClick on “New Directory”\n\nClick on “New Project”\n\nFill out the project name (making sure they follow the same standards as file names). Choose a directory to place the project.\nThe new project will open. The working directory will be the project location.\ngetwd()\n[1] \"/Users/dgerard/Documents/teaching/r4ds\"\nIf you quit R Studio and double click on the “.Rproj” file, then this will reopen the project. Try this now.\nYou can switch between projects by clicking on the project name at the top right of R Studio and toggling between projects.\nOnce inside a project, you should only use relative paths.\n\nIn an R script, assume the working directory is the root of the project.\nIn a Quarto  document, assume the working directory is the location of the Quarto  document.\n\n\n\n\nProject Organization\n\nMy default project structure is here: https://github.com/gerardlab/proj\nIt contains four basic folders:\nanalysis/:\n\nContains Quarto  documents for exploratory analysis and reports.\n\ncode/:\n\nContains R scripts. Use this for reusable functions or large code that doesn’t require literate programming.\n\noutput/:\n\nContains figures and cleaned data that were created in code/ and analysis/.\n\ndata/:\n\nContains raw data. Once placed here, data should never be modified.\nAlways keep a copy of the original dataset untouched. If you overwrite it, you may never be able to reproduce your results.\n\nThere are lots of other perfectly fine project structures, like Jim Hester’s.\n\n\n\nREADME files\n\nIn a repo/project, you should have a small summary of your project so others can see what it’s about.\nCreate a README file in your open project by writing:\nusethis::use_readme_rmd()\nThis opens up an R markdown file that you can edit to include a summary of your project.\nWhen you knit the R markdown file, it renders it to a markdown file that is viewable on GitHub.\n\n\n\nExercise\n\nIn your r4ds project, create four folders: analysis, code, output, and data.\nDownload the Big Mac data from here, as described here. Place it in the data folder.\nCreate a Quarto  file in the analysis folder. Load the tidyverse and the Big Mac data. Make sure the document renders.\nCreate an R script in the code folder. Load the tidyverse and the Big Mac data. Confirm that the working directory is still the root of the project.\nCreate a README file that summarizes that this repo’s goal is to analyze the Big Mac data."
  },
  {
    "objectID": "02_organization/02_scrum.html",
    "href": "02_organization/02_scrum.html",
    "title": "Project Management",
    "section": "",
    "text": "Learning Objectives\n\nKanban: https://kanban.university/kanban-guide/\n\n\n\nKanban\n\nThere are many project management paradigms out there.\nWe will focus on one that is popular in software engineering and adapt it to data science: kanban.\nOther systems (like scrum) are too focused on deliverables, whereas in data science outcomes are typically uncertain.\n\nE.g., sometimes the questions you have are impossible to answer given the data available.\n\nKanban is also natively supported in GitHub repos.\nKanban is a board with (at least) three columns: “To do”, “In Progress”, and “Done”."
  },
  {
    "objectID": "02_organization/02_kanban.html",
    "href": "02_organization/02_kanban.html",
    "title": "Project Management",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand the principles of the Kanban system for project management.\nApply Kanban using GitHub Projects.\nReflect on common group project challenges and how Kanban addresses them.\nDevelop, prioritize, and manage tasks using a Kanban workflow.\nThe Kanban Guide\nThe Kanban Pocket Guide\n\n\n\nMotivation\n\nAlmost every group project I have been on as a student was one of these three types:\n\nI did all of the work.\nI did none of the work.\nI did a fair share of the work, but it was completely separate from my collaborators and we just stapled the different parts together.\n\nI think this experience is really common with students, which is why so many hate group projects.\nThis is a problem with project management, not student abilities.\nThere are whole fields devoted to improving project management and team collaboration.\nWe won’t get into the full details of these. Some folks get entire degrees in the subject.\nHere, we will just try to apply a couple of the principles of project management to team collaboration.\nExercise: In groups of ~4, discuss your past experiences with group projects and answer:\n\nHow do you plan out tasks?\nHow do you assign roles?\nHow do you monitor progress?\nAre you satisfied with how most group projects go? Why or why not?\n\nOutput: Summarize 2-3 key pain points or challenges, and be prepared to share.\n\n\n\nKanban\n\nThere are many project management paradigms out there, but we’ll focus on Kanban.\n\nKanban is so basic, that many more complicated paradigms build on it, or use it as a component.\n\nKanban originated in the Japanese auto industry and has expanded to other fields.\nOther systems (like scrum) are too focused on deliverables, whereas in data science outcomes are typically uncertain. This makes other systems not as useful for data science collaborations (Saltz, Crowston, et al. 2017)\n\nE.g., sometimes the questions you have are impossible to answer given the data available.\nSaltz and Suthrland (2019) has a very nice summary of the different project management systems used in Data Science.\n\nKanban is also natively supported in GitHub Projects.\nKanban has five underlying principles:\n\nVisualize the workflow\nLimit work-in-progress,\nManage flow,\nMake process policies explicit,\nImprove collaboration by implementing feedback loops\n\n\n\n\nVisualize Workflow\n\nKanban is a board with three or more columns of increasing levels of completion.\nE.g.: “To do”, “In progress”, and “Done”.\nGitHub’s default board has: Backlog, Ready, In progress, In review, and Done\nYou place work items on (virtual) cards on the board. Example work items might be:\n\nDownload and clean the EPA water data\nExploratory data analysis on the College scorecard data.\nRandom forest implementation of the Netflix movie data.\nDraft introduction and literature review for final project\nBAD: “Do the final project” (too vague/broad)\n\nYou place the name of the person responsible for the card on that card.\n\nLimit it to one person, otherwise the split responsibility could make it so that nothing is completed.\nIf it requires two people, split the task up into smaller tasks.\n\nAll work items begin at the left-most column. As the work-items progress you move them rightward one column until you reach “done”.\nThis allows all members of a team to see what items are being worked on and the overall progress of the project.\nThese boards can get way more complicated for more complicated projects.\nTypical board sites: Trello, Microsoft Planner, and GitHub (where each task is an Issue).\nYou should have access to Microsoft Planner, but AU for some reason is very restrictive about creating 365 groups, so we won’t deal with that.\nInstead, since we already have a GitHub organization, we will use GitHub Projects to create a Kanban board.\n\n\n\nGitHub Projects\n\nGo to the GitHub organization (https://github.com/data-science-fall-2025) and click on the “Projects” tab\n\nClick on “New project”\n\nI already created a template called “Kanban”. Click on that.\n\nChoose your project name and click “Create project”\n\nClick “Add item” in the “To do” column to create a new card\n\nClick on “Create a draft” after you have the card name.\n\n“Draft” is short for “Draft issue”, which is an issue that only exists in the Project. GitHub Projects are set up to correlate most cards to real issues in real repos.\nWe’ll stick with basic features for now. Integrating with issues is more advanced and optional.\nClick on the card to open up the card settings.\n\nYou can add a description of what work needs to be done by clicking “Edit”. You can assign who is responsible for that work by clicking “Add assignees”\n\nYou can now move the card between columns.\nBy default, I set the limit in “In Progress” to 4, but you can change this by clicking the “…” next to that column.\n\n\n\nLimit Work-in-progress (WIP)\n\nKeep each card to about 1–3 days of effort\nSet a numeric WIP limit above the column: e.g., “In progress (max 3).”\nWhen the column is full, finish something before starting new work.\nThis has many advantages:\n\nallows you to prioritize what tasks to do first.\nminimizes bottlenecks. If “In progress” (or some other intermediate step) is overloaded, your group might need to prioritize finishing the tasks in progress over starting new tasks.\nallows you to reprioritize different aspects of the project by reprioritizing what items are moved to “In progress”.\n\n\n\n\nMeasure and Manage Flow\n\nOrganize your “To Do” column vertically by priority.\n\nHighest priority at the top, lowest priority at the bottom.\n\nManage bottlenecks.\n\nIt’s easy to see what is stalled on the visual board.\n\nKeep your board up-to-date.\n\nKanban does not work if anyone doesn’t buy-in to the system and fails to update cards.\nYou can assign a team member to be in charge of making sure the board is up-to-date.\n\n\n\n\nMake Process Policies Explicit\n\nBe explicit about when you can move a work item to the next column.\nWrite your rules on the board\nE.g.:\n\nCode must run from a new environment without errors to move on.\nQuarto’s  must render without error.\n\n\n\n\nImprove Collaboration\n\nUse stand-ups (tech-speak for short meetings) and retros (tech-speak for meetings focused on reviewing progress) to tweak the board and process together.\nStand-up\n\nRelatively frequent meetings\n10–15 minutes\nDiscuss items on the board from right to left (most done to least done)\nUnblock stuck cards — discuss where bottlenecks are.\nKeep it to the board\nNo deep dives\n\nRetro\n\nLess frequent meetings.\nWhat flowed?\nWhat stalled?\nUpdate WIP limits / policies\nAction items become new cards.\n\n\n\n\nExercise\nYou’re working in a team to analyze the College Scorecard Data and identify which features are most associated with post-graduate median earnings.\n\nCreate a GitHub Kanban board in your team repo.\nCreate 6–10 task cards, covering any or all of the following:\n\nData import\nCleaning/missingness\nExploratory analysis\nModel selection\nModel fitting\nInterpretation of results\nReport writing\n\nAssign a student to each card. Ensure no one has more than two cards.\nSet WIP limits and write 1–2 process policies.\nOrganize the “To do” column by priority.\n\n\n\n\n\n\n\n\n\nReferences\n\nSaltz, Jeffrey, Kevin Crowston, et al. 2017. “Comparing Data Science Project Management Methodologies via a Controlled Experiment.”\n\n\nSaltz, Jeffrey, and Alex Suthrland. 2019. “SKI: An Agile Framework for Data Science.” In 2019 IEEE International Conference on Big Data (Big Data), 3468–76. IEEE. https://doi.org/10.1109/BigData47090.2019.9005591."
  },
  {
    "objectID": "02_organization/02_files.html#file-extensions",
    "href": "02_organization/02_files.html#file-extensions",
    "title": "File Structure",
    "section": "File extensions",
    "text": "File extensions\n\nA file extension is a suffix for a file (like “.txt”, “.jpg”, “.R”, etc). It is primarily used to tell the operating system what program to use to open it.\nWhen you double click on a file, your operating system knows what program to use (almost) only because of the file extension.\nWindows sometimes hides the file extension. You should change the settings so you can see them:\n\nOpen File Explorer, click on the “View” tab, and check the “File name extensions” box.\n\nThere are ways to choose the default program to use when opening a file with a given file extension."
  },
  {
    "objectID": "02_organization/02_files.html#hidden-files",
    "href": "02_organization/02_files.html#hidden-files",
    "title": "File Structure",
    "section": "Hidden files",
    "text": "Hidden files\n\nSome files/folders are hidden by default. These typically begin with a period “.”.\nCommon examples:\n\n.git: A folder that contains all of your version history.\n.gitignore: A file that you can edit to say what files to never commit.\n.DS_Store: A Mac-specific file used by Finder to maintain properties of your folder.\n.Renviron: A file you can create that contains variables (format: name=value) that are available on startup.\n.Rprofile: A file you can create that is sourced on R’s startup. Can be any R code.\n\nThere are ways to see hidden files by default."
  },
  {
    "objectID": "05_web_scraping/05_web_scraping_old.html",
    "href": "05_web_scraping/05_web_scraping_old.html",
    "title": "Web Scraping with rvest",
    "section": "",
    "text": "Learning Objectives\n\nBasics of Web Scraping\nChapter 24 of RDS\nOverview of rvest.\nSelectorGadget.\nWeb Scraping\n\n\n\nData on the Web\n\nThere are at least 4 ways people download data on the web:\n\nClick to download a csv/xls/txt file.\nUse a package that interacts with an API.\nUse an API directly.\nScrape from directly from the HTML file.\n\nThis lesson, we talk about how to do 4.\nNote: You shouldn’t download thousands of HTML files from a website to parse — the admins might block you if you send too many requests.\nNote: Web scraping can be illegal in some circumstances, particularly if you intend to make money off of it or if you are collecting personal information. I don’t give legal advice, so see Chapter 24 of RDS for some general recommendations, and talk to a lawyer if you are not sure.\nLet’s load the tidyverse:\n\nlibrary(tidyverse)\n\n\n\n\nCSS\n\nWe have to know a little bit about HTML and CSS in order to understand how to extract certain elements from a website.\nCSS stands from “Cascading Style Sheets”. It’s a formatting language that indicates how HTML files should look. Every website you have been on is formatted with CSS.\nHere is some example CSS:\nh3 {\n  color: red;\n  font-style: italic;\n}\n\nfooter div.alert {\n  display: none;\n}\nThe part before the curly braces is called a selector. It corresponds to HTML tags. Specifically, for those two they would correspond to:\n&lt;h3&gt;Some text&lt;/h3&gt;\n\n&lt;footer&gt;\n&lt;div class=\"alert\"&gt;More text&lt;/div&gt;\n&lt;/footer&gt;\nThe code inside the curly braces are properties. For example, the h3 properties tells us to make the h3 headers red and in italics. The second CSS chunk says that all &lt;div&gt; tags of class \"alert\" in the &lt;footer&gt; should be hidden.\nCSS applies the same properties to the same selectors. So every time we use h3 will result in the h3 styling of red and italicized text.\nCSS selectors define patterns for selecting HTML elements. This is useful for scraping because we can extract all text in an HTML that corresponds to some CSS selector.\nYou can get a long way just selecting all p elements (standing for “paragraph”) since that is where a lot of text lives. Also .title and #title.\n\n\n\nSelectorGadget\n\nSelectorGadget is a tool for you to see what selector influences a particular element on a website.\nTo install SelectorGadget, drag this link to your bookmark bar on Chrome: SelectorGadget\nSuppose we wanted to get the top 100 movies of all time from IMDB. The web page is very unstructured:\nhttps://www.imdb.com/list/ls055592025/\n \nIf we click on the ranking of the Godfather, the “1” turns green (indicating what we have selected).\n \nThe “.text-primary” is the selector associated with the “1” we clicked on.\nEverything highlighted in yellow also has the “.text-primary” selector associated with it.\nWe will also want the name of the movie. So if we click on that we get the selector associated with both the rank and the movie name: “a , .text-primary”.\n \nBut we also got a lot of stuff we don’t want (in yellow). If we click one of the yellow items that we don’t want, it turns red. This indicates that we don’t want to select it.\n \nOnly the ranking and the name remain, which are under the selector “.lister-item-header a , .text-primary”.\nIt’s important to visually inspect the selected elements throughout the whole HTML file. SelectorGadget doesn’t always get all of what you want, or it sometimes gets too much.\nExercise: What selector can we use to get just the genres of each film, the metacritic score, and the IMDB rating?\n\n\n\nChrome developer tools:\n\nIf you have trouble with SelectorGadget, you can also use the Chrome developer tools.\nOpen up the list of selectors with: ⋮ &gt; More tools &gt; Developer tools.\nClicking on the element selector on the top left of the developer tools will show you what selectors are possible with each element.\n \n\n\n\nrvest\n\nWe’ll use rvest to extract elements from HTML files.\n\nlibrary(rvest)\n\nUse read_html() to save an HTML file to a variable. The variable will be an “xml_document” object\n\nhtml_obj &lt;- read_html(\"https://www.imdb.com/list/ls055592025/\")\nhtml_obj\nclass(html_obj)\n\nXML stands for “Extensible Markup Language”. It’s a markup language (like HTML and Markdown), useful for representing data. rvest will store the HTML file as an XML.\nWe can use html_elements() and the selectors we found in the previous section to get the elements we want. Insert the found selectors as the css argument.\n\nranking_elements &lt;- html_elements(html_obj, css = \".lister-item-header a , .text-primary\")\nhead(ranking_elements)\n\n{xml_nodeset (6)}\n[1] &lt;span class=\"lister-item-index unbold text-primary\"&gt;1.&lt;/span&gt;\n[2] &lt;a href=\"/title/tt0068646/?ref_=ttls_li_tt\"&gt;The Godfather&lt;/a&gt;\n[3] &lt;span class=\"lister-item-index unbold text-primary\"&gt;2.&lt;/span&gt;\n[4] &lt;a href=\"/title/tt0111161/?ref_=ttls_li_tt\"&gt;The Shawshank Redemption&lt;/a&gt;\n[5] &lt;span class=\"lister-item-index unbold text-primary\"&gt;3.&lt;/span&gt;\n[6] &lt;a href=\"/title/tt0108052/?ref_=ttls_li_tt\"&gt;Schindler's List&lt;/a&gt;\n\n\nNote: html_element() is similar, but will return exactly one response per element, so is useful if some elements have missing components.\nTo extract the text inside the obtained nodes, use html_text() or html_text2():\n\nhtml_text2() just does a little more pre-formatting (like converting line breaks from HTML to R code, removing white spaces, etc). So you should typically use this.\n\n\nranking_text &lt;- html_text2(ranking_elements)\nhead(ranking_text)\n\n[1] \"1.\"                       \"The Godfather\"           \n[3] \"2.\"                       \"The Shawshank Redemption\"\n[5] \"3.\"                       \"Schindler's List\"        \n\n\nAfter you do this, you need to tidy the data using your data munging tools.\n\ntibble(text = ranking_text) |&gt;\n  mutate(rownum = row_number(),\n         iseven = rownum %% 2 == 0,\n         movie = rep(1:100, each = 2)) |&gt;\n  select(-rownum) |&gt;\n  spread(key = \"iseven\", value = \"text\") |&gt;\n  select(-movie, \"Rank\" = \"FALSE\", movie = \"TRUE\") |&gt;\n  mutate(Rank = parse_number(Rank)) -&gt;\n  movierank\nmovierank\n\n# A tibble: 100 × 2\n    Rank movie                          \n   &lt;dbl&gt; &lt;chr&gt;                          \n 1     1 The Godfather                  \n 2     2 The Shawshank Redemption       \n 3     3 Schindler's List               \n 4     4 Raging Bull                    \n 5     5 Casablanca                     \n 6     6 Citizen Kane                   \n 7     7 Gone with the Wind             \n 8     8 The Wizard of Oz               \n 9     9 One Flew Over the Cuckoo's Nest\n10    10 Lawrence of Arabia             \n# ℹ 90 more rows\n\n\nExercise: Extract the directors and the names of each film. You can extract them separately and combine them.\n\n\n\nBigger example using rvest\n\nLet’s try and get the name, rank, year, genre, and metascore for each movie:\n \nWe copy the CSS selectors and make a text vector\n\ndataobj &lt;- html_elements(html_obj, css = \".favorable , .genre, .unbold, .lister-item-header a\")\ndatatext &lt;- html_text(dataobj)\n\nWe now have a lot of cleaning to do. Note that the first 132 elements we didn’t even want:\n\nhead(datatext)\n\n[1] \"\\n        Prime Video\\n            (15)\\n    \"              \n[2] \"\\n        IMDb TV\\n            (2)\\n    \"                   \n[3] \"\\n        Prime Video (Rent or Buy)\\n            (95)\\n    \"\n[4] \"\\n        Drama\\n            (80)\\n    \"                    \n[5] \"\\n        Romance\\n            (25)\\n    \"                  \n[6] \"\\n        Adventure\\n            (18)\\n    \"                \n\ndatatext[130:136]\n\n[1] \"\\n        Anti Hero\\n            (16)\\n    \"       \n[2] \"\\n        Cigar Smoking\\n            (16)\\n    \"   \n[3] \"\\n        Good Versus Evil\\n            (16)\\n    \"\n[4] \"1.\"                                                \n[5] \"The Godfather\"                                     \n[6] \"(1972)\"                                            \n[7] \"\\nCrime, Drama            \"                        \n\n\nThe rankings are always of the form \"\\\\d+\\\\.\". We’ll use this and a cumulative sum to indicate which movies the variables belong to. This is necessary because some data have elements that are missing (e.g.  “The Great Dictator” doesn’t have a metacritic score).\n\ndatadf &lt;- tibble(text = datatext)\n\ndatadf |&gt;\n  mutate(ismovierank = str_detect(text, \"^\\\\d+\\\\.$\")) -&gt;\n  datadf\n\n## make sure it is 100\nsum(datadf$ismovierank)\n\n[1] 100\n\n## get movie numbers and remove non-movie elements:\ndatadf |&gt;\n  mutate(movienum = cumsum(ismovierank)) |&gt;\n  filter(movienum &gt; 0) -&gt;\n  datadf\n\ndatadf\n\n# A tibble: 496 × 3\n   text                         ismovierank movienum\n   &lt;chr&gt;                        &lt;lgl&gt;          &lt;int&gt;\n 1 \"1.\"                         TRUE               1\n 2 \"The Godfather\"              FALSE              1\n 3 \"(1972)\"                     FALSE              1\n 4 \"\\nCrime, Drama            \" FALSE              1\n 5 \"100        \"                FALSE              1\n 6 \"2.\"                         TRUE               2\n 7 \"The Shawshank Redemption\"   FALSE              2\n 8 \"(1994)\"                     FALSE              2\n 9 \"\\nDrama            \"        FALSE              2\n10 \"80        \"                 FALSE              2\n# ℹ 486 more rows\n\n\nWe’ll use the movierank$movie variable we created before see which rows are movie names\n\ndatadf |&gt;\n  mutate(isname = text %in% movierank$movie) -&gt;\n  datadf\n\n## make sure we have 100 movies:\nsum(datadf$isname)\n\n[1] 100\n\ndatadf\n\n# A tibble: 496 × 4\n   text                         ismovierank movienum isname\n   &lt;chr&gt;                        &lt;lgl&gt;          &lt;int&gt; &lt;lgl&gt; \n 1 \"1.\"                         TRUE               1 FALSE \n 2 \"The Godfather\"              FALSE              1 TRUE  \n 3 \"(1972)\"                     FALSE              1 FALSE \n 4 \"\\nCrime, Drama            \" FALSE              1 FALSE \n 5 \"100        \"                FALSE              1 FALSE \n 6 \"2.\"                         TRUE               2 FALSE \n 7 \"The Shawshank Redemption\"   FALSE              2 TRUE  \n 8 \"(1994)\"                     FALSE              2 FALSE \n 9 \"\\nDrama            \"        FALSE              2 FALSE \n10 \"80        \"                 FALSE              2 FALSE \n# ℹ 486 more rows\n\n\nYears are surrounded by parentheses:\n\ndatadf |&gt;\n  mutate(isyear = str_detect(text, \"\\\\(\\\\d+\\\\)\")) -&gt;\n  datadf\n\n## make sure it is 100\nsum(datadf$isyear)\n\n[1] 100\n\ndatadf\n\n# A tibble: 496 × 5\n   text                         ismovierank movienum isname isyear\n   &lt;chr&gt;                        &lt;lgl&gt;          &lt;int&gt; &lt;lgl&gt;  &lt;lgl&gt; \n 1 \"1.\"                         TRUE               1 FALSE  FALSE \n 2 \"The Godfather\"              FALSE              1 TRUE   FALSE \n 3 \"(1972)\"                     FALSE              1 FALSE  TRUE  \n 4 \"\\nCrime, Drama            \" FALSE              1 FALSE  FALSE \n 5 \"100        \"                FALSE              1 FALSE  FALSE \n 6 \"2.\"                         TRUE               2 FALSE  FALSE \n 7 \"The Shawshank Redemption\"   FALSE              2 TRUE   FALSE \n 8 \"(1994)\"                     FALSE              2 FALSE  TRUE  \n 9 \"\\nDrama            \"        FALSE              2 FALSE  FALSE \n10 \"80        \"                 FALSE              2 FALSE  FALSE \n# ℹ 486 more rows\n\n\nGenre’s begin with a new line:\n\ndatadf |&gt;\n  mutate(isgenre = str_detect(text, \"^\\\\n\")) -&gt;\n  datadf\n\n## make sure it is 100\nsum(datadf$isgenre)\n\n[1] 100\n\n\nEverything else should be the metacritic score:\n\ndatadf |&gt;\n  group_by(ismovierank, isname, isyear, isgenre) |&gt;\n  count()\n\n# A tibble: 5 × 5\n# Groups:   ismovierank, isname, isyear, isgenre [5]\n  ismovierank isname isyear isgenre     n\n  &lt;lgl&gt;       &lt;lgl&gt;  &lt;lgl&gt;  &lt;lgl&gt;   &lt;int&gt;\n1 FALSE       FALSE  FALSE  FALSE      96\n2 FALSE       FALSE  FALSE  TRUE      100\n3 FALSE       FALSE  TRUE   FALSE     100\n4 FALSE       TRUE   FALSE  FALSE     100\n5 TRUE        FALSE  FALSE  FALSE     100\n\ndatadf |&gt;\n  mutate(ismeta = !ismovierank & !isname & !isyear & !isgenre) -&gt;\n  datadf\n\ndatadf\n\n# A tibble: 496 × 7\n   text                        ismovierank movienum isname isyear isgenre ismeta\n   &lt;chr&gt;                       &lt;lgl&gt;          &lt;int&gt; &lt;lgl&gt;  &lt;lgl&gt;  &lt;lgl&gt;   &lt;lgl&gt; \n 1 \"1.\"                        TRUE               1 FALSE  FALSE  FALSE   FALSE \n 2 \"The Godfather\"             FALSE              1 TRUE   FALSE  FALSE   FALSE \n 3 \"(1972)\"                    FALSE              1 FALSE  TRUE   FALSE   FALSE \n 4 \"\\nCrime, Drama           … FALSE              1 FALSE  FALSE  TRUE    FALSE \n 5 \"100        \"               FALSE              1 FALSE  FALSE  FALSE   TRUE  \n 6 \"2.\"                        TRUE               2 FALSE  FALSE  FALSE   FALSE \n 7 \"The Shawshank Redemption\"  FALSE              2 TRUE   FALSE  FALSE   FALSE \n 8 \"(1994)\"                    FALSE              2 FALSE  TRUE   FALSE   FALSE \n 9 \"\\nDrama            \"       FALSE              2 FALSE  FALSE  TRUE    FALSE \n10 \"80        \"                FALSE              2 FALSE  FALSE  FALSE   TRUE  \n# ℹ 486 more rows\n\n\nLet’s create a key for these data then spread them:\n\ndatadf |&gt;\n  mutate(key = case_when(ismovierank ~ \"rank\",\n                         isname ~ \"name\",\n                         isyear ~ \"year\",\n                         isgenre ~ \"genre\",\n                         ismeta ~ \"metacritic\")) |&gt;\n  select(key, text, movienum) |&gt;\n  spread(key = \"key\", value = \"text\") -&gt;\n  datawide\n\ndatawide\n\n# A tibble: 100 × 6\n   movienum genre                                   metacritic name  rank  year \n      &lt;int&gt; &lt;chr&gt;                                   &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1        1 \"\\nCrime, Drama            \"            \"100     … The … 1.    (197…\n 2        2 \"\\nDrama            \"                   \"80      … The … 2.    (199…\n 3        3 \"\\nBiography, Drama, History          … \"94      … Schi… 3.    (199…\n 4        4 \"\\nBiography, Drama, Sport            \" \"89      … Ragi… 4.    (198…\n 5        5 \"\\nDrama, Romance, War            \"     \"100     … Casa… 5.    (194…\n 6        6 \"\\nDrama, Mystery            \"          \"100     … Citi… 6.    (194…\n 7        7 \"\\nDrama, History, Romance            \" \"97      … Gone… 7.    (193…\n 8        8 \"\\nAdventure, Family, Fantasy         … \"100     … The … 8.    (193…\n 9        9 \"\\nDrama            \"                   \"83      … One … 9.    (197…\n10       10 \"\\nAdventure, Biography, Drama        … \"100     … Lawr… 10.   (196…\n# ℹ 90 more rows\n\n\nLet’s clean up the remaining variables:\n\ndatawide |&gt;\n  mutate(genre = str_replace_all(genre, \"\\\\n\", \"\"),\n         genre = str_squish(genre),\n         metacritic = parse_number(metacritic),\n         rank = parse_number(rank),\n         year = parse_number(year)) -&gt;\n  datawide\n\ndatawide\n\n# A tibble: 100 × 6\n   movienum genre                       metacritic name               rank  year\n      &lt;int&gt; &lt;chr&gt;                            &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n 1        1 Crime, Drama                       100 The Godfather         1  1972\n 2        2 Drama                               80 The Shawshank Re…     2  1994\n 3        3 Biography, Drama, History           94 Schindler's List      3  1993\n 4        4 Biography, Drama, Sport             89 Raging Bull           4  1980\n 5        5 Drama, Romance, War                100 Casablanca            5  1942\n 6        6 Drama, Mystery                     100 Citizen Kane          6  1941\n 7        7 Drama, History, Romance             97 Gone with the Wi…     7  1939\n 8        8 Adventure, Family, Fantasy         100 The Wizard of Oz      8  1939\n 9        9 Drama                               83 One Flew Over th…     9  1975\n10       10 Adventure, Biography, Drama        100 Lawrence of Arab…    10  1962\n# ℹ 90 more rows\n\n\n\n\n\nhtml_table()\n\nWhen data is in the form of a table, you can format it more easily with html_table().\nThe Wikipedia article on hurricanes: https://en.wikipedia.org/wiki/Atlantic_hurricane_season\nThis contains many tables which might be a pain to copy and paste into Excel (and we would be prone to error if we did so). Let’s try to automate this procedure.\nSave the HTML\n\nwikixml &lt;- read_html(\"https://en.wikipedia.org/wiki/Atlantic_hurricane_season\")\n\nWe’ll extract all of the “table” elements.\n\nwikidat &lt;- html_elements(wikixml, \"table\")\n\nUse html_table() to get a list of tables from table elements:\n\ntablist &lt;- html_table(wikidat)\nclass(tablist)\n\n[1] \"list\"\n\nlength(tablist)\n\n[1] 20\n\ntablist[[19]] |&gt;\n  select(1:4)\n\n# A tibble: 11 × 4\n   Year  Map     `Number oftropical cyclones` `Number oftropical storms`\n   &lt;chr&gt; &lt;chr&gt;                          &lt;int&gt;                      &lt;int&gt;\n 1 2010  \"\"                                21                         19\n 2 2011  \"\"                                20                         19\n 3 2012  \"\"                                19                         19\n 4 2013  \"\"                                15                         14\n 5 2014  \"\"                                 9                          8\n 6 2015  \"\"                                12                         11\n 7 2016  \"\"                                16                         15\n 8 2017  \"\"                                18                         17\n 9 2018  \"\"                                16                         15\n10 2019  \"\"                                18                         16\n11 Total \"Total\"                          164                        153\n\n\nYou can clean up, bind, or merge these tables after you have read them in.\nExercise: The Wikipedia page on the oldest mosques in the world has many tables.\nhttps://en.wikipedia.org/wiki/List_of_the_oldest_mosques\n\nUse rvest to read these tables into R.\nUse rvest and SelectorGadget to extract out the category for the table (mentioned in Quran, in northeast Africa, etc).\nMerge the data frames together. You only need to keep the building name, the country, and the time it was first build.\n\nHint: It’s easier if you use a css selector of \"table.wikitable\" to get the table rather than just \"table\". I found this out by getting to the developer tools in Chrome with CTRL + Shift + I then playing around with the tables.\nThe first 15 rows should look like this:\n\n\n# A tibble: 15 × 4\n   Building                 Country      fb    category              \n   &lt;chr&gt;                    &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;                 \n 1 Al-Haram Mosque          Saudi Arabia &lt;NA&gt;  Mentioned in the Quran\n 2 Al-Aqsa Mosque           Palestine    &lt;NA&gt;  Mentioned in the Quran\n 3 The Sacred Monument      Saudi Arabia &lt;NA&gt;  Mentioned in the Quran\n 4 Quba Mosque              Saudi Arabia 622   Mentioned in the Quran\n 5 Mosque of the Companions Eritrea      610   Northeast Africa      \n 6 Negash Āmedīn Mesgīd     Ethiopia     620   Northeast Africa      \n 7 Masjid al-Qiblatayn      Somalia      620   Northeast Africa      \n 8 Korijib Masjid           Djibouti     630   Northeast Africa      \n 9 Mosque of Amr ibn al-As  Egypt        641   Northeast Africa      \n10 Mosque of Ibn Tulun      Egypt        879   Northeast Africa      \n11 Al-Hakim Mosque          Egypt        928   Northeast Africa      \n12 Al-Azhar Mosque          Egypt        972   Northeast Africa      \n13 Arba'a Rukun Mosque      Somalia      1268  Northeast Africa      \n14 Fakr ad-Din Mosque       Somalia      1269  Northeast Africa      \n15 Great Mosque of Kairouan Tunisia      670   Northwest Africa"
  },
  {
    "objectID": "03_vectors_iterators/03_memory.html",
    "href": "03_vectors_iterators/03_memory.html",
    "title": "Memory",
    "section": "",
    "text": "Names/Values\nCopy-on-modify\nModify-by-reference\nChapter 2 from Advanced R\n\nThese lecture notes are mostly taken straight out of Hadley’s book. Many thanks for making my life easier.\nHis images, which I use here, are licensed under"
  },
  {
    "objectID": "03_vectors_iterators/03_memory.html#character-vectors",
    "href": "03_vectors_iterators/03_memory.html#character-vectors",
    "title": "Memory",
    "section": "Character Vectors",
    "text": "Character Vectors\n\nA character vector is a vector of references to a global string pool.\n\nx &lt;- c(\"a\", \"a\", \"abc\", \"d\")\n\n\nBut Hadley usually writes this as\n\nUse lobstr::ref() to show these references.\n\nlobstr::ref(x, character = TRUE)\n\n█ [1:0x134484e78] &lt;chr&gt; \n├─[2:0x15330c540] &lt;string: \"a\"&gt; \n├─[2:0x15330c540] \n├─[3:0x134479278] &lt;string: \"abc\"&gt; \n└─[4:0x1544867f8] &lt;string: \"d\"&gt; \n\n\nExercise (from Advanced R): Why do you think x is copied here? (it is only copied twice if you use R studio). Modify the code so that x is not copied.\n\nx &lt;- c(1L, 2L, 3L)\ntracemem(x)\n\n[1] \"&lt;0x153581388&gt;\"\n\nx[[3]] &lt;- 4\n\ntracemem[0x153581388 -&gt; 0x144fac6c8]: eval eval withVisible withCallingHandlers eval eval with_handlers doWithOneRestart withOneRestart withRestartList doWithOneRestart withOneRestart withRestartList withRestarts &lt;Anonymous&gt; evaluate in_dir in_input_dir eng_r block_exec call_block process_group withCallingHandlers &lt;Anonymous&gt; process_file &lt;Anonymous&gt; &lt;Anonymous&gt; execute .main \ntracemem[0x144fac6c8 -&gt; 0x1455ea2d8]: eval eval withVisible withCallingHandlers eval eval with_handlers doWithOneRestart withOneRestart withRestartList doWithOneRestart withOneRestart withRestartList withRestarts &lt;Anonymous&gt; evaluate in_dir in_input_dir eng_r block_exec call_block process_group withCallingHandlers &lt;Anonymous&gt; process_file &lt;Anonymous&gt; &lt;Anonymous&gt; execute .main \n\n\n\nx &lt;- c(1L, 2L, 3L)\ntracemem(x)\n\n[1] \"&lt;0x1472bf148&gt;\"\n\nx[[3]] &lt;- 4L\n\ntracemem[0x1472bf148 -&gt; 0x1473fb888]: eval eval withVisible withCallingHandlers eval eval with_handlers doWithOneRestart withOneRestart withRestartList doWithOneRestart withOneRestart withRestartList withRestarts &lt;Anonymous&gt; evaluate in_dir in_input_dir eng_r block_exec call_block process_group withCallingHandlers &lt;Anonymous&gt; process_file &lt;Anonymous&gt; &lt;Anonymous&gt; execute .main \n\n\nExercise (From Advanced R): Sketch out the relationship between the following objects:\n\na &lt;- 1:10\nb &lt;- list(a, a)\nc &lt;- list(b, a, 1:10)"
  },
  {
    "objectID": "03_vectors_iterators/03_s3.html",
    "href": "03_vectors_iterators/03_s3.html",
    "title": "S3 Objects",
    "section": "",
    "text": "Learning Objectives\n\nChapter 13 of Advanced R\n\nMost notes taken from Hadley’s book. Thank you so much.\n\n\n\n\nMotivation\n\nS3 is the most commonly used object-oriented programming (OOP) system in R.\nMost of the common data types you are used to are S3.\n\n# Data frames are S3\nsloop::otype(mtcars)\n\n[1] \"S3\"\n\n# tibbles are S3\nmt_tb &lt;- tibble::as_tibble(mtcars)\nsloop::otype(mt_tb)\n\n[1] \"S3\"\n\n# lm objects are S3\nlmout &lt;- lm(mpg ~ wt, data = mtcars)\nsloop::otype(lmout) \n\n[1] \"S3\"\n\n# ggplot2 plots are S3\npl &lt;- ggplot2::ggplot(mtcars, ggplot2::aes(x = wt, y = mpg)) + \n  ggplot2::geom_point()\nsloop::otype(pl)\n\n[1] \"S3\"\n\n# tidymodels use S3\ntdout &lt;- \n  parsnip::linear_reg() |&gt;\n  parsnip::set_engine(\"lm\") |&gt;\n  parsnip::fit(mpg ~ wt, data = mtcars)\nsloop::otype(tdout)\n\n[1] \"S3\"\n\n# Factors are S3\nx &lt;- factor(c(1, 2, 3))\nsloop::otype(x)\n\n[1] \"S3\"\n\n# Dates are S3\nx &lt;- lubridate::make_date(year = 1970, month = 1, day = 1)\nsloop::otype(x)\n\n[1] \"S3\"\n\n\nIf you are creating a package and you want OOP features, you should use S3 unless\n\nYou work in a large team, or you need to contribute to Bioconductor (use S4).\nModify-by-reference is important (use R6).\n\nThis is since most R programmers are used to S3 (intuitively) and are not used to S4 or R6.\n\n\n\nS3 Basics\n\nAn S3 object is any variable with a class attribute. This is the full definition.\nS3 objects may or may not have more attributes.\nE.g. the factor class always has the levels attribute.\n\nx &lt;- factor(c(\"A\", \"B\", \"B\", \"A\", \"C\", \"A\"))\nattributes(x)\n\n$levels\n[1] \"A\" \"B\" \"C\"\n\n$class\n[1] \"factor\"\n\n\nYou can get the underlying base type by unclass().\n\nunclass(x)\n\n[1] 1 2 2 1 3 1\nattr(,\"levels\")\n[1] \"A\" \"B\" \"C\"\n\n\nFunctions can be S3 objects as well as long as they have the class attribute.\n\nsout &lt;- stepfun(1:3, 0:3)\nsloop::otype(sout)\n\n[1] \"S3\"\n\nclass(sout)\n\n[1] \"stepfun\"  \"function\"\n\n\nS3 objects behave differently when passed to a generic function, a special type of function meant to provide different implementations based on the S3 class of the object.\nUse sloop::ftype() to see if a function is generic. If it has the word “generic” is anywhere, it can be used as an S3 generic.\nThese are all S3 generics\n\nsloop::ftype(print)\n\n[1] \"S3\"      \"generic\"\n\nsloop::ftype(summary)\n\n[1] \"S3\"      \"generic\"\n\nsloop::ftype(plot)\n\n[1] \"S3\"      \"generic\"\n\n\nBut these are not:\n\nsloop::ftype(lm)\n\n[1] \"function\"\n\nsloop::ftype(stop)\n\n[1] \"internal\"\n\n\nGeneric functions behave differently depending on the class of the object.\n\nprint(mt_tb)\n\n# A tibble: 32 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n\nprint(lmout)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n      37.29        -5.34  \n\nprint(pl)\n\n\n\n\n\n\n\n\nThis is not implemented by if-else statements. That would be inefficient because only the authors of print() (i.e. the R Core team) could add new functionality to new S3 objects. The idea of using generic functions allows us (new developers) to define new functionality to the same generics.\nThe implementation of a generic for a specific class is called a method.\nThe act of choosing a method from a generic is called method dispatch. Use sloop::s3_dispatch() to see this process.\n\nsloop::s3_dispatch(print(mt_tb))\n\n   print.tbl_df\n=&gt; print.tbl\n * print.data.frame\n * print.default\n\n\n\nThe * means the method exists but is not used.\nThe =&gt; means the method exists and is used.\nSo above, it found a method for tbl_df and used it. So it did not go on to look for other methods (tbl, data.frame, or the default method), even though those classes have methods.\n\nBelow there is no aperm() method for matrices, integers, or numerics, so it used the default one, which is for arrays.\n\nmat &lt;- matrix(1:12, nrow = 4, ncol = 3)\nsloop::s3_dispatch(aperm(mat, c(2, 1)))\n\n   aperm.matrix\n   aperm.integer\n   aperm.numeric\n=&gt; aperm.default\n\n\nYou can access specific methods by generic.class(). E.g.\n\nstats:::print.lm(lmout)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n      37.29        -5.34  \n\naperm.default(mat, c(2, 1))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n\n\nBut these are often not exported and should generally not be accessed directly by the user, or other developers.\nLots of methods have . in the middle. But not all functions with . are methods. E.g. read.csv() and t.test() are not methods of generic functions. read.csv() is just a function with a dot in the name, and t.test() is just a generic function with a dot in the name. These functions were created before S3, which is why they are named poorly.\nYou can confirm that a function with a . in it is a method also with sloop::is_s3_method().\n\nsloop::is_s3_method(\"read.csv\")\n\n[1] FALSE\n\nsloop::is_s3_method(\"t.test\")\n\n[1] FALSE\n\nsloop::is_s3_method(\"print.default\")\n\n[1] TRUE\n\n\nBecause of the important role of ., you should never name variables or non method functions with a dot in them.\nTo find all of the methods of a generic, use sloop::s3_methods_generic().\n\nsloop::s3_methods_generic(\"print\")\n\n# A tibble: 307 × 4\n   generic class             visible source             \n   &lt;chr&gt;   &lt;chr&gt;             &lt;lgl&gt;   &lt;chr&gt;              \n 1 print   acf               FALSE   registered S3method\n 2 print   activeConcordance FALSE   registered S3method\n 3 print   AES               FALSE   registered S3method\n 4 print   all_vars          FALSE   registered S3method\n 5 print   anova             FALSE   registered S3method\n 6 print   any_vars          FALSE   registered S3method\n 7 print   aov               FALSE   registered S3method\n 8 print   aovlist           FALSE   registered S3method\n 9 print   ar                FALSE   registered S3method\n10 print   Arima             FALSE   registered S3method\n# ℹ 297 more rows\n\n\nTo find all methods for a class, use sloop::s3_methods_class().\n\nsloop::s3_methods_class(\"data.frame\")\n\n# A tibble: 57 × 4\n   generic       class      visible source\n   &lt;chr&gt;         &lt;chr&gt;      &lt;lgl&gt;   &lt;chr&gt; \n 1 [             data.frame TRUE    base  \n 2 [[            data.frame TRUE    base  \n 3 [[&lt;-          data.frame TRUE    base  \n 4 [&lt;-           data.frame TRUE    base  \n 5 $&lt;-           data.frame TRUE    base  \n 6 aggregate     data.frame TRUE    stats \n 7 anyDuplicated data.frame TRUE    base  \n 8 anyNA         data.frame TRUE    base  \n 9 as.data.frame data.frame TRUE    base  \n10 as.list       data.frame TRUE    base  \n# ℹ 47 more rows\n\n\nExercise: Explain the difference between each of the dots in as.data.frame.data.frame(). How would you typically use this method? Include in your discussion calls from the functions in the {sloop} package.\nExercise: mean() is an S3 generic. What classes have a method for mean(). What is the difference between them?\nExercise (Advanced R): What class of object does the following code return? What base type is it built on? What attributes does it use?\n\nset.seed(21)\nx &lt;- ecdf(rpois(100, 10))\nx\n\nEmpirical CDF \nCall: ecdf(rpois(100, 10))\n x[1:14] =   4,   5,   6,  ...,  16,  17\n\n\nExercise: (Advanced R): What class of object does the following code return? What base type is it built on? What attributes does it use?\n\nx &lt;- table(rpois(100, 5))\nx\n\n\n 1  2  3  4  5  6  7  8  9 10 12 \n 1 11 17 13 11 22  8 11  4  1  1 \n\n\n\n\n\nClasses\n\nAgain, an S3 object is any object with a class attribute,\n\n\n\n\n\n\n\nCreating S3 Objects\n\n\n\nThis can be done in two ways:\nCreate and assign class in one step\n\nx &lt;- structure(list(), class = \"my_class\")\n\nCreate, then set class\n\nx &lt;- list()\nclass(x) &lt;- \"my_class\"\n\n\n\n\nYou can get the class attribute by class() (as long as it is S3).\n\nclass(x)\n\n[1] \"my_class\"\n\n\nThus, it is a little safer to use sloop::s3_class().\n\nsloop::s3_class(x)\n\n[1] \"my_class\"\n\n\nYou can test that an object is a certain class by inherits().\n\nclass(mtcars)\n\n[1] \"data.frame\"\n\ninherits(mtcars, \"data.frame\")\n\n[1] TRUE\n\ninherits(mtcars, \"tibble\")\n\n[1] FALSE\n\nmt_tb &lt;- tibble::as_tibble(mtcars)\ninherits(mt_tb, \"tbl_df\")\n\n[1] TRUE\n\ninherits(mt_tb, \"data.frame\")\n\n[1] TRUE\n\n\nR has no checks that the structure of the class is as you intended. E.g., we can change the “data.frame” class to \"Date\" and bad things will happen (i.e. R will try to use the wrong generics on the data).\n\nclass(mt_tb) &lt;- \"Date\"\nmt_tb\n\nError in as.POSIXlt(.Internal(Date2POSIXlt(x, tz)), tz = tz): 'list' object cannot be coerced to type 'double'\n\n\n\n\n\nGenerics\n\nA generic function is just one that performs method dispatch. Method dispatch is implemented through UseMethod(), so it is really easy to create a new generic.\n\n\n\n\n\n\n\nCreating Generic Functions\n\n\n\nTo create a new generic function called mygeneric with a required argument x:\n\nmygeneric &lt;- function(x, ...) {\n  UseMethod(\"mygeneric\")\n}\n\n\n\n\nNo arguments are passed to UseMethod() except the name of the generic.\nThe x is a required argument that all methods must have. You can choose to have this be a different name, to have more required arguments, or to have no required arguments.\nThe ... allows methods of your generic to include other variables than just x.\nThis is literally what most generic function definitions look like.\n\nmean\n\nfunction (x, ...) \nUseMethod(\"mean\")\n&lt;bytecode: 0x13e0eb7e8&gt;\n&lt;environment: namespace:base&gt;\n\nprint\n\nfunction (x, ...) \nUseMethod(\"print\")\n&lt;bytecode: 0x14f8e57e8&gt;\n&lt;environment: namespace:base&gt;\n\nplot\n\nfunction (x, y, ...) \nUseMethod(\"plot\")\n&lt;bytecode: 0x13e958738&gt;\n&lt;environment: namespace:base&gt;\n\nsummary\n\nfunction (object, ...) \nUseMethod(\"summary\")\n&lt;bytecode: 0x14e8429d8&gt;\n&lt;environment: namespace:base&gt;\n\n\nThe key of a generic is its goals. Methods should generally align with the goals of the generic so that R users don’t get unexpected results. E.g. when you type plot() you shouldn’t output a mean (even though S3 makes this valid behavior).\nHow UseMethod() works: If an object has a class vector of c(\"cl1\", \"cl2\") then UseMethod() will first search for a method for cl1, if it does not exist it will use the method for cl2, and if that does not exist it will use the default method (there is usually one).\nE.g. all tibbles have class\n\nmt_tb &lt;- tibble::as_tibble(mtcars)\nclass(mt_tb)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nSo any generic called with a tibble will first search for a tbl_df method, then a tbl method, then a data.frame method, then a default method (which would be for a list if applicable since tibbles are built on lists).\n\nsloop::s3_dispatch(print(mt_tb))\n\n   print.tbl_df\n=&gt; print.tbl\n * print.data.frame\n * print.default\n\nsloop::s3_dispatch(str(mt_tb))\n\n=&gt; str.tbl_df\n   str.tbl\n * str.data.frame\n * str.default\n\nsloop::s3_dispatch(summary(mt_tb))\n\n   summary.tbl_df\n   summary.tbl\n=&gt; summary.data.frame\n * summary.default\n\nsloop::s3_dispatch(mean(mt_tb))\n\n   mean.tbl_df\n   mean.tbl\n   mean.data.frame\n=&gt; mean.default\n\n\nThe “default” class is not a real class, but is there so that there is always a fall back.\n\n\n\nMethods\n\n\n\n\n\n\nCreating a Method\n\n\n\nTo create a method\n\nCreate a function definition for generic.method().\nMake sure you use the same arguments as the generic (but you can usually include more arguments if there is ... in the generic).\n\n\n\n\nE.g., let’s create plot and print methods for the factor2 class.\n\nprint.factor2 &lt;- function(x) {\n  print(attr(x, \"levels\")[x])\n  return(invisible(x))\n}\n\nplot.factor2 &lt;- function(x, y = NULL) {\n  tabx &lt;- table(attr(x, \"levels\")[x])\n  barplot(table(attr(x, \"levels\")[x]))\n  return(invisible(x))\n}\n\nNow, we get better printing for factor’s\n\nx &lt;- factor(c(\"A\", \"A\", \"B\", \"B\", \"A\", \"B\"))\nclass(x) &lt;- \"factor2\"\nprint(x)\n\n[1] \"A\" \"A\" \"B\" \"B\" \"A\" \"B\"\n\n\nNote: If you don’t know, whenever you just run something and have it print to the console, that is R implicitly running print(). So this looks better too:\n\nx\n\n[1] \"A\" \"A\" \"B\" \"B\" \"A\" \"B\"\n\n\nNote: In a print method, you either call the print() method of another S3 object, or you call cat(), which does less under the hood than print().\nWe can verify that method dispatch is working appropriately\n\nsloop::s3_dispatch(print(x))\n\n=&gt; print.factor2\n * print.default\n\n\nPlotting looks better too\n\nplot(x)\n\n\n\n\n\n\n\n\nYou should only build methods for classes you own, or generics you own. It is considered bad manners to define a method for a class you do not own unless you own the generics.\nE.g. if you define a new print method for tbl_df, then include that in your package, that would be impolite to the tidyverse folks.\nA method should have the same arguments as the generic. You can have more arguments if the generic has ... in it. E.g. if you create plot(), then you must include x and y, but may include anything else.\n\nformals(plot)\n\n$x\n\n\n$y\n\n\n$...\n\n\nExercise (Advanced R): What generics does the table class have methods for?\nExercise: Create a new generic called pop that will remove the last element and return the shortened object. Make a default method for any vector. Then make methods for the matrix class that will remove the last column or row, depending on the user choice of an argument called by.\n\n\n\nThe Design of an S3 Object\n\nThere are three most common structures for an S3 object.\nIn decreasing order of most common usage by you:\n\n\nA list-like object, where the list represents one thing (e.g. model output, function, dataset, etc…).\n\nFor example, the output of lm() is a list like object that represents one model fit.\n\n\nlmout &lt;- lm(mpg ~ wt, data = mtcars)\nsloop::otype(lmout)\n\n[1] \"S3\"\n\ntypeof(lmout)\n\n[1] \"list\"\n\n\n\nI use this format all of the time for the outputs of my model fits.\n\nA vector with new functionality. E.g. factors and Dates. You combine, print, mathematically operate with these vectors in different ways.\n\nx &lt;- factor(c(\"A\", \"A\", \"B\", \"A\", \"B\"))\nsloop::otype(x)\n\n[1] \"S3\"\n\ntypeof(x)\n\n[1] \"integer\"\n\n\nLists of equal length length vectors. E.g. data.frames and POSIXlt objects.\n\nPOSIXlt objects are lists of years, days, minutes, seconds, etc… with the ith element of each vector contributing to indicating the same moment in time.\n\n\nx &lt;- as.POSIXlt(ISOdatetime(2020, 1, 1, 0, 0, 1:3))\nx\n\n[1] \"2020-01-01 00:00:01 EST\" \"2020-01-01 00:00:02 EST\"\n[3] \"2020-01-01 00:00:03 EST\"\n\ntypeof(x)\n\n[1] \"list\"\n\n\n\ndata.frame objects are lists of vectors where each vector is a variable and the ith element of each vector represents the same observational unit.\n\n\ntypeof(mtcars)\n\n[1] \"list\"\n\n\n\n\n\nInheritance\n\nInheritance is shared behavior. You can make your new class inherit from another class so that if you did not create a method, then it will fall back on the parent method.\nWe call the child class the subclass and the parent class the superclass.\nE.g. the tbl_df (sub)class inherits from the data.frame (super)class.\nYou can simply create a subclass by including a vector of in the class attribute.\n\nmt_tb &lt;- tibble::as_tibble(mtcars)\nclass(mt_tb)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nYou should make sure your subclass is of the same base type as the superclass you are inheriting from. E.g. make sure anything you build off of data.frames also has a list base type.\nYou should make sure that you have at least all of the same attributes as the superclass you are inheriting from. E.g. data.frames can have names and row.names, and so any subclass should also have those attributes.\n\n\n\nDocumenting S3\n\nIt is sometimes nice to have the same help file for the default method and the generic. You can do that via the @describeIn {roxygen} tag.\n\n#' Generic Function for generic.\n#' \n#' @param x An R object. \ngeneric &lt;- function(x, ...) {\n\n}\n\n#' @describeIn generic Default Method\n#' \n#' @param y is some default option\n#' \ngeneric.default &lt;- function(x, y = NULL, ...) {\n\n}\n\nSee an example usage of this for the mean() and summary() documentation.\nExercise: Document your pop() generic and the methods you made for pop().\n\n\n\nMethod Dispatch Technicalities\n\nEvery variable in R has some implicit class even if it does not have a class attribute.\nThis implicit class is used to define methods for these objects, and to control method dispatch when you use a base type on a generic.\nsloop::s3_class() will return the implicit or explicit class of all objects.\n\nx &lt;- c(1, 2, 3)\nsloop::otype(x) ## not an S3 object\n\n[1] \"base\"\n\nsloop::s3_class(x) ## implicit S3 class\n\n[1] \"double\"  \"numeric\"\n\nx &lt;- matrix(1:6, nrow = 3, ncol = 2)\nsloop::otype(x) ## not an S3 object\n\n[1] \"base\"\n\nsloop::s3_class(x)\n\n[1] \"matrix\"  \"integer\" \"numeric\"\n\n\nSo to create new matrix methods, you can do\n\ngeneric.matrix &lt;- function(...) {\n\n}\n\neven though matrix is not an S3 class.\nThe following functions are called “group generics” +, -, *, /, ^, %%, %/%, &, |, !, ==, !=, &lt;, &lt;=, &gt;=, and &gt;.\nYou can define methods for these group generics, but undergo what’s called double dispatch, choosing a method based on both arguments. This is what allows you to add integers and dates together. We will talk about how to do this correctly in the next lecture.\n\n\n\nNew functions\n\nclass(): Assign or get the class attribute.\nunclass(): Remove class attribute and obtain underlying base type.\ninherits(): Test if an object is an instance of a given class.\nsloop::ftype(): See if a function is a “regular/primitive/internal function, a internal/S3/S4 generic, or a S3/S4/RC method”.\nsloop::s3_dispatch(): View method dispatch.\nsloop::s3_methods_generic(): View all methods of a generic function.\nsloop::s3_methods_class(): View all methods implemented for a specific class.\nsloop::s3_class(): Returns implicit and explicit class.\nsloop::is_s3_method(): Predicate function for determining if a function is an S3 method.\nUseMethod(): Used in a generic to define it as a generic.\nNextMethod(): Apply the next method, in the method dispatch chain, of the called generic."
  },
  {
    "objectID": "03_vectors_iterators/03_s3.html#constructor",
    "href": "03_vectors_iterators/03_s3.html#constructor",
    "title": "S3 Objects",
    "section": "Constructor",
    "text": "Constructor\n\nYour constructor should\n\nBe called new_myclass(), replacing “myclass” with the name of your class.\nHave one argument for the base object (e.g. list, numeric vector, etc).\nCheck the type of the base object and types of each attribute.\n\nE.g. if we were to create our own class to recapitulate factors, called factor2, we would do\n\nnew_factor2 &lt;- function(x = integer(), levels = character()) {\n  stopifnot(is.integer(x))\n  stopifnot(is.character(levels))\n\n  return(\n    structure(x,\n              levels = levels,\n              class = \"factor2\")\n  )\n}\n\nWe can construct a factor as follows\n\nx &lt;- new_factor2(c(1L, 1L, 2L, 1L, 1L), levels = c(\"A\", \"B\"))\nx\n\n[1] 1 1 2 1 1\nattr(,\"levels\")\n[1] \"A\" \"B\"\nattr(,\"class\")\n[1] \"factor2\""
  },
  {
    "objectID": "03_vectors_iterators/03_s3.html#validator",
    "href": "03_vectors_iterators/03_s3.html#validator",
    "title": "S3 Objects",
    "section": "Validator",
    "text": "Validator\n\nMaking sure the structure of an object is what you would expect is expensive.\nE.g., we need to make sure that the number of unique values in a factor is at most the number of levels in that factor.\nValidator functions should:\n\nBe named validator_myclass().\nTake as input just an object from your class.\nInclude a bunch of assertions testing the structure of the inputted object.\nReturn the original object.\n\nLet’s make a validator for factor2.\n\nvalidate_factor2 &lt;- function(x) {\n  stopifnot(inherits(x, \"factor2\"))\n  values &lt;- unclass(x)\n  levels &lt;- attr(x, \"levels\")\n\n  if (length(levels) &lt; max(values)) {\n    stop(\"There must be at least as many `levels` as possible values in `x`\")\n  }\n  return(x)\n}\n\n\nvalidate_factor2(x)\n\n[1] 1 1 2 1 1\nattr(,\"levels\")\n[1] \"A\" \"B\"\nattr(,\"class\")\n[1] \"factor2\""
  },
  {
    "objectID": "03_vectors_iterators/03_s3.html#helpers",
    "href": "03_vectors_iterators/03_s3.html#helpers",
    "title": "S3 Objects",
    "section": "Helpers",
    "text": "Helpers\n\nA helper function is a user-facing function that will\n\nBe called myclass().\nCall first the constructor function, then the validator function.\nBe user friendly.\n\nGood defaults.\nAccepts multiple types for the base object and coerces intelligently.\n\n\nLet’s do this for factor2.\n\nfactor2 &lt;- function(x = character(), levels = unique(x)) {\n  ind &lt;- match(x, levels)\n  return(validate_factor2(new_factor2(ind, levels)))\n}\n\nfactor2(c(\"A\", \"B\", \"B\", \"A\"))\n\n[1] 1 2 2 1\nattr(,\"levels\")\n[1] \"A\" \"B\"\nattr(,\"class\")\n[1] \"factor2\"\n\n\nSide note: match() is a useful function. It will provide the positions of the second argument that match the values in the second argument. E.g.\n\nmatch(c(\"A\", \"A\", \"B\", \"A\", \"B\"), c(\"A\", \"B\"))\n\n[1] 1 1 2 1 2\n\n\nExercise (Advanced R): Write a constructor for data.frame objects. What base type is a data frame built on? What attributes does it use? What are the restrictions placed on the individual elements? What about the names?"
  },
  {
    "objectID": "03_vectors_iterators/03_s3.html#next-method",
    "href": "03_vectors_iterators/03_s3.html#next-method",
    "title": "S3 Objects",
    "section": "Next Method",
    "text": "Next Method\n\nNextMethod() allows you define methods for your class that use the functionality of classes that you inherit from.\nE.g. recall that most attributes are lost with [.\n\nx &lt;- factor2(c(\"A\", \"A\", \"B\", \"A\", \"B\"))\nx[1]\n\n[1] 1\n\n\nThis is because R is using the integer version for [ and so we lose the class.\n\nsloop::s3_dispatch(x[1])\n\n   [.factor2\n   [.default\n=&gt; [ (internal)\n\n\nWe cannot use [ inside a definition for a method because we haven’t defined it yet.\n\n## won't work\n`[.factor2` &lt;- function(x, i) {\n  return(x[i]) # but we haven't defined `[` yet\n}\nx[1] ## infinite recursion\n\nYou can define your method to use the next method by NextMethod(). NextMethod() will take the arguments inside your function definition and run them through the next method in the inheritance list. So it returns an unclassed object, that you can then pass to your constructor function.\nMake sure you also include the attributes in your constructor.\n\n`[.factor2` &lt;- function(x, i) {\n  new_factor2(NextMethod(), levels = attr(x, \"levels\"))\n}\n\n\nsloop::s3_dispatch(x[1:3])\n\n=&gt; [.factor2\n   [.default\n-&gt; [ (internal)\n\nx[1:3]\n\n[1] \"A\" \"A\" \"B\"\n\n\nIf we did not pass NextMethod() to our constructor, it would just run the integer subsetting:\n\n`[.factor2` &lt;- function(x, i) {\n  NextMethod()\n}\nx[1:3]\n\n[1] 1 1 2"
  },
  {
    "objectID": "03_vectors_iterators/03_oop.html",
    "href": "03_vectors_iterators/03_oop.html",
    "title": "Introduction to Object Oriented Programming",
    "section": "",
    "text": "Learning Objectives\n\nOverview of OOP.\nVocabulary of OOP.\nR Base Types.\nIntroduction to OOP and Chapter 12 of Advanced R\n\n\n\nMotivation\n\nThere are various strategies to programming that folks use.\nYou are mostly used to procedural programming where you list out a sequence of steps that are carried out in succession.\n\nmean_vec &lt;- rep(NA_real_, length.out = length(mtcars))\nnames(mean_vec) &lt;- names(mtcars)\nfor (i in seq_along(mtcars)) {\n  mean_vec[[i]] &lt;- mean(mtcars[[i]])\n}\nmean_vec\n\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 20.0906   6.1875 230.7219 146.6875   3.5966   3.2172  17.8487   0.4375 \n      am     gear     carb \n  0.4062   3.6875   2.8125 \n\n\nYou have also been exposed to functional programming where you compose functions with other functions.\n\nsapply(mtcars, mean)\n\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 20.0906   6.1875 230.7219 146.6875   3.5966   3.2172  17.8487   0.4375 \n      am     gear     carb \n  0.4062   3.6875   2.8125 \n\npurrr::map_dbl(mtcars, mean) ## tidyverse version\n\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 20.0906   6.1875 230.7219 146.6875   3.5966   3.2172  17.8487   0.4375 \n      am     gear     carb \n  0.4062   3.6875   2.8125 \n\n\nObject oriented programming (OOP) is a different style of programming than you are used to, centered around objects with data and functions attached to them and their class.\nR has three native object oriented programming systems (S3, S4, and RC for “reference classes”), and many other third-party packages have made their own object oriented systems ({R6} being the most popular).\n\nThese systems are listed in increasing order of complexity, with S3 being “baby” OOP, S4 being “YA” OOP, and RC and R6 being “big boy” OOP.\nIf you are extending {ggplot2} then you will learn about another OOP system specific to {ggplot2}: ggproto.\n\nE.g.: To calculate the column means in S3 OOP, we would probably create a generic function for column means.\n\ncol_means &lt;- function(x, ...) {\n  UseMethod(\"col_means\")\n}\n\nand then create a specific method for the data.frame class\n\ncol_means.data.frame &lt;- function(x) {\n  sapply(x, mean)\n}\n\nFinally, we would call the generic function on a object of class data.frame.\n\ncol_means(mtcars)\n\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 20.0906   6.1875 230.7219 146.6875   3.5966   3.2172  17.8487   0.4375 \n      am     gear     carb \n  0.4062   3.6875   2.8125 \n\n\nE.g.: To calculate the column means in S4 OOP is very similar, just more formal:\n\nsetOldClass(Classes = \"data.frame\")\nsetGeneric(name = \"col_means_s4\", def = function(x) standardGeneric(\"col_means_s4\"))\n\n[1] \"col_means_s4\"\n\nsetMethod(f = \"col_means_s4\", \n          signature = \"data.frame\", \n          definition = function(x) {\n            sapply(x, mean)\n          }\n)\ncol_means_s4(mtcars)\n\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 20.0906   6.1875 230.7219 146.6875   3.5966   3.2172  17.8487   0.4375 \n      am     gear     carb \n  0.4062   3.6875   2.8125 \n\n\nE.g.: To calculate the column means in R6 OOP, we would probably create a new class that has the $col_means() method that we could call.\n\ndatFrame &lt;- R6::R6Class(classname = \"datFrame\", public = list(\n  df = NULL,\n  initialize = function(df) {\n    stopifnot(is.data.frame(df))\n    self$df &lt;- df\n  },\n  col_means = function() {\n    sapply(self$df, mean)\n  }\n)\n)\n\nmtcars_df &lt;- datFrame$new(df = mtcars)\nmtcars_df$col_means()\n\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 20.0906   6.1875 230.7219 146.6875   3.5966   3.2172  17.8487   0.4375 \n      am     gear     carb \n  0.4062   3.6875   2.8125 \n\n\nBecause R programmers are not OOP programmers, you should be coding mostly in S3 and S4 when using OOP. We’ll only spend time on S3 for this class (the most popular one).\nS3 and S4 use generic function OOP where the same function name is evaluated differently based on the class of the object.\nE.g. that allows the output of summary() to differ between doubles and factors.\n\nx &lt;- sample(1:10, size = 100, replace = TRUE)\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.00    5.00    5.58    8.00   10.00 \n\ny &lt;- factor(x)\nsummary(y)\n\n 1  2  3  4  5  6  7  8  9 10 \n 7 10  9 12 14 10 10  8  8 12 \n\n\nR6 and RC use encapsulated OOP where objects are the center of everything, holding fields (data) and methods (functions) that operate on those values. These are closest to what you would be used to if you are coming from an OOP language. Try not to use them.\nE.g. in R we apply a function, like mean() to a vector, like x. But in an encapsulated object oriented programming system would have the function mean() attached to a vector x. That’s one difference between R and Python.\nR\n\nx &lt;- c(19, 22, 31)\nmean(x) ## apply mean to x\n\n[1] 24\n\n\nPython\n\nimport numpy as np\nx = np.array([19, 22, 31])\nx.mean() ## mean belongs to x\n\nnp.float64(24.0)\n\n\nS3 allows you to use functions like print() and summary() and plot() on outputs of your functions. You can also define your own “generics.”\nS4 is similar to S3 but is more formal and strict. S4 is important to understand if you want to use or contribute to Bioconductor.\n\n\n\nOOP Vocabulary\n\nPolymorphism: Use the same function name for different types of input, but have the function evaluate differently based on the types of input.\nAn object is a specific instance of a class. E.g. below, x is an object of class factor.\n\nx &lt;- factor(c(119, 22, 31))\nclass(x)\n\n[1] \"factor\"\n\n\nA function for a specific class is a method.\n\nIn R6, methods belong to objects, like the col_means() method for our R6 class above.\nIn S3 and S4, methods are specific versions of generics. Like in S3, print.factor() is the print method for factor objects.\n\nA field is data that belongs to an object. In our R6 example, we had the df and mean_vec fields.\n\nIn S3, fields are called attributes.\nIn S4, fields are called slots.\n\nClasses are defined in a hierarchy. So if a method does not exist in one class it is searched for in the parent class. It is said that the child class inherits the behavior the parent class.\nE.g. tibbles inherit the behavior of data frames.\n\nclass(tibble::tibble(a = 1))\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThe order in which classes are searched for methods is called method dispatch.\n\n\n\n{sloop}\n\nThe {sloop} package is an interface for exploring OOP systems.\nsloop::otype() allows you to see if the system is S3, S4, R6, etc…\n\nsloop::otype(mtcars) ## Most R stuff is in S3.\n\n[1] \"S3\"\n\ndata(\"USCounties\", package = \"Matrix\") ## Efficient matrix computations package\nsloop::otype(USCounties) \n\n[1] \"S4\"\n\npb &lt;- progress::progress_bar$new() ## progress bars for for-loops\nsloop::otype(pb)\n\n[1] \"R6\"\n\n\n\n\n\nBase Types\n\nS (the precursor to R) was developed first without an OOP system. So their only objects were “base types”. But these don’t have basic OOP functionality like polymorphism, inheritance, etc..\nR users often call base types “objects” even though they aren’t OOP objects.\n\nx &lt;- 1:10\nsloop::otype(x)\n\n[1] \"base\"\n\n\nIn R, an OO object has a class attribute and a base type does not.\n\nx &lt;- 1:10\nattr(x, \"class\")\n\nNULL\n\ny &lt;- factor(x)\nattr(y, \"class\")\n\n[1] \"factor\"\n\n\nclass() will return the result of typeof() if an object has no class attribute, this is called its implicit class.\n\nclass(x)\n\n[1] \"integer\"\n\ntypeof(x)\n\n[1] \"integer\"\n\n\nEvery object, including OO objects, have a base type that can be seen by typeof().\n\ntypeof(y)\n\n[1] \"integer\"\n\ntypeof(mtcars)\n\n[1] \"list\"\n\ntypeof(USCounties)\n\n[1] \"S4\"\n\ntypeof(pb)\n\n[1] \"environment\"\n\n\nThere are 25 base types. From Hadley’s list, the important ones are:\n\nVector: NULL, logical, integer, double, character, list\n\ntypeof(NULL)\n\n[1] \"NULL\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(\"1\")\n\n[1] \"character\"\n\ntypeof(list(1))\n\n[1] \"list\"\n\n\nFunctions: closure (regular R functions), special (internal R functions), builtin (“primitive” functions in the base namespace that were built using C)\n\ntypeof(mean)\n\n[1] \"closure\"\n\ntypeof(`if`)\n\n[1] \"special\"\n\ntypeof(sum)\n\n[1] \"builtin\"\n\n\nEnvironments: environment\n\ntypeof(rlang::global_env())\n\n[1] \"environment\"\n\n\nS4 types: S4\n\ntypeof(USCounties)\n\n[1] \"S4\"\n\n\nLanguage types (used in metaprogramming): symbol, language, pairlist, and expression.\n\ntypeof(quote(a))\n\n[1] \"symbol\"\n\ntypeof(quote(a + 1))\n\n[1] \"language\"\n\ntypeof(formals(mean))\n\n[1] \"pairlist\"\n\ntypeof(expression(a))\n\n[1] \"expression\"\n\n\n\nExercise: What’s the (i) type, (ii) OOP system, and (iii) class of the following objects.\n\nx &lt;- lubridate::make_date(year = c(1990, 2022), month = c(1, 2), day = c(30, 22))\ny &lt;- matrix(NA_real_, nrow = 10, ncol = 2)\nz &lt;- tibble::tibble(a = 1:3)\naa &lt;- lm(mpg ~ wt, data = mtcars)\nbb &lt;- t.test(mpg ~ am, data = mtcars)\ncc &lt;- rTensor::as.tensor(array(1:30, dim = c(2, 3, 5)))\n\nExercise: Why do we get different results from summary() with the following code?\n\na &lt;- lm(mpg ~ wt, data = mtcars)\nb &lt;- t.test(mpg ~ am, data = mtcars)\nsummary(a)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.543 -2.365 -0.125  1.410  6.873 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   37.285      1.878   19.86  &lt; 2e-16\nwt            -5.344      0.559   -9.56  1.3e-10\n\nResidual standard error: 3.05 on 30 degrees of freedom\nMultiple R-squared:  0.753, Adjusted R-squared:  0.745 \nF-statistic: 91.4 on 1 and 30 DF,  p-value: 1.29e-10\n\nsummary(b)\n\n            Length Class  Mode     \nstatistic   1      -none- numeric  \nparameter   1      -none- numeric  \np.value     1      -none- numeric  \nconf.int    2      -none- numeric  \nestimate    2      -none- numeric  \nnull.value  1      -none- numeric  \nstderr      1      -none- numeric  \nalternative 1      -none- character\nmethod      1      -none- character\ndata.name   1      -none- character\n\n\nExercise: From the previous exercise, if we remove the class from a and b, what happens to the summary() call? What does this tell you about the summary() methods of the htest and lm classes?"
  },
  {
    "objectID": "05_web_scraping/05_eventbrite.html",
    "href": "05_web_scraping/05_eventbrite.html",
    "title": "Eventbrite OAuth2 Example",
    "section": "",
    "text": "Another example of using OAuth2 for authorization.\nEventBrite API: https://www.eventbrite.com/platform/api/#/introduction/about-our-api\nIntro to API’s: https://www.eventbrite.com/platform/docs/introduction\nIntro to Authorization: https://www.eventbrite.com/platform/docs/authentication\nhttps://httr2.r-lib.org/articles/oauth.html"
  },
  {
    "objectID": "05_web_scraping/05_eventbrite.html#learning-objectives",
    "href": "05_web_scraping/05_eventbrite.html#learning-objectives",
    "title": "Eventbrite OAuth2 Example",
    "section": "",
    "text": "Another example of using OAuth2 for authorization.\nEventBrite API: https://www.eventbrite.com/platform/api/#/introduction/about-our-api\nIntro to API’s: https://www.eventbrite.com/platform/docs/introduction\nIntro to Authorization: https://www.eventbrite.com/platform/docs/authentication\nhttps://httr2.r-lib.org/articles/oauth.html"
  },
  {
    "objectID": "05_web_scraping/05_eventbrite.html#api-key-method",
    "href": "05_web_scraping/05_eventbrite.html#api-key-method",
    "title": "Eventbrite OAuth2 Example",
    "section": "API Key method",
    "text": "API Key method\n\nThe documents indicate that you can use an API key for private use. But I haven’t tried it.\nGo to https://www.eventbrite.com/platform/ and click “Get a Free API Key”\nThe base URL is: https://www.eventbriteapi.com/v3/users/me/\nYou use the query: ?token=&lt;api_key&gt;"
  },
  {
    "objectID": "05_web_scraping/05_eventbrite.html#oauth2-method",
    "href": "05_web_scraping/05_eventbrite.html#oauth2-method",
    "title": "Eventbrite OAuth2 Example",
    "section": "OAuth2 Method",
    "text": "OAuth2 Method\n\nFor authenticating other users, you use OAuth2.\nThe base URL is: https://www.eventbriteapi.com/v3\nGo to https://www.eventbrite.com/platform/ and click “Get a Free API Key”\nGo to https://www.eventbrite.com/account-settings/apps and get information on the Client Secret, and Private and Public Tokens.\nYou need to tell EventBrite the redirect uri. Go to https://www.eventbrite.com/account-settings/apps, click on “API key details” and tell it the OAuth Redirect URI is http://localhost:1410/\nThe docs say the API key is the client ID. The client secret is listed explicitly.\nThe docs say the token URL is: https://www.eventbrite.com/oauth/token\nThey say the authorization URL is: https://www.eventbrite.com/oauth/authorize\n\n\nlibrary(tidyverse)\nlibrary(httr2)\n\nBased on the docs, this is the flow (your id and secret will be different):\n\nclient &lt;- oauth_client(\n  id = \"7Q4MLH2PBGUJWNWLZO\",\n  secret = \"YQW7ZQS6LX525KTISRBBAI5MVAEYLDY6IT6V3IDKLHSOP3JT5O\",\n  token_url = \"https://www.eventbrite.com/oauth/token\"\n)\nauth_url &lt;- \"https://www.eventbrite.com/oauth/authorize\"\nredirect_uri &lt;- \"http://localhost:1410/\"\n\nYou can get a token this way and then reuse it:\n\n## Get the token\ntoken &lt;- oauth_flow_auth_code(\n  client = client,\n  auth_url = auth_url,\n  redirect_uri = redirect_uri\n)\nrequest(\"https://www.eventbriteapi.com/v3\") |&gt;\n  req_url_path_append(\"categories\") |&gt;\n  req_url_query(token = token$access_token) |&gt;\n  req_perform() -&gt;\n  rout\n\nOr you can cache the token after one use (recommended):\n\nrequest(\"https://www.eventbriteapi.com/v3\") |&gt;\n  req_url_path_append(\"categories\") |&gt;\n  req_oauth_auth_code(\n    client = client, \n    auth_url = auth_url,\n    redirect_uri = redirect_uri, \n    cache_disk = TRUE) |&gt;\n  req_perform() -&gt;\n  rout\n\nHere is the output:\n\ntibble(dat = resp_body_json(rout)[[3]]) |&gt;\n  unnest_wider(dat)\n\n# A tibble: 21 × 6\n   resource_uri       id    name  name_localized short_name short_name_localized\n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;               \n 1 https://www.event… 103   Music Music          Music      Music               \n 2 https://www.event… 101   Busi… Business & Pr… Business   Business            \n 3 https://www.event… 110   Food… Food & Drink   Food & Dr… Food & Drink        \n 4 https://www.event… 113   Comm… Community & C… Community  Community           \n 5 https://www.event… 105   Perf… Performing & … Arts       Arts                \n 6 https://www.event… 104   Film… Film, Media &… Film & Me… Film & Media        \n 7 https://www.event… 108   Spor… Sports & Fitn… Sports & … Sports & Fitness    \n 8 https://www.event… 107   Heal… Health & Well… Health     Health              \n 9 https://www.event… 102   Scie… Science & Tec… Science &… Science & Tech      \n10 https://www.event… 109   Trav… Travel & Outd… Travel & … Travel & Outdoor    \n# ℹ 11 more rows"
  },
  {
    "objectID": "10_llms/10_shinychat.html",
    "href": "10_llms/10_shinychat.html",
    "title": "Shiny Chats",
    "section": "",
    "text": "Learning Objectives\n\nAdd chat functionality to your shiny app.\nhttps://posit-dev.github.io/shinychat/r/index.html"
  },
  {
    "objectID": "10_llms/10_embeddings.html",
    "href": "10_llms/10_embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "Learning Objectives\n\nConvert text data to embeddings for use in standard ML models.\n3blue1brown course\nhttps://www.sbert.net/\nhttps://huggingface.co/models?library=sentence-transformers\nhttps://ollama.com/blog/embedding-models\nhttps://vickiboykis.com/what_are_embeddings/\n\n\n\nWhat are embeddings?\n\n\nUse case for embedding models\n\nSemantic textual similarity,\nSemantic search\nClustering\nClassification\nParaphrase Mining,\n\n\n\nGenerating embeddings\n\nI think the easiest way to generate embeddings is with the SentenceTransformers python package.\nIf you are using the {reticulate} package, you can set this up with:\n\nlibrary(reticulate)\n\nconda_create(envname = \"embed\", packages = c(\"sentence-transformers\", \"pandas\", \"seaborn\", \"scikit-learn\"))\n\nuse_condaenv(condaenv = \"embed\")\n\nWe first load SentenceTransformer\n\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nWe choose the embedding model, and download it from huggingface.\n\nPossible choices are here: https://huggingface.co/models?library=sentence-transformers\n\nIf you have done the authentication, then you can try out Google’s model:\n\nmodel = SentenceTransformer(\"google/embeddinggemma-300m\")\n\nIf not, then you can try out a free one: python     model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n\n# Run inference with queries and documents\nquery = \"Which planet is known as the Red Planet?\"\ndocuments = [\n    \"Venus is often called Earth's twin because of its similar size and proximity.\",\n    \"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n    \"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n    \"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\nquery_embeddings = model.encode_query(query)\nquery_embeddings2 = model.encode(query)\ndocument_embeddings = model.encode_document(documents)\n\n# Compute similarities to determine a ranking\nsimilarities = model.similarity(query_embeddings, document_embeddings)\nsimilarities\n\ntensor([[0.3008, 0.6361, 0.4927, 0.4889]])\n\n\n\n\nPractical example"
  },
  {
    "objectID": "10_llms/10_llama.html",
    "href": "10_llms/10_llama.html",
    "title": "Free Models",
    "section": "",
    "text": "Use weights from free models on your local machine.\nOllama: https://ollama.com/\nHuggingface: https://huggingface.co/\nrollama: https://jbgruber.github.io/rollama/\n\n\nlibrary(tidyverse)\nlibrary(rollama)"
  },
  {
    "objectID": "10_llms/10_ellmer.html",
    "href": "10_llms/10_ellmer.html",
    "title": "LLM APIs",
    "section": "",
    "text": "Learning Objectives\n\nInterface with an LLM API via ellmer\nhttps://ellmer.tidyverse.org/\nInclude API interface and output in a Shiny app."
  },
  {
    "objectID": "16_maps/16_maps.html",
    "href": "16_maps/16_maps.html",
    "title": "Simple Features and Graphing Maps",
    "section": "",
    "text": "Learning Objectives\n\nUnderstand the concept and utility of spatial data visualization in R.\nGet acquainted with the basic functions and data types in the {sf} package.\nCreate simple maps using the {sf} package.\nSimple Features\n\nhttps://r-spatial.github.io/sf/articles/\n\nMap Plotting\n\nhttps://ggplot2.tidyverse.org/reference/ggsf.html\n\n\n\n\nMotivation\n\nFor some reason, people are really impressed with data visualizations that include maps (e.g. of D.C., the U.S., the Earth, etc).\nLet’s go over how to build simple map visualizations in R.\nFor this lecture, we will try to visualize the 2020 presidential election elections from (https://github.com/TheUpshot/presidential-precinct-map-2020).\n\n\n\nIntroduction to Simple Features in R\n\nThe sf package, implements the “simple features” standard, making it easier to manipulate geographical data in R.\n\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "10_llms/10_llama.html#common-tasks-for-llms",
    "href": "10_llms/10_llama.html#common-tasks-for-llms",
    "title": "Free Models",
    "section": "Common Tasks for LLM’s",
    "text": "Common Tasks for LLM’s\n\nThere are different models that are appropriate for different tasks\nThe list of models: https://ollama.com/search\nFrom Hugging Face: https://huggingface.co/models?library=gguf\nGeneral Chat:\n\nTuned for: Conversations, questions, answers\nExample Use-case: Creating a chat bot inside a shiny app.\nExample model: llama3.1\n\nInstruct models:\n\nTuned for: completing a specific task\nExample Use-case: Classifying text based on a number of categories.\nExample model:\n\nEmbedding:\n\nConverting text into some a vector of features. To be used in later machine learning tasks.\nExample Use-case: Convert yelp reviews into features to predict properties of a restaurant (e.g., whether it will close in a year).\nExample model:"
  },
  {
    "objectID": "10_llms/10_basics.html",
    "href": "10_llms/10_basics.html",
    "title": "LLM Basics",
    "section": "",
    "text": "3Blue1Brown LLM Explainer\n3Blue1Brown Trasnformer\nHugging Face Model Search"
  },
  {
    "objectID": "10_llms/10_basics.html#tokenization",
    "href": "10_llms/10_basics.html#tokenization",
    "title": "LLM Basics",
    "section": "Tokenization:",
    "text": "Tokenization:\n\nThe input text is split into tokens using a tokenizer.\nTokens are subwords or word pieces.\nModels always list the “context length” in terms of tokens. This is the maximum number of tokens the model can handle.\n\nIf you insert too much text, the model truncates earlier text.\nSo the model “forgets” if you have longer text than the maximum token length.\n\nGPT-5 API has a maximum context length of 400,000 tokens (this includes input and output).\n\nRoughly 100 tokens is 75 words\nSo this is roughly 300,000 words\nSo it cannot yet take as input the entirety of War and Peace (587,287 words), but it can handle the first Twilight Novel (118,975 words) source.\n\nLet’s play with some tokens. We will use the google/gemma-3-1b-it model.\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport torch.nn.functional as F\n\nmodel_name = \"google/gemma-3-1b-it\"\n\nYou can access the tokenizer the model uses via the AutoTokenizer.from_pretrained() function\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nNow, let’s tokenize some text with tokenizer.tokenize()\n\ntokenizer.tokenize(\"Once upon a time, a very long time ago now, about last Friday\")\n\n['Once', '▁upon', '▁a', '▁time', ',', '▁a', '▁very', '▁long', '▁time', '▁ago', '▁now', ',', '▁about', '▁last', '▁Friday']\n\n\nThe _ indiciates that the token is the start of a new word, not a continuation of a word.\nE.g., “imagination” is two tokens:\n\ntokenizer.tokenize(\"Imagination\")\n\n['Imag', 'ination']\n\n\nIf it sees a strange work, it does its best\n\ntokenizer.tokenize(\"Heffalump\")\n\n['He', 'ff', 'al', 'ump']\n\n\nExercise: Try to guess the tokens of this sentence\n\ntokenizer.tokenize(\"Gaiety, song-and-dance. Here we go round the mulberry bush.\")\n\nExercise: Generate some tokens of some works. Are you surprised by any of them?"
  },
  {
    "objectID": "10_llms/10_basics.html#embedding",
    "href": "10_llms/10_basics.html#embedding",
    "title": "LLM Basics",
    "section": "Embedding:",
    "text": "Embedding:\n\nEach token is mapped to a vector using an embedding layer.\nThis converts discrete tokens into continuous representations that the model can process.\nThe idea is that “cat” might be closer (via some distance metric) to “dog” than it is to “stapler”.\n\nEach of those tokens live in a super high dimensional space.\nEmbedding dimension sizes can be 4096 or higher.\n\nEmbeddings are useful in their own right. They can be used as inputs to many machine learning tasks that use natural language:\n\nCluster X posts into different categories.\nUse descriptions of user ratings as inputs into a machine learning model to predict if someone will buy a product.\nInside a document, find the sentence which is most similar to a query.\n\nIn those applications, you pool the embeddings from the tokens into a single embedding for the text.\nTypically, though, you use models that were designed where the embeddings are the goal.\n\nEven though LLMs use embeddings, those are sometimes not the best for applications that use embeddings.\nhttps://huggingface.co/models?library=sentence-transformers\n\nLet’s see the embeddings for some tokens.\n\nDon’t worry about the exact code. Folks don’t do thism so it’s a little wonky.\n\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntext = \"Once upon a time, a very long time ago now,\"\ninput_ids = tokenizer(text, return_tensors = \"pt\").input_ids\nembedding_layer = model.get_input_embeddings()\ntoken_embeddings = embedding_layer(input_ids)\ntoken_embeddings[0,0]\n\ntensor([ 0.1388, -0.2165, -0.5386,  ..., -0.2465,  0.0267,  0.1258],\n       grad_fn=&lt;SelectBackward0&gt;)\n\ntoken_embeddings[0,1]\n\ntensor([-0.2113,  0.6753, -0.6505,  ...,  0.7541,  0.6588,  1.7319],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nThe object is a tensor, a multidimensional array from torch.\n\nVectors or lists of numbers\nMatrices are rectangles of numbers.\nTensors are blocks (or hyperblocks) of numbers."
  },
  {
    "objectID": "10_llms/10_basics.html#neural-network",
    "href": "10_llms/10_basics.html#neural-network",
    "title": "LLM Basics",
    "section": "Neural Network:",
    "text": "Neural Network:\n\nThese embeddings are passed through a black-box neural network.\nThe specific form of the neural network is very important for how they work.\nIt is not important for applying these methods.\nYou need a PhD in order for this process to matter to you, or for you to have any effect on it.\nAU has a super computer, but it is definitely not able to train any of the state-of-the-art LLMs that folks are putting out there."
  },
  {
    "objectID": "10_llms/10_basics.html#sample",
    "href": "10_llms/10_basics.html#sample",
    "title": "LLM Basics",
    "section": "Sample",
    "text": "Sample\n\nThe black box provides a probability distribution of tokens.\nThe next token is sampled from this probability distribution.\n\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport torch.nn.functional as F\n\nmodel_name = \"google/gemma-3-1b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\nmodel.eval()\n\ninput_text = \"The cat sat on the mat.\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(input_ids)\n    logits = outputs.logits\n\nnext_token_logits = logits[0, -1, :]\nprobs = F.softmax(next_token_logits, dim=-1)\n\ntop_k = torch.topk(probs, k=10)\nfor idx, prob in zip(top_k.indices, top_k.values):\n    print(f\"{tokenizer.decode(idx)}: {prob.item():.4f}\")"
  }
]