---
title: "Spark"
author: "David Gerard"
date: "`r Sys.Date()`"
output:  
  html_document:
    toc: true
    toc_depth: 4
urlcolor: "blue"
---


```{r setup, include=FALSE}
set.seed(1)
knitr::opts_chunk$set(echo       = TRUE, 
                      fig.height = 3, 
                      fig.width  = 6,
                      fig.align  = "center")
ggplot2::theme_set(ggplot2::theme_bw())
```

# Learning Objectives

- Learn some Spark
- [Mastering Spark in R](https://therinspark.com/index.html)

# Motivation

- Apache Spark is a program that allows for computing on databases distributed across multiple computers.

- With Spark:
    - You can do data munging (dplyr/SQL stuff) on a dataset that is distributed across many computers,
    - You can fit statistical (or train machine learning) models (like logistic regression) on a dataset that is distributed across many computers.
    - More general computing operations, distributed across multiple computers.
    
- You can do arbitrarily complicated things with Spark, but for you the point of Spark is to do simple things on Big Data which won't fit on one computer.
